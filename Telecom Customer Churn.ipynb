{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Classification - Sanketh Reddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This dataset consists of 5986 rows and 21 columns (when importing the csv file, there will be 22 columns as the first column will be \"Unnamed: 0\")\n",
    "\n",
    "## This dataset has characteristics about customers from a Telecom company, and it has information regarding whether or not these customers left the company (churn). Each of the rows contains characteristics for each unique customer.\n",
    "\n",
    "## The goal will be to create a binary classification model that can predict Customer Churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing pandas, numpy, matplotlib, and seaborn and reading in the dataset and checking its characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>Churn</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>7010-BRBUU</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>No</td>\n",
       "      <td>24.10</td>\n",
       "      <td>1734.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4528</td>\n",
       "      <td>9688-YGXVR</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>44</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>No</td>\n",
       "      <td>88.15</td>\n",
       "      <td>3973.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6344</td>\n",
       "      <td>9286-DOJGF</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>74.95</td>\n",
       "      <td>2869.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6739</td>\n",
       "      <td>6994-KERXL</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>No</td>\n",
       "      <td>55.90</td>\n",
       "      <td>238.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432</td>\n",
       "      <td>2181-UAESM</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>No</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>No</td>\n",
       "      <td>53.45</td>\n",
       "      <td>119.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  customerID  gender  SeniorCitizen Partner Dependents  tenure  \\\n",
       "0        1869  7010-BRBUU    Male              0     Yes        Yes      72   \n",
       "1        4528  9688-YGXVR  Female              0      No         No      44   \n",
       "2        6344  9286-DOJGF  Female              1     Yes         No      38   \n",
       "3        6739  6994-KERXL    Male              0      No         No       4   \n",
       "4         432  2181-UAESM    Male              0      No         No       2   \n",
       "\n",
       "  PhoneService MultipleLines InternetService  ...     DeviceProtection  \\\n",
       "0          Yes           Yes              No  ...  No internet service   \n",
       "1          Yes            No     Fiber optic  ...                  Yes   \n",
       "2          Yes           Yes     Fiber optic  ...                   No   \n",
       "3          Yes            No             DSL  ...                   No   \n",
       "4          Yes            No             DSL  ...                  Yes   \n",
       "\n",
       "           TechSupport          StreamingTV      StreamingMovies  \\\n",
       "0  No internet service  No internet service  No internet service   \n",
       "1                   No                  Yes                   No   \n",
       "2                   No                   No                   No   \n",
       "3                   No                   No                  Yes   \n",
       "4                   No                   No                   No   \n",
       "\n",
       "         Contract PaperlessBilling              PaymentMethod Churn  \\\n",
       "0        Two year               No    Credit card (automatic)    No   \n",
       "1  Month-to-month              Yes    Credit card (automatic)    No   \n",
       "2  Month-to-month              Yes  Bank transfer (automatic)   Yes   \n",
       "3  Month-to-month              Yes           Electronic check    No   \n",
       "4  Month-to-month               No           Electronic check    No   \n",
       "\n",
       "  MonthlyCharges TotalCharges  \n",
       "0          24.10      1734.65  \n",
       "1          88.15       3973.2  \n",
       "2          74.95      2869.85  \n",
       "3          55.90        238.5  \n",
       "4          53.45        119.5  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"telecom_users.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5986 entries, 0 to 5985\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        5986 non-null   int64  \n",
      " 1   customerID        5986 non-null   object \n",
      " 2   gender            5986 non-null   object \n",
      " 3   SeniorCitizen     5986 non-null   int64  \n",
      " 4   Partner           5986 non-null   object \n",
      " 5   Dependents        5986 non-null   object \n",
      " 6   tenure            5986 non-null   int64  \n",
      " 7   PhoneService      5986 non-null   object \n",
      " 8   MultipleLines     5986 non-null   object \n",
      " 9   InternetService   5986 non-null   object \n",
      " 10  OnlineSecurity    5986 non-null   object \n",
      " 11  OnlineBackup      5986 non-null   object \n",
      " 12  DeviceProtection  5986 non-null   object \n",
      " 13  TechSupport       5986 non-null   object \n",
      " 14  StreamingTV       5986 non-null   object \n",
      " 15  StreamingMovies   5986 non-null   object \n",
      " 16  Contract          5986 non-null   object \n",
      " 17  PaperlessBilling  5986 non-null   object \n",
      " 18  PaymentMethod     5986 non-null   object \n",
      " 19  Churn             5986 non-null   object \n",
      " 20  MonthlyCharges    5986 non-null   float64\n",
      " 21  TotalCharges      5986 non-null   object \n",
      "dtypes: float64(1), int64(3), object(18)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TotalCharges column is a type \"object\" which means there may be some empty strings there. Thus, the following cell will convert those empty strings to be a float value of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertStringToFloat(total):\n",
    "    if total == ' ':\n",
    "        return float(0)\n",
    "    else:\n",
    "        return float(total)\n",
    "\n",
    "df['TotalCharges'] = [convertStringToFloat(t) for t in df['TotalCharges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5986 entries, 0 to 5985\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        5986 non-null   int64  \n",
      " 1   customerID        5986 non-null   object \n",
      " 2   gender            5986 non-null   object \n",
      " 3   SeniorCitizen     5986 non-null   int64  \n",
      " 4   Partner           5986 non-null   object \n",
      " 5   Dependents        5986 non-null   object \n",
      " 6   tenure            5986 non-null   int64  \n",
      " 7   PhoneService      5986 non-null   object \n",
      " 8   MultipleLines     5986 non-null   object \n",
      " 9   InternetService   5986 non-null   object \n",
      " 10  OnlineSecurity    5986 non-null   object \n",
      " 11  OnlineBackup      5986 non-null   object \n",
      " 12  DeviceProtection  5986 non-null   object \n",
      " 13  TechSupport       5986 non-null   object \n",
      " 14  StreamingTV       5986 non-null   object \n",
      " 15  StreamingMovies   5986 non-null   object \n",
      " 16  Contract          5986 non-null   object \n",
      " 17  PaperlessBilling  5986 non-null   object \n",
      " 18  PaymentMethod     5986 non-null   object \n",
      " 19  Churn             5986 non-null   object \n",
      " 20  MonthlyCharges    5986 non-null   float64\n",
      " 21  TotalCharges      5986 non-null   float64\n",
      "dtypes: float64(2), int64(3), object(17)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          0\n",
       "customerID          0\n",
       "gender              0\n",
       "SeniorCitizen       0\n",
       "Partner             0\n",
       "Dependents          0\n",
       "tenure              0\n",
       "PhoneService        0\n",
       "MultipleLines       0\n",
       "InternetService     0\n",
       "OnlineSecurity      0\n",
       "OnlineBackup        0\n",
       "DeviceProtection    0\n",
       "TechSupport         0\n",
       "StreamingTV         0\n",
       "StreamingMovies     0\n",
       "Contract            0\n",
       "PaperlessBilling    0\n",
       "PaymentMethod       0\n",
       "Churn               0\n",
       "MonthlyCharges      0\n",
       "TotalCharges        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above, there are 0 null values in the dataset. Thus, the following cells will be randomly placing null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Introducing NaN Values in the dataset\n",
    "np.random.seed(2)\n",
    "for i in range(3):\n",
    "    col = np.random.randint(21)\n",
    "    n = np.random.uniform(0, 0.1) # 10% missing values\n",
    "    df.loc[df.sample(frac=n).index, df.columns[col]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "customerID            0\n",
       "gender                0\n",
       "SeniorCitizen         0\n",
       "Partner             210\n",
       "Dependents            0\n",
       "tenure                0\n",
       "PhoneService          0\n",
       "MultipleLines       111\n",
       "InternetService       0\n",
       "OnlineSecurity      299\n",
       "OnlineBackup          0\n",
       "DeviceProtection      0\n",
       "TechSupport           0\n",
       "StreamingTV           0\n",
       "StreamingMovies       0\n",
       "Contract              0\n",
       "PaperlessBilling      0\n",
       "PaymentMethod         0\n",
       "Churn                 0\n",
       "MonthlyCharges        0\n",
       "TotalCharges          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.357500835282325"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent of null values in dataset\n",
    "df.isna().sum().sum()/len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above, there are now 620 null values in the dataset which means about 10% of the dataset has null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>Churn</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>7010-BRBUU</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>No internet service</td>\n",
       "      <td>Two year</td>\n",
       "      <td>No</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>No</td>\n",
       "      <td>24.10</td>\n",
       "      <td>1734.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4528</td>\n",
       "      <td>9688-YGXVR</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>44</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>No</td>\n",
       "      <td>88.15</td>\n",
       "      <td>3973.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6344</td>\n",
       "      <td>9286-DOJGF</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>74.95</td>\n",
       "      <td>2869.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6739</td>\n",
       "      <td>6994-KERXL</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DSL</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>No</td>\n",
       "      <td>55.90</td>\n",
       "      <td>238.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432</td>\n",
       "      <td>2181-UAESM</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>No</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>No</td>\n",
       "      <td>53.45</td>\n",
       "      <td>119.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  customerID  gender  SeniorCitizen Partner Dependents  tenure  \\\n",
       "0        1869  7010-BRBUU    Male              0     Yes        Yes      72   \n",
       "1        4528  9688-YGXVR  Female              0      No         No      44   \n",
       "2        6344  9286-DOJGF  Female              1     Yes         No      38   \n",
       "3        6739  6994-KERXL    Male              0      No         No       4   \n",
       "4         432  2181-UAESM    Male              0      No         No       2   \n",
       "\n",
       "  PhoneService MultipleLines InternetService  ...     DeviceProtection  \\\n",
       "0          Yes           Yes              No  ...  No internet service   \n",
       "1          Yes            No     Fiber optic  ...                  Yes   \n",
       "2          Yes           Yes     Fiber optic  ...                   No   \n",
       "3          Yes           NaN             DSL  ...                   No   \n",
       "4          Yes            No             DSL  ...                  Yes   \n",
       "\n",
       "           TechSupport          StreamingTV      StreamingMovies  \\\n",
       "0  No internet service  No internet service  No internet service   \n",
       "1                   No                  Yes                   No   \n",
       "2                   No                   No                   No   \n",
       "3                   No                   No                  Yes   \n",
       "4                   No                   No                   No   \n",
       "\n",
       "         Contract PaperlessBilling              PaymentMethod Churn  \\\n",
       "0        Two year               No    Credit card (automatic)    No   \n",
       "1  Month-to-month              Yes    Credit card (automatic)    No   \n",
       "2  Month-to-month              Yes  Bank transfer (automatic)   Yes   \n",
       "3  Month-to-month              Yes           Electronic check    No   \n",
       "4  Month-to-month               No           Electronic check    No   \n",
       "\n",
       "  MonthlyCharges TotalCharges  \n",
       "0          24.10      1734.65  \n",
       "1          88.15      3973.20  \n",
       "2          74.95      2869.85  \n",
       "3          55.90       238.50  \n",
       "4          53.45       119.50  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5986 entries, 0 to 5985\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        5986 non-null   int64  \n",
      " 1   customerID        5986 non-null   object \n",
      " 2   gender            5986 non-null   object \n",
      " 3   SeniorCitizen     5986 non-null   int64  \n",
      " 4   Partner           5776 non-null   object \n",
      " 5   Dependents        5986 non-null   object \n",
      " 6   tenure            5986 non-null   int64  \n",
      " 7   PhoneService      5986 non-null   object \n",
      " 8   MultipleLines     5875 non-null   object \n",
      " 9   InternetService   5986 non-null   object \n",
      " 10  OnlineSecurity    5687 non-null   object \n",
      " 11  OnlineBackup      5986 non-null   object \n",
      " 12  DeviceProtection  5986 non-null   object \n",
      " 13  TechSupport       5986 non-null   object \n",
      " 14  StreamingTV       5986 non-null   object \n",
      " 15  StreamingMovies   5986 non-null   object \n",
      " 16  Contract          5986 non-null   object \n",
      " 17  PaperlessBilling  5986 non-null   object \n",
      " 18  PaymentMethod     5986 non-null   object \n",
      " 19  Churn             5986 non-null   object \n",
      " 20  MonthlyCharges    5986 non-null   float64\n",
      " 21  TotalCharges      5986 non-null   float64\n",
      "dtypes: float64(2), int64(3), object(17)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info() # Partner, MultipleLines, OnlineSecurity columns have null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The columns with missing values are: Partner, MultipleLines, and OnlineSecurity\n",
    "###  The next few cells will be figuring out how to impute the missing values in these columns\n",
    "###  The distribution of values from other columns will help determine how to impute the missing values in the Partner, MultipleLines, and OnlineSecurity columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values for Partner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  From the barplot below, customers with dependents were significantly more likely to have partners\n",
    "##### Thus, the missing values for the Partner column will be imputed to be \"Yes\" if the value for the Dependents column was also \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['Dependents']=='Yes']['Partner'].value_counts().index, y=df[df['Dependents'] == 'Yes']['Partner'].value_counts())\n",
    "plt.title(\"Distribution of Partner Values When Dependents = 'Yes'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers with no dependents were significantly more likely to not have partners\n",
    "##### Thus, the missing values for the Partner column will be imputed to be \"No\" if the value for the Dependents column was also \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['Dependents']=='No']['Partner'].value_counts().index, y=df[df['Dependents'] == 'No']['Partner'].value_counts())\n",
    "plt.title(\"Distribution of Partner Values When Dependents = 'No'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing the missing values for the Partner column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "customerID            0\n",
       "gender                0\n",
       "SeniorCitizen         0\n",
       "Partner               0\n",
       "Dependents            0\n",
       "tenure                0\n",
       "PhoneService          0\n",
       "MultipleLines       111\n",
       "InternetService       0\n",
       "OnlineSecurity      299\n",
       "OnlineBackup          0\n",
       "DeviceProtection      0\n",
       "TechSupport           0\n",
       "StreamingTV           0\n",
       "StreamingMovies       0\n",
       "Contract              0\n",
       "PaperlessBilling      0\n",
       "PaymentMethod         0\n",
       "Churn                 0\n",
       "MonthlyCharges        0\n",
       "TotalCharges          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fillMissingValuesPartner(partner, dependents):\n",
    "    if isinstance(partner, float):\n",
    "        if dependents == 'Yes':\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'No'\n",
    "    else:\n",
    "        return partner\n",
    "\n",
    "df['Partner'] = [fillMissingValuesPartner(p,d) for (p,d) in zip(df['Partner'], df['Dependents'])]\n",
    "df.isna().sum() # Partner column has 0 null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     3140\n",
       "Yes    2846\n",
       "Name: Partner, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Partner'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values for MultipleLines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers that streamed movies were significantly more likely to have multiple lines\n",
    "##### Thus, the missing values for the MultipleLines column will be imputed to be \"Yes\" if the value for the StreamingMovies column was also \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['StreamingMovies']=='Yes']['MultipleLines'].value_counts().index, y=df[df['StreamingMovies'] == 'Yes']['MultipleLines'].value_counts())\n",
    "plt.title(\"Distribution of MultipleLines values when StreamingMoves = 'Yes'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers that did not stream movies were more likely to not have multiple lines\n",
    "##### Thus, the missing values for the MultipleLines column will be imputed to be \"No\" if the value for the StreamingMovies column was also \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['StreamingMovies']=='No']['MultipleLines'].value_counts().index, y=df[df['StreamingMovies'] == 'No']['MultipleLines'].value_counts())\n",
    "plt.title(\"Distribution of MultipleLines values when StreamingMovies = 'No'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers without internet service were significantly more likely to not have multiple lines\n",
    "##### Thus, the missing values for the MultipleLines column will be imputed to be \"No\" if the value for the InternetService column was also \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['InternetService']=='No']['MultipleLines'].value_counts().index, y=df[df['InternetService'] == 'No']['MultipleLines'].value_counts())\n",
    "plt.title(\"Distribution of MultipleLines values when InternetService = 'No'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers without phone service did not have multiple lines\n",
    "##### Thus, the missing values for the MultipleLines column will be imputed to be \"No phone service\" if the value for the PhoneService column was \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['PhoneService']=='No']['MultipleLines'].value_counts().index, y=df[df['PhoneService'] == 'No']['MultipleLines'].value_counts())\n",
    "plt.title(\"Distribution of MultipleLines values when PhoneService = 'No'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing Missing Values for MultipleLines column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0            0\n",
       "customerID            0\n",
       "gender                0\n",
       "SeniorCitizen         0\n",
       "Partner               0\n",
       "Dependents            0\n",
       "tenure                0\n",
       "PhoneService          0\n",
       "MultipleLines         0\n",
       "InternetService       0\n",
       "OnlineSecurity      299\n",
       "OnlineBackup          0\n",
       "DeviceProtection      0\n",
       "TechSupport           0\n",
       "StreamingTV           0\n",
       "StreamingMovies       0\n",
       "Contract              0\n",
       "PaperlessBilling      0\n",
       "PaymentMethod         0\n",
       "Churn                 0\n",
       "MonthlyCharges        0\n",
       "TotalCharges          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fillMissingValuesMultipleLines(multipleLines, streamingMovies, internetService, phoneService):\n",
    "    if isinstance(multipleLines, float):\n",
    "        if streamingMovies == 'Yes':\n",
    "            return 'Yes'\n",
    "        elif streamingMovies == 'No':\n",
    "            return 'No'\n",
    "        elif internetService == 'No':\n",
    "            return 'No'\n",
    "        elif phoneService == 'No':\n",
    "            return 'No phone service'\n",
    "    else:\n",
    "        return multipleLines\n",
    "\n",
    "df['MultipleLines'] = [fillMissingValuesMultipleLines(ml, sm, iservice, ps) for (ml,sm,iservice,ps) in zip(df['MultipleLines'], df['StreamingMovies'], df['InternetService'], df['PhoneService'])]\n",
    "df.isna().sum() # MultipleLines column has 0 null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No                  2860\n",
       "Yes                 2548\n",
       "No phone service     578\n",
       "Name: MultipleLines, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MultipleLines'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values for OnlineSecurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers without internet service did not have OnlineSecurity\n",
    "##### Thus, the missing values for the OnlineSecurity column will be imputed to be \"No internet service\" if the value for the InternetService column was \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['InternetService']=='No']['OnlineSecurity'].value_counts().index, y=df[df['InternetService'] == 'No']['OnlineSecurity'].value_counts())\n",
    "plt.title(\"Distribution of OnlineSecurity values when InternetService = 'No'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the barplot below, customers without online backup were significantly more likely to not have online security\n",
    "##### Thus, the missing values for the OnlineSecurity column will be imputed to be \"No\" if the value for the OnlineBackup column was also \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=df[df['OnlineBackup']=='No']['OnlineSecurity'].value_counts().index, y=df[df['OnlineBackup'] == 'No']['OnlineSecurity'].value_counts())\n",
    "plt.title(\"Distribution of OnlineSecurity values when OnlineBackup = 'No'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing Missing Values for OnlineSecurity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          0\n",
       "customerID          0\n",
       "gender              0\n",
       "SeniorCitizen       0\n",
       "Partner             0\n",
       "Dependents          0\n",
       "tenure              0\n",
       "PhoneService        0\n",
       "MultipleLines       0\n",
       "InternetService     0\n",
       "OnlineSecurity      0\n",
       "OnlineBackup        0\n",
       "DeviceProtection    0\n",
       "TechSupport         0\n",
       "StreamingTV         0\n",
       "StreamingMovies     0\n",
       "Contract            0\n",
       "PaperlessBilling    0\n",
       "PaymentMethod       0\n",
       "Churn               0\n",
       "MonthlyCharges      0\n",
       "TotalCharges        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fillMissingValuesOnlineSecurity(onlineSecurity, internetService, onlineBackup):\n",
    "    if isinstance(onlineSecurity, float):\n",
    "        if internetService == 'No':\n",
    "            return 'No internet service'\n",
    "        elif onlineBackup == 'No':\n",
    "            return 'No'\n",
    "        else:\n",
    "            return 'Yes'\n",
    "    else:\n",
    "        return onlineSecurity\n",
    "\n",
    "df['OnlineSecurity'] = [fillMissingValuesOnlineSecurity(os, iservice, ob) for (os,iservice,ob) in zip(df['OnlineSecurity'], df['InternetService'], df['OnlineBackup'])]\n",
    "df.isna().sum() # OnlineSecurity has 0 null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No                     2976\n",
       "Yes                    1719\n",
       "No internet service    1291\n",
       "Name: OnlineSecurity, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OnlineSecurity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barplot showing the distribution of the target column of \"Churn\"\n",
    "#### From the Barplot below, there is a huge imbalance of values in the \"Churn\" column as the values for \"No\" are significantly higher than the values for \"Yes\"\n",
    "#### This imbalance of values in the \"Churn\" column will be taken into effect later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.countplot(df['Churn'])\n",
    "g.set_xticklabels(['No','Yes'])\n",
    "plt.title('Churn vs. No Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping categorical columns with 'Yes'/'No' values to integer values\n",
    "#### Note, for the 'Churn' column, 'Yes' will be marked as 1 and 'No' will be marked as 0 as customers churning is the target we want to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df['gender'].map({'Male':1, 'Female':0}).astype(int)\n",
    "df['Partner'] = df['Partner'].map({'Yes':1, 'No':0}).astype(int)\n",
    "df['Dependents'] = df['Dependents'].map({'Yes':1, 'No':0}).astype(int)\n",
    "df['PhoneService'] = df['PhoneService'].map({'Yes':1, 'No':0}).astype(int)\n",
    "df['PaperlessBilling'] = df['PaperlessBilling'].map({'Yes':1, 'No':0}).astype(int)\n",
    "df['Churn'] = df['Churn'].map({'Yes':1, 'No':0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5986 entries, 0 to 5985\n",
      "Data columns (total 53 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   Unnamed: 0                               5986 non-null   int64  \n",
      " 1   customerID                               5986 non-null   object \n",
      " 2   gender                                   5986 non-null   int32  \n",
      " 3   SeniorCitizen                            5986 non-null   int64  \n",
      " 4   Partner                                  5986 non-null   int32  \n",
      " 5   Dependents                               5986 non-null   int32  \n",
      " 6   tenure                                   5986 non-null   int64  \n",
      " 7   PhoneService                             5986 non-null   int32  \n",
      " 8   MultipleLines                            5986 non-null   object \n",
      " 9   InternetService                          5986 non-null   object \n",
      " 10  OnlineSecurity                           5986 non-null   object \n",
      " 11  OnlineBackup                             5986 non-null   object \n",
      " 12  DeviceProtection                         5986 non-null   object \n",
      " 13  TechSupport                              5986 non-null   object \n",
      " 14  StreamingTV                              5986 non-null   object \n",
      " 15  StreamingMovies                          5986 non-null   object \n",
      " 16  Contract                                 5986 non-null   object \n",
      " 17  PaperlessBilling                         5986 non-null   int32  \n",
      " 18  PaymentMethod                            5986 non-null   object \n",
      " 19  Churn                                    5986 non-null   int32  \n",
      " 20  MonthlyCharges                           5986 non-null   float64\n",
      " 21  TotalCharges                             5986 non-null   float64\n",
      " 22  MultipleLines_No                         5986 non-null   uint8  \n",
      " 23  MultipleLines_No phone service           5986 non-null   uint8  \n",
      " 24  MultipleLines_Yes                        5986 non-null   uint8  \n",
      " 25  InternetService_DSL                      5986 non-null   uint8  \n",
      " 26  InternetService_Fiber optic              5986 non-null   uint8  \n",
      " 27  InternetService_No                       5986 non-null   uint8  \n",
      " 28  OnlineSecurity_No                        5986 non-null   uint8  \n",
      " 29  OnlineSecurity_No internet service       5986 non-null   uint8  \n",
      " 30  OnlineSecurity_Yes                       5986 non-null   uint8  \n",
      " 31  OnlineBackup_No                          5986 non-null   uint8  \n",
      " 32  OnlineBackup_No internet service         5986 non-null   uint8  \n",
      " 33  OnlineBackup_Yes                         5986 non-null   uint8  \n",
      " 34  DeviceProtection_No                      5986 non-null   uint8  \n",
      " 35  DeviceProtection_No internet service     5986 non-null   uint8  \n",
      " 36  DeviceProtection_Yes                     5986 non-null   uint8  \n",
      " 37  TechSupport_No                           5986 non-null   uint8  \n",
      " 38  TechSupport_No internet service          5986 non-null   uint8  \n",
      " 39  TechSupport_Yes                          5986 non-null   uint8  \n",
      " 40  StreamingTV_No                           5986 non-null   uint8  \n",
      " 41  StreamingTV_No internet service          5986 non-null   uint8  \n",
      " 42  StreamingTV_Yes                          5986 non-null   uint8  \n",
      " 43  StreamingMovies_No                       5986 non-null   uint8  \n",
      " 44  StreamingMovies_No internet service      5986 non-null   uint8  \n",
      " 45  StreamingMovies_Yes                      5986 non-null   uint8  \n",
      " 46  Contract_Month-to-month                  5986 non-null   uint8  \n",
      " 47  Contract_One year                        5986 non-null   uint8  \n",
      " 48  Contract_Two year                        5986 non-null   uint8  \n",
      " 49  PaymentMethod_Bank transfer (automatic)  5986 non-null   uint8  \n",
      " 50  PaymentMethod_Credit card (automatic)    5986 non-null   uint8  \n",
      " 51  PaymentMethod_Electronic check           5986 non-null   uint8  \n",
      " 52  PaymentMethod_Mailed check               5986 non-null   uint8  \n",
      "dtypes: float64(2), int32(6), int64(3), object(11), uint8(31)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "cols = pd.get_dummies(df['MultipleLines'], prefix= 'MultipleLines')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['InternetService'], prefix= 'InternetService')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['OnlineSecurity'], prefix= 'OnlineSecurity')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['OnlineBackup'], prefix= 'OnlineBackup')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['DeviceProtection'], prefix= 'DeviceProtection')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['TechSupport'], prefix= 'TechSupport')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['StreamingTV'], prefix= 'StreamingTV')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['StreamingMovies'], prefix= 'StreamingMovies')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['Contract'], prefix= 'Contract')\n",
    "df[cols.columns] = cols\n",
    "cols = pd.get_dummies(df['PaymentMethod'], prefix= 'PaymentMethod')\n",
    "df[cols.columns] = cols\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['customerID','Unnamed: 0','MultipleLines', 'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaymentMethod'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5986 entries, 0 to 5985\n",
      "Data columns (total 41 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   gender                                   5986 non-null   int32  \n",
      " 1   SeniorCitizen                            5986 non-null   int64  \n",
      " 2   Partner                                  5986 non-null   int32  \n",
      " 3   Dependents                               5986 non-null   int32  \n",
      " 4   tenure                                   5986 non-null   int64  \n",
      " 5   PhoneService                             5986 non-null   int32  \n",
      " 6   PaperlessBilling                         5986 non-null   int32  \n",
      " 7   Churn                                    5986 non-null   int32  \n",
      " 8   MonthlyCharges                           5986 non-null   float64\n",
      " 9   TotalCharges                             5986 non-null   float64\n",
      " 10  MultipleLines_No                         5986 non-null   uint8  \n",
      " 11  MultipleLines_No phone service           5986 non-null   uint8  \n",
      " 12  MultipleLines_Yes                        5986 non-null   uint8  \n",
      " 13  InternetService_DSL                      5986 non-null   uint8  \n",
      " 14  InternetService_Fiber optic              5986 non-null   uint8  \n",
      " 15  InternetService_No                       5986 non-null   uint8  \n",
      " 16  OnlineSecurity_No                        5986 non-null   uint8  \n",
      " 17  OnlineSecurity_No internet service       5986 non-null   uint8  \n",
      " 18  OnlineSecurity_Yes                       5986 non-null   uint8  \n",
      " 19  OnlineBackup_No                          5986 non-null   uint8  \n",
      " 20  OnlineBackup_No internet service         5986 non-null   uint8  \n",
      " 21  OnlineBackup_Yes                         5986 non-null   uint8  \n",
      " 22  DeviceProtection_No                      5986 non-null   uint8  \n",
      " 23  DeviceProtection_No internet service     5986 non-null   uint8  \n",
      " 24  DeviceProtection_Yes                     5986 non-null   uint8  \n",
      " 25  TechSupport_No                           5986 non-null   uint8  \n",
      " 26  TechSupport_No internet service          5986 non-null   uint8  \n",
      " 27  TechSupport_Yes                          5986 non-null   uint8  \n",
      " 28  StreamingTV_No                           5986 non-null   uint8  \n",
      " 29  StreamingTV_No internet service          5986 non-null   uint8  \n",
      " 30  StreamingTV_Yes                          5986 non-null   uint8  \n",
      " 31  StreamingMovies_No                       5986 non-null   uint8  \n",
      " 32  StreamingMovies_No internet service      5986 non-null   uint8  \n",
      " 33  StreamingMovies_Yes                      5986 non-null   uint8  \n",
      " 34  Contract_Month-to-month                  5986 non-null   uint8  \n",
      " 35  Contract_One year                        5986 non-null   uint8  \n",
      " 36  Contract_Two year                        5986 non-null   uint8  \n",
      " 37  PaymentMethod_Bank transfer (automatic)  5986 non-null   uint8  \n",
      " 38  PaymentMethod_Credit card (automatic)    5986 non-null   uint8  \n",
      " 39  PaymentMethod_Electronic check           5986 non-null   uint8  \n",
      " 40  PaymentMethod_Mailed check               5986 non-null   uint8  \n",
      "dtypes: float64(2), int32(6), int64(2), uint8(31)\n",
      "memory usage: 508.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the target value for the model as the 'Churn' column and the independent variables for the model as the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Churn']\n",
    "X = df.drop('Churn', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the training and test sets\n",
    "### Preprocessing the data by implementing the StandardScaler library\n",
    "### Standardizing the training set first and then using the same parameters that were used to standardize the training set as the parameters that will be used to standardize the test set\n",
    "### The same parameters are being used to standardize both the training and test sets to avoid data leakage issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below cell illustrates the issue with the imbalance of values in the target values for the training set (as mentioned above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values from the original y_train:\n",
      "[[   0 2969]\n",
      " [   1 1041]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train.values, return_counts=True)\n",
    "print('Values from the original y_train:')\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To solve this imbalance of values issue in the target values for the training set, the Random Undersampling method will be used to make the distribution of values to be more equal in the target values for the training set\n",
    "\n",
    "#### Random Undersampling was used because Random Oversampling can result in overfitting for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values from the original y_train:\n",
      "[[   0 2969]\n",
      " [   1 1041]]\n",
      "\n",
      "Values from the original y_train after random undersampling:\n",
      "[[   0 1041]\n",
      " [   1 1041]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "unique, counts = np.unique(y_train.values, return_counts=True)\n",
    "print('Values from the original y_train:')\n",
    "print(np.asarray((unique, counts)).T)\n",
    "print()\n",
    "unique, counts = np.unique(y_train_rus.values, return_counts=True)\n",
    "print('Values from the original y_train after random undersampling:')\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values from y_test:\n",
      "[[   0 1430]\n",
      " [   1  546]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test.values, return_counts=True)\n",
    "print('Values from y_test:')\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV for KNN, Logistic Regression, SVM Rbf Kernel, SVM Poly (degree = 3) Kernel, SVM Linear Kernel, Linear SVM, Decision Tree, and Random Forest Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation strategy that will be used to determine how effective each model is will be determining the models' f1-score values. \n",
    "## Since our dataset and target values are focused on customer churn, we want the company to accurately see which types of customers will churn ('Churn' column having a value of 1) and leave the company.\n",
    "## Thus, we want our model to identify which customers will actually churn as accurate as possible. Additionally, we also want our model to identify the correct customers that will actually churn, and we do not want the model to predict that a significant amount of non-churn customers will churn\n",
    "## If the company knows which types of customers are more likely to churn, then the company can provide these types of customers with different incentives to stay with the company such as providing these customers with retention messages, coupons, discounts, promo deals, etc. to try and persuade these customers to stay with the company. Additionally, we also don't want the company to unnecessarily give these incentives to customers who will not be churning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99951992318771"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors':[1, 5, 10, 15, 20], 'weights': ['uniform', 'distance']}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid=param_grid, cv = 5, scoring='f1')\n",
    "grid_knn.fit(X_train_rus, y_train_rus)\n",
    "grid_knn.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5849802371541502"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 20, 'weights': 'distance'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788615285156442"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>param_weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001197</td>\n",
       "      <td>4.011393e-04</td>\n",
       "      <td>0.025474</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': 'uniform'}</td>\n",
       "      <td>0.732057</td>\n",
       "      <td>0.717445</td>\n",
       "      <td>0.712589</td>\n",
       "      <td>0.736077</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.730358</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>6.248665e-03</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': 'distance'}</td>\n",
       "      <td>0.732057</td>\n",
       "      <td>0.717445</td>\n",
       "      <td>0.712589</td>\n",
       "      <td>0.736077</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.730358</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.024994</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': 'uniform'}</td>\n",
       "      <td>0.728111</td>\n",
       "      <td>0.722611</td>\n",
       "      <td>0.746137</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.751708</td>\n",
       "      <td>0.741713</td>\n",
       "      <td>0.014169</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': 'distance'}</td>\n",
       "      <td>0.744076</td>\n",
       "      <td>0.733496</td>\n",
       "      <td>0.764302</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>0.781176</td>\n",
       "      <td>0.759540</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.026097</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 10, 'weights': 'uniform'}</td>\n",
       "      <td>0.732719</td>\n",
       "      <td>0.737327</td>\n",
       "      <td>0.756264</td>\n",
       "      <td>0.757991</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.749197</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.992081e-04</td>\n",
       "      <td>0.018196</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 10, 'weights': 'distance'}</td>\n",
       "      <td>0.751152</td>\n",
       "      <td>0.758294</td>\n",
       "      <td>0.771689</td>\n",
       "      <td>0.786207</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.014642</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>4.886945e-04</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 15, 'weights': 'uniform'}</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.755940</td>\n",
       "      <td>0.788136</td>\n",
       "      <td>0.740576</td>\n",
       "      <td>0.754339</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 15, 'weights': 'distance'}</td>\n",
       "      <td>0.768889</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.813483</td>\n",
       "      <td>0.774942</td>\n",
       "      <td>0.779586</td>\n",
       "      <td>0.019913</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000401</td>\n",
       "      <td>4.906867e-04</td>\n",
       "      <td>0.029348</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>20</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 20, 'weights': 'uniform'}</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>0.762008</td>\n",
       "      <td>0.016507</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000998</td>\n",
       "      <td>1.784161e-07</td>\n",
       "      <td>0.017225</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>20</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 20, 'weights': 'distance'}</td>\n",
       "      <td>0.776786</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.817778</td>\n",
       "      <td>0.778802</td>\n",
       "      <td>0.788615</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.001197  4.011393e-04         0.025474        0.007009   \n",
       "1       0.003124  6.248665e-03         0.006249        0.007653   \n",
       "2       0.000000  0.000000e+00         0.024994        0.007652   \n",
       "3       0.000000  0.000000e+00         0.015622        0.000002   \n",
       "4       0.000000  0.000000e+00         0.026097        0.008788   \n",
       "5       0.000200  3.992081e-04         0.018196        0.003003   \n",
       "6       0.000599  4.886945e-04         0.027126        0.003878   \n",
       "7       0.000000  0.000000e+00         0.018746        0.006248   \n",
       "8       0.000401  4.906867e-04         0.029348        0.008788   \n",
       "9       0.000998  1.784161e-07         0.017225        0.001627   \n",
       "\n",
       "  param_n_neighbors param_weights                                      params  \\\n",
       "0                 1       uniform    {'n_neighbors': 1, 'weights': 'uniform'}   \n",
       "1                 1      distance   {'n_neighbors': 1, 'weights': 'distance'}   \n",
       "2                 5       uniform    {'n_neighbors': 5, 'weights': 'uniform'}   \n",
       "3                 5      distance   {'n_neighbors': 5, 'weights': 'distance'}   \n",
       "4                10       uniform   {'n_neighbors': 10, 'weights': 'uniform'}   \n",
       "5                10      distance  {'n_neighbors': 10, 'weights': 'distance'}   \n",
       "6                15       uniform   {'n_neighbors': 15, 'weights': 'uniform'}   \n",
       "7                15      distance  {'n_neighbors': 15, 'weights': 'distance'}   \n",
       "8                20       uniform   {'n_neighbors': 20, 'weights': 'uniform'}   \n",
       "9                20      distance  {'n_neighbors': 20, 'weights': 'distance'}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.732057           0.717445           0.712589           0.736077   \n",
       "1           0.732057           0.717445           0.712589           0.736077   \n",
       "2           0.728111           0.722611           0.746137           0.760000   \n",
       "3           0.744076           0.733496           0.764302           0.774648   \n",
       "4           0.732719           0.737327           0.756264           0.757991   \n",
       "5           0.751152           0.758294           0.771689           0.786207   \n",
       "6           0.744186           0.742857           0.755940           0.788136   \n",
       "7           0.768889           0.754098           0.786517           0.813483   \n",
       "8           0.747826           0.750000           0.769231           0.791304   \n",
       "9           0.776786           0.772414           0.797297           0.817778   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.753623         0.730358        0.014556                9  \n",
       "1           0.753623         0.730358        0.014556                9  \n",
       "2           0.751708         0.741713        0.014169                8  \n",
       "3           0.781176         0.759540        0.018091                5  \n",
       "4           0.761682         0.749197        0.011795                7  \n",
       "5           0.787879         0.771044        0.014642                3  \n",
       "6           0.740576         0.754339        0.017715                6  \n",
       "7           0.774942         0.779586        0.019913                2  \n",
       "8           0.751678         0.762008        0.016507                4  \n",
       "9           0.778802         0.788615        0.016879                1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knn_results = pd.DataFrame(grid_knn.cv_results_)\n",
    "df_knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 20, 'weights': 'distance'}</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score                                      params  \\\n",
       "9                1  {'n_neighbors': 20, 'weights': 'distance'}   \n",
       "\n",
       "   mean_test_score  \n",
       "9         0.788615  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knn_results[df_knn_results['rank_test_score'] == 1][['rank_test_score', 'params', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[902 528]\n",
      " [102 444]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.63      0.74      1430\n",
      "           1       0.46      0.81      0.58       546\n",
      "\n",
      "    accuracy                           0.68      1976\n",
      "   macro avg       0.68      0.72      0.66      1976\n",
      "weighted avg       0.78      0.68      0.70      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_knn_train_predict = grid_knn.predict(X_train_rus)\n",
    "y_knn_predict = grid_knn.predict(X_test)\n",
    "knn_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_knn_predict)\n",
    "knn_classification_report = classification_report(y_true = y_test, y_pred = y_knn_predict)\n",
    "print(knn_cmatrix)\n",
    "print()\n",
    "print(knn_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.99951992318771\n",
      "Test F1-Score: 0.5849802371541502\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=20, weights='distance')\n",
    "knn_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=knn_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=knn_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table = [['KNN', \"n_neighbors = 20, weights = 'distance'\", f1_score(y_true=y_train_rus, y_pred=knn_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=knn_classifier.predict(X_test)),\n",
    "               df_knn_results[df_knn_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.99952</td>\n",
       "      <td>0.58498</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classification Model                         Model Parameter  \\\n",
       "0                  KNN  n_neighbors = 20, weights = 'distance'   \n",
       "\n",
       "   Train F1-Score  Test F1-Score  Best Mean Test F1-Score  \n",
       "0         0.99952        0.58498                 0.788615  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7824897400820793"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'C':[0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear',multi_class='auto', random_state=0)\n",
    "\n",
    "grid_log = GridSearchCV(log_reg, param_grid = param_grid, cv = 5, scoring = 'f1')\n",
    "grid_log.fit(X_train_rus, y_train_rus)\n",
    "grid_log.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6097388849682428"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779720276930007"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_log.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>4.194451e-03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>0.767932</td>\n",
       "      <td>0.755187</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.786008</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>7.979876e-04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l2'}</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.780045</td>\n",
       "      <td>0.783964</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.779720</td>\n",
       "      <td>0.017353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008776</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>6.468134e-07</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.773455</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.775120</td>\n",
       "      <td>0.773064</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>3.984693e-04</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.753488</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.780045</td>\n",
       "      <td>0.795556</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>0.776378</td>\n",
       "      <td>0.013877</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035072</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>4.883052e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>0.752887</td>\n",
       "      <td>0.778555</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.777070</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>3.991609e-04</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.783599</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.777348</td>\n",
       "      <td>0.015463</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.252954</td>\n",
       "      <td>0.083336</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>7.985362e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.756381</td>\n",
       "      <td>0.772093</td>\n",
       "      <td>0.779043</td>\n",
       "      <td>0.795506</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.777208</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017552</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>4.920712e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.756381</td>\n",
       "      <td>0.772093</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.795506</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.777188</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.227379</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3.997803e-04</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 100, 'penalty': 'l1'}</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.795506</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.777767</td>\n",
       "      <td>0.012442</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.011689</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>6.248665e-03</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 100, 'penalty': 'l2'}</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.794582</td>\n",
       "      <td>0.783019</td>\n",
       "      <td>0.777224</td>\n",
       "      <td>0.012445</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.003683      0.002356         0.003812    4.194451e-03    0.01   \n",
       "1       0.005784      0.000399         0.001596    7.979876e-04    0.01   \n",
       "2       0.008776      0.002309         0.000997    6.468134e-07     0.1   \n",
       "3       0.008377      0.001197         0.001197    3.984693e-04     0.1   \n",
       "4       0.035072      0.006707         0.001596    4.883052e-04       1   \n",
       "5       0.011369      0.000489         0.001796    3.991609e-04       1   \n",
       "6       0.252954      0.083336         0.000599    7.985362e-04      10   \n",
       "7       0.017552      0.001953         0.001400    4.920712e-04      10   \n",
       "8       0.227379      0.025247         0.000200    3.997803e-04     100   \n",
       "9       0.012497      0.011689         0.003124    6.248665e-03     100   \n",
       "\n",
       "  param_penalty                        params  split0_test_score  \\\n",
       "0            l1  {'C': 0.01, 'penalty': 'l1'}           0.767932   \n",
       "1            l2  {'C': 0.01, 'penalty': 'l2'}           0.747706   \n",
       "2            l1   {'C': 0.1, 'penalty': 'l1'}           0.750000   \n",
       "3            l2   {'C': 0.1, 'penalty': 'l2'}           0.753488   \n",
       "4            l1     {'C': 1, 'penalty': 'l1'}           0.752887   \n",
       "5            l2     {'C': 1, 'penalty': 'l2'}           0.750000   \n",
       "6            l1    {'C': 10, 'penalty': 'l1'}           0.756381   \n",
       "7            l2    {'C': 10, 'penalty': 'l2'}           0.756381   \n",
       "8            l1   {'C': 100, 'penalty': 'l1'}           0.759259   \n",
       "9            l2   {'C': 100, 'penalty': 'l2'}           0.759259   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.755187           0.775000           0.786008           0.775862   \n",
       "1           0.780045           0.783964           0.800000           0.786885   \n",
       "2           0.766744           0.773455           0.800000           0.775120   \n",
       "3           0.771028           0.780045           0.795556           0.781775   \n",
       "4           0.778555           0.781818           0.791946           0.780142   \n",
       "5           0.775701           0.783599           0.797297           0.780142   \n",
       "6           0.772093           0.779043           0.795506           0.783019   \n",
       "7           0.772093           0.781818           0.795506           0.780142   \n",
       "8           0.769231           0.781818           0.795506           0.783019   \n",
       "9           0.767442           0.781818           0.794582           0.783019   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.771998        0.010192               10  \n",
       "1         0.779720        0.017353                1  \n",
       "2         0.773064        0.016136                9  \n",
       "3         0.776378        0.013877                8  \n",
       "4         0.777070        0.012964                7  \n",
       "5         0.777348        0.015463                3  \n",
       "6         0.777208        0.012899                5  \n",
       "7         0.777188        0.012842                6  \n",
       "8         0.777767        0.012442                2  \n",
       "9         0.777224        0.012445                4  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_results = pd.DataFrame(grid_log.cv_results_)\n",
    "df_log_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l2'}</td>\n",
       "      <td>0.77972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score                        params  mean_test_score\n",
       "1                1  {'C': 0.01, 'penalty': 'l2'}          0.77972"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_results[df_log_results['rank_test_score'] == 1][['rank_test_score', 'params', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[991 439]\n",
      " [114 432]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78      1430\n",
      "           1       0.50      0.79      0.61       546\n",
      "\n",
      "    accuracy                           0.72      1976\n",
      "   macro avg       0.70      0.74      0.70      1976\n",
      "weighted avg       0.79      0.72      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_log_reg_train_predict = grid_log.predict(X_train_rus)\n",
    "y_log_reg_predict = grid_log.predict(X_test)\n",
    "log_reg_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_log_reg_predict)\n",
    "log_reg_classification_report = classification_report(y_true = y_test, y_pred = y_log_reg_predict)\n",
    "print(log_reg_cmatrix)\n",
    "print()\n",
    "print(log_reg_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.7824897400820793\n",
      "Test F1-Score: 0.6097388849682428\n"
     ]
    }
   ],
   "source": [
    "log_reg_classifier = LogisticRegression(solver='liblinear',multi_class='auto', random_state=0, C=0.01, penalty='l2')\n",
    "log_reg_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=log_reg_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=log_reg_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [['Logistic Regression', \"C = 0.01, penalty = 'l2'\", f1_score(y_true=y_train_rus, y_pred=log_reg_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=log_reg_classifier.predict(X_test)),\n",
    "                 df_log_results[df_log_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.99952</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.78249</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classification Model                         Model Parameter  \\\n",
       "0                  KNN  n_neighbors = 20, weights = 'distance'   \n",
       "1  Logistic Regression                C = 0.01, penalty = 'l2'   \n",
       "\n",
       "   Train F1-Score  Test F1-Score  Best Mean Test F1-Score  \n",
       "0         0.99952       0.584980                 0.788615  \n",
       "1         0.78249       0.609739                 0.779720  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (kernel = 'rbf') Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.75700611 0.75802842        nan 0.77090895 0.76964639        nan\n",
      " 0.77571766 0.77647656        nan 0.74625692 0.75177623        nan\n",
      " 0.718476   0.71746781        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8190563444800734"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc_rbf = svm.SVC(kernel='rbf', random_state=0) \n",
    "param_grid = {'C':[0.01, 0.1, 1, 10, 100], 'gamma': ['scale', 'auto', '0.5']}\n",
    "\n",
    "grid_rbf = GridSearchCV(svc_rbf, param_grid = param_grid, cv = 5, n_jobs = -1, scoring='f1')\n",
    "grid_rbf.fit(X_train_rus, y_train_rus)\n",
    "grid_rbf.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6068675543097407"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rbf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 'auto'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rbf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7764765643671859"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rbf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.221407</td>\n",
       "      <td>4.278684e-03</td>\n",
       "      <td>0.318946</td>\n",
       "      <td>0.220483</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 0.01, 'gamma': 'scale'}</td>\n",
       "      <td>0.738041</td>\n",
       "      <td>0.766440</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.757006</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.227788</td>\n",
       "      <td>6.167958e-03</td>\n",
       "      <td>0.312963</td>\n",
       "      <td>0.207089</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 0.01, 'gamma': 'auto'}</td>\n",
       "      <td>0.740909</td>\n",
       "      <td>0.766440</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.764192</td>\n",
       "      <td>0.770302</td>\n",
       "      <td>0.758028</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001994</td>\n",
       "      <td>5.135693e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.01, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.158973</td>\n",
       "      <td>3.969724e-03</td>\n",
       "      <td>0.163962</td>\n",
       "      <td>0.124014</td>\n",
       "      <td>0.1</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 0.1, 'gamma': 'scale'}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.803532</td>\n",
       "      <td>0.765661</td>\n",
       "      <td>0.770909</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.167749</td>\n",
       "      <td>6.978608e-03</td>\n",
       "      <td>0.175730</td>\n",
       "      <td>0.126595</td>\n",
       "      <td>0.1</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 0.1, 'gamma': 'auto'}</td>\n",
       "      <td>0.748268</td>\n",
       "      <td>0.766147</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.803532</td>\n",
       "      <td>0.762791</td>\n",
       "      <td>0.769646</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003190</td>\n",
       "      <td>7.448751e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.1, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.166750</td>\n",
       "      <td>5.726585e-03</td>\n",
       "      <td>0.192884</td>\n",
       "      <td>0.125760</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 1, 'gamma': 'scale'}</td>\n",
       "      <td>0.751740</td>\n",
       "      <td>0.760820</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.811530</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.775718</td>\n",
       "      <td>0.021669</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.162763</td>\n",
       "      <td>1.113065e-02</td>\n",
       "      <td>0.202259</td>\n",
       "      <td>0.135985</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 1, 'gamma': 'auto'}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.760820</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.811530</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.776477</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002594</td>\n",
       "      <td>7.970023e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 1, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.215024</td>\n",
       "      <td>7.152925e-03</td>\n",
       "      <td>0.144015</td>\n",
       "      <td>0.111638</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 10, 'gamma': 'scale'}</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.737819</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.772182</td>\n",
       "      <td>0.746257</td>\n",
       "      <td>0.022123</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.228188</td>\n",
       "      <td>5.101359e-03</td>\n",
       "      <td>0.196475</td>\n",
       "      <td>0.129890</td>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 10, 'gamma': 'auto'}</td>\n",
       "      <td>0.730594</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.778281</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.751776</td>\n",
       "      <td>0.021795</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002792</td>\n",
       "      <td>3.986127e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 10, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.328120</td>\n",
       "      <td>1.315658e-02</td>\n",
       "      <td>0.169547</td>\n",
       "      <td>0.112524</td>\n",
       "      <td>100</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 100, 'gamma': 'scale'}</td>\n",
       "      <td>0.694836</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.724221</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.718476</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.334777</td>\n",
       "      <td>2.440762e-02</td>\n",
       "      <td>0.181191</td>\n",
       "      <td>0.117932</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 100, 'gamma': 'auto'}</td>\n",
       "      <td>0.694836</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.725537</td>\n",
       "      <td>0.735084</td>\n",
       "      <td>0.724566</td>\n",
       "      <td>0.717468</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002593</td>\n",
       "      <td>4.885949e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 100, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.221407  4.278684e-03         0.318946        0.220483    0.01   \n",
       "1        0.227788  6.167958e-03         0.312963        0.207089    0.01   \n",
       "2        0.001994  5.135693e-07         0.000000        0.000000    0.01   \n",
       "3        0.158973  3.969724e-03         0.163962        0.124014     0.1   \n",
       "4        0.167749  6.978608e-03         0.175730        0.126595     0.1   \n",
       "5        0.003190  7.448751e-04         0.000000        0.000000     0.1   \n",
       "6        0.166750  5.726585e-03         0.192884        0.125760       1   \n",
       "7        0.162763  1.113065e-02         0.202259        0.135985       1   \n",
       "8        0.002594  7.970023e-04         0.000000        0.000000       1   \n",
       "9        0.215024  7.152925e-03         0.144015        0.111638      10   \n",
       "10       0.228188  5.101359e-03         0.196475        0.129890      10   \n",
       "11       0.002792  3.986127e-04         0.000000        0.000000      10   \n",
       "12       0.328120  1.315658e-02         0.169547        0.112524     100   \n",
       "13       0.334777  2.440762e-02         0.181191        0.117932     100   \n",
       "14       0.002593  4.885949e-04         0.000000        0.000000     100   \n",
       "\n",
       "   param_gamma                         params  split0_test_score  \\\n",
       "0        scale  {'C': 0.01, 'gamma': 'scale'}           0.738041   \n",
       "1         auto   {'C': 0.01, 'gamma': 'auto'}           0.740909   \n",
       "2          0.5    {'C': 0.01, 'gamma': '0.5'}                NaN   \n",
       "3        scale   {'C': 0.1, 'gamma': 'scale'}           0.750000   \n",
       "4         auto    {'C': 0.1, 'gamma': 'auto'}           0.748268   \n",
       "5          0.5     {'C': 0.1, 'gamma': '0.5'}                NaN   \n",
       "6        scale     {'C': 1, 'gamma': 'scale'}           0.751740   \n",
       "7         auto      {'C': 1, 'gamma': 'auto'}           0.750000   \n",
       "8          0.5       {'C': 1, 'gamma': '0.5'}                NaN   \n",
       "9        scale    {'C': 10, 'gamma': 'scale'}           0.722222   \n",
       "10        auto     {'C': 10, 'gamma': 'auto'}           0.730594   \n",
       "11         0.5      {'C': 10, 'gamma': '0.5'}                NaN   \n",
       "12       scale   {'C': 100, 'gamma': 'scale'}           0.694836   \n",
       "13        auto    {'C': 100, 'gamma': 'auto'}           0.694836   \n",
       "14         0.5     {'C': 100, 'gamma': '0.5'}                NaN   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.766440           0.745455           0.765864   \n",
       "1            0.766440           0.748299           0.764192   \n",
       "2                 NaN                NaN                NaN   \n",
       "3            0.767857           0.767494           0.803532   \n",
       "4            0.766147           0.767494           0.803532   \n",
       "5                 NaN                NaN                NaN   \n",
       "6            0.760820           0.765766           0.811530   \n",
       "7            0.760820           0.771300           0.811530   \n",
       "8                 NaN                NaN                NaN   \n",
       "9            0.737819           0.726027           0.773034   \n",
       "10           0.746544           0.727273           0.778281   \n",
       "11                NaN                NaN                NaN   \n",
       "12           0.707317           0.724221           0.733333   \n",
       "13           0.707317           0.725537           0.735084   \n",
       "14                NaN                NaN                NaN   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.769231         0.757006        0.012728                6  \n",
       "1            0.770302         0.758028        0.011376                5  \n",
       "2                 NaN              NaN             NaN               11  \n",
       "3            0.765661         0.770909        0.017607                3  \n",
       "4            0.762791         0.769646        0.018271                4  \n",
       "5                 NaN              NaN             NaN               12  \n",
       "6            0.788732         0.775718        0.021669                2  \n",
       "7            0.788732         0.776477        0.021688                1  \n",
       "8                 NaN              NaN             NaN               13  \n",
       "9            0.772182         0.746257        0.022123                8  \n",
       "10           0.776190         0.751776        0.021795                7  \n",
       "11                NaN              NaN             NaN               14  \n",
       "12           0.732673         0.718476        0.015091                9  \n",
       "13           0.724566         0.717468        0.014430               10  \n",
       "14                NaN              NaN             NaN               15  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_rbf_results = pd.DataFrame(grid_rbf.cv_results_)\n",
    "df_grid_rbf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1, 'gamma': 'auto'}</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_test_score                     params  mean_test_score\n",
       "7                1  {'C': 1, 'gamma': 'auto'}         0.776477"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_rbf_results[df_grid_rbf_results['rank_test_score'] == 1][['rank_test_score', 'params', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[982 448]\n",
      " [113 433]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78      1430\n",
      "           1       0.49      0.79      0.61       546\n",
      "\n",
      "    accuracy                           0.72      1976\n",
      "   macro avg       0.69      0.74      0.69      1976\n",
      "weighted avg       0.78      0.72      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_rbf_train_predict = grid_rbf.predict(X_train_rus)\n",
    "y_rbf_predict = grid_rbf.predict(X_test)\n",
    "rbf_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_rbf_predict)\n",
    "rbf_classification_report = classification_report(y_true = y_test, y_pred = y_rbf_predict)\n",
    "print(rbf_cmatrix)\n",
    "print()\n",
    "print(rbf_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.8190563444800734\n",
      "Test F1-Score: 0.6068675543097407\n"
     ]
    }
   ],
   "source": [
    "rbf_classifier = svm.SVC(kernel='rbf', C=1, gamma='auto', random_state=0) \n",
    "rbf_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=rbf_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=rbf_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"SVM (kernel='rbf')\", \"C = 1, gamma = 'auto'\", f1_score(y_true=y_train_rus, y_pred=rbf_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=rbf_classifier.predict(X_test)),\n",
    "                 df_grid_rbf_results[df_grid_rbf_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classification Model                         Model Parameter  \\\n",
       "0                  KNN  n_neighbors = 20, weights = 'distance'   \n",
       "1  Logistic Regression                C = 0.01, penalty = 'l2'   \n",
       "2   SVM (kernel='rbf')                   C = 1, gamma = 'auto'   \n",
       "\n",
       "   Train F1-Score  Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.999520       0.584980                 0.788615  \n",
       "1        0.782490       0.609739                 0.779720  \n",
       "2        0.819056       0.606868                 0.776477  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (kernel = 'poly', degree = 3) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.70821391 0.70535071        nan 0.76344705 0.75976307        nan\n",
      " 0.77090996 0.77074046        nan 0.72450685 0.73078961        nan\n",
      " 0.70792952 0.71465627        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8351548269581057"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc_poly = svm.SVC(kernel='poly', degree = 3, random_state=0) \n",
    "param_grid = {'C':[0.01, 0.1, 1, 10, 100], 'gamma': ['scale', 'auto', '0.5']}\n",
    "\n",
    "grid_svcPoly = GridSearchCV(svc_poly, param_grid = param_grid, cv = 5, n_jobs = -1, scoring='f1')\n",
    "grid_svcPoly.fit(X_train_rus, y_train_rus)\n",
    "grid_svcPoly.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6027586206896552"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svcPoly.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 'scale'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svcPoly.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7709099582710408"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svcPoly.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186952</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.042888</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 0.01, 'gamma': 'scale'}</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.701627</td>\n",
       "      <td>0.710623</td>\n",
       "      <td>0.700885</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.708214</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.216549</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0.045679</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 0.01, 'gamma': 'auto'}</td>\n",
       "      <td>0.699454</td>\n",
       "      <td>0.697842</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.699647</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.705351</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.01, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159754</td>\n",
       "      <td>0.018823</td>\n",
       "      <td>0.038621</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>0.1</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 0.1, 'gamma': 'scale'}</td>\n",
       "      <td>0.743697</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.765073</td>\n",
       "      <td>0.773931</td>\n",
       "      <td>0.780911</td>\n",
       "      <td>0.763447</td>\n",
       "      <td>0.013447</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.166323</td>\n",
       "      <td>0.010318</td>\n",
       "      <td>0.036313</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.1</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 0.1, 'gamma': 'auto'}</td>\n",
       "      <td>0.746331</td>\n",
       "      <td>0.747433</td>\n",
       "      <td>0.757202</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.780172</td>\n",
       "      <td>0.759763</td>\n",
       "      <td>0.012792</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.1, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.136123</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.031824</td>\n",
       "      <td>0.009102</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 1, 'gamma': 'scale'}</td>\n",
       "      <td>0.758465</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.755056</td>\n",
       "      <td>0.807947</td>\n",
       "      <td>0.789838</td>\n",
       "      <td>0.770910</td>\n",
       "      <td>0.024090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.139324</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.024994</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>1</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 1, 'gamma': 'auto'}</td>\n",
       "      <td>0.752834</td>\n",
       "      <td>0.750562</td>\n",
       "      <td>0.756152</td>\n",
       "      <td>0.807947</td>\n",
       "      <td>0.786207</td>\n",
       "      <td>0.770740</td>\n",
       "      <td>0.022646</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 1, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.183834</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.021871</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>10</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 10, 'gamma': 'scale'}</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.725173</td>\n",
       "      <td>0.700696</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.755344</td>\n",
       "      <td>0.724507</td>\n",
       "      <td>0.021331</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.189088</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.008724</td>\n",
       "      <td>10</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 10, 'gamma': 'auto'}</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.729792</td>\n",
       "      <td>0.697460</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.769953</td>\n",
       "      <td>0.730790</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 10, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.417527</td>\n",
       "      <td>0.049318</td>\n",
       "      <td>0.023780</td>\n",
       "      <td>0.006051</td>\n",
       "      <td>100</td>\n",
       "      <td>scale</td>\n",
       "      <td>{'C': 100, 'gamma': 'scale'}</td>\n",
       "      <td>0.688679</td>\n",
       "      <td>0.698337</td>\n",
       "      <td>0.696262</td>\n",
       "      <td>0.726841</td>\n",
       "      <td>0.729529</td>\n",
       "      <td>0.707930</td>\n",
       "      <td>0.016869</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.367040</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.025333</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>100</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'C': 100, 'gamma': 'auto'}</td>\n",
       "      <td>0.702326</td>\n",
       "      <td>0.701422</td>\n",
       "      <td>0.705336</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.730864</td>\n",
       "      <td>0.714656</td>\n",
       "      <td>0.014322</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 100, 'gamma': '0.5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.186952      0.007098         0.042888        0.003512    0.01   \n",
       "1        0.216549      0.004773         0.045679        0.004653    0.01   \n",
       "2        0.003789      0.001596         0.000000        0.000000    0.01   \n",
       "3        0.159754      0.018823         0.038621        0.006705     0.1   \n",
       "4        0.166323      0.010318         0.036313        0.002501     0.1   \n",
       "5        0.002593      0.000489         0.000000        0.000000     0.1   \n",
       "6        0.136123      0.003474         0.031824        0.009102       1   \n",
       "7        0.139324      0.005805         0.024994        0.007653       1   \n",
       "8        0.000000      0.000000         0.000000        0.000000       1   \n",
       "9        0.183834      0.006076         0.021871        0.007653      10   \n",
       "10       0.189088      0.007060         0.026045        0.008724      10   \n",
       "11       0.000000      0.000000         0.000000        0.000000      10   \n",
       "12       0.417527      0.049318         0.023780        0.006051     100   \n",
       "13       0.367040      0.021067         0.025333        0.010753     100   \n",
       "14       0.002195      0.000398         0.000000        0.000000     100   \n",
       "\n",
       "   param_gamma                         params  split0_test_score  \\\n",
       "0        scale  {'C': 0.01, 'gamma': 'scale'}           0.703297   \n",
       "1         auto   {'C': 0.01, 'gamma': 'auto'}           0.699454   \n",
       "2          0.5    {'C': 0.01, 'gamma': '0.5'}                NaN   \n",
       "3        scale   {'C': 0.1, 'gamma': 'scale'}           0.743697   \n",
       "4         auto    {'C': 0.1, 'gamma': 'auto'}           0.746331   \n",
       "5          0.5     {'C': 0.1, 'gamma': '0.5'}                NaN   \n",
       "6        scale     {'C': 1, 'gamma': 'scale'}           0.758465   \n",
       "7         auto      {'C': 1, 'gamma': 'auto'}           0.752834   \n",
       "8          0.5       {'C': 1, 'gamma': '0.5'}                NaN   \n",
       "9        scale    {'C': 10, 'gamma': 'scale'}           0.701595   \n",
       "10        auto     {'C': 10, 'gamma': 'auto'}           0.701595   \n",
       "11         0.5      {'C': 10, 'gamma': '0.5'}                NaN   \n",
       "12       scale   {'C': 100, 'gamma': 'scale'}           0.688679   \n",
       "13        auto    {'C': 100, 'gamma': 'auto'}           0.702326   \n",
       "14         0.5     {'C': 100, 'gamma': '0.5'}                NaN   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.701627           0.710623           0.700885   \n",
       "1            0.697842           0.709091           0.699647   \n",
       "2                 NaN                NaN                NaN   \n",
       "3            0.753623           0.765073           0.773931   \n",
       "4            0.747433           0.757202           0.767677   \n",
       "5                 NaN                NaN                NaN   \n",
       "6            0.743243           0.755056           0.807947   \n",
       "7            0.750562           0.756152           0.807947   \n",
       "8                 NaN                NaN                NaN   \n",
       "9            0.725173           0.700696           0.739726   \n",
       "10           0.729792           0.697460           0.755149   \n",
       "11                NaN                NaN                NaN   \n",
       "12           0.698337           0.696262           0.726841   \n",
       "13           0.701422           0.705336           0.733333   \n",
       "14                NaN                NaN                NaN   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.724638         0.708214        0.008909                8  \n",
       "1            0.720721         0.705351        0.008648               10  \n",
       "2                 NaN              NaN             NaN               11  \n",
       "3            0.780911         0.763447        0.013447                3  \n",
       "4            0.780172         0.759763        0.012792                4  \n",
       "5                 NaN              NaN             NaN               12  \n",
       "6            0.789838         0.770910        0.024090                1  \n",
       "7            0.786207         0.770740        0.022646                2  \n",
       "8                 NaN              NaN             NaN               13  \n",
       "9            0.755344         0.724507        0.021331                6  \n",
       "10           0.769953         0.730790        0.028606                5  \n",
       "11                NaN              NaN             NaN               14  \n",
       "12           0.729529         0.707930        0.016869                9  \n",
       "13           0.730864         0.714656        0.014322                7  \n",
       "14                NaN              NaN             NaN               15  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_svcPoly_results = pd.DataFrame(grid_svcPoly.cv_results_)\n",
    "df_grid_svcPoly_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[963 467]\n",
      " [109 437]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.67      0.77      1430\n",
      "           1       0.48      0.80      0.60       546\n",
      "\n",
      "    accuracy                           0.71      1976\n",
      "   macro avg       0.69      0.74      0.69      1976\n",
      "weighted avg       0.78      0.71      0.72      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_svcPoly_train_predict = grid_svcPoly.predict(X_train_rus)\n",
    "y_svcPoly_predict = grid_svcPoly.predict(X_test)\n",
    "svcPoly_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_svcPoly_predict)\n",
    "svcPoly_classification_report = classification_report(y_true = y_test, y_pred = y_svcPoly_predict)\n",
    "print(svcPoly_cmatrix)\n",
    "print()\n",
    "print(svcPoly_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.8351548269581057\n",
      "Test F1-Score: 0.6027586206896552\n"
     ]
    }
   ],
   "source": [
    "svcPoly_classifier = svm.SVC(kernel='poly', degree = 3, C=1, gamma='scale', random_state=0) \n",
    "svcPoly_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=svcPoly_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=svcPoly_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"SVM (kernel='poly', degree = 3)\", \"C = 1, gamma = 'scale'\", f1_score(y_true=y_train_rus, y_pred=svcPoly_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=svcPoly_classifier.predict(X_test)),\n",
    "                 df_grid_svcPoly_results[df_grid_svcPoly_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classification Model                         Model Parameter  \\\n",
       "0                              KNN  n_neighbors = 20, weights = 'distance'   \n",
       "1              Logistic Regression                C = 0.01, penalty = 'l2'   \n",
       "2               SVM (kernel='rbf')                   C = 1, gamma = 'auto'   \n",
       "3  SVM (kernel='poly', degree = 3)                  C = 1, gamma = 'scale'   \n",
       "\n",
       "   Train F1-Score  Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.999520       0.584980                 0.788615  \n",
       "1        0.782490       0.609739                 0.779720  \n",
       "2        0.819056       0.606868                 0.776477  \n",
       "3        0.835155       0.602759                 0.770910  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (kernel = 'linear') Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7703435804701627"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc_linear = svm.SVC(kernel='linear', random_state=0) \n",
    "param_grid = {'C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_svcLinear = GridSearchCV(svc_linear, param_grid = param_grid, cv = 5, n_jobs = -1, scoring='f1')\n",
    "grid_svcLinear.fit(X_train_rus, y_train_rus)\n",
    "grid_svcLinear.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6131284916201117"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svcLinear.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svcLinear.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7670116394109571"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svcLinear.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.115597</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>0.739910</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.774336</td>\n",
       "      <td>0.781857</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.761640</td>\n",
       "      <td>0.017967</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156214</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.739910</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.768889</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.760083</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.308891</td>\n",
       "      <td>0.045468</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.739910</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.776786</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.763220</td>\n",
       "      <td>0.019003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.389562</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>0.023932</td>\n",
       "      <td>0.007057</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.741419</td>\n",
       "      <td>0.741722</td>\n",
       "      <td>0.775056</td>\n",
       "      <td>0.786813</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.765366</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.453059</td>\n",
       "      <td>2.215521</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.741419</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>0.775056</td>\n",
       "      <td>0.785088</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.767012</td>\n",
       "      <td>0.017325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.115597      0.007654         0.021870        0.007653    0.01   \n",
       "1       0.156214      0.019760         0.018746        0.006248     0.1   \n",
       "2       0.308891      0.045468         0.021870        0.007653       1   \n",
       "3       1.389562      0.083319         0.023932        0.007057      10   \n",
       "4      10.453059      2.215521         0.020079        0.005602     100   \n",
       "\n",
       "        params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.01}           0.739910           0.740088           0.774336   \n",
       "1   {'C': 0.1}           0.739910           0.740088           0.768889   \n",
       "2     {'C': 1}           0.739910           0.740088           0.776786   \n",
       "3    {'C': 10}           0.741419           0.741722           0.775056   \n",
       "4   {'C': 100}           0.741419           0.751678           0.775056   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.781857           0.772009         0.761640        0.017967   \n",
       "1           0.778495           0.773034         0.760083        0.016679   \n",
       "2           0.778495           0.780822         0.763220        0.019003   \n",
       "3           0.786813           0.781818         0.765366        0.019784   \n",
       "4           0.785088           0.781818         0.767012        0.017325   \n",
       "\n",
       "   rank_test_score  \n",
       "0                4  \n",
       "1                5  \n",
       "2                3  \n",
       "3                2  \n",
       "4                1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_svcLinear_results = pd.DataFrame(grid_svcLinear.cv_results_)\n",
    "df_grid_svcLinear_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[983 447]\n",
      " [107 439]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78      1430\n",
      "           1       0.50      0.80      0.61       546\n",
      "\n",
      "    accuracy                           0.72      1976\n",
      "   macro avg       0.70      0.75      0.70      1976\n",
      "weighted avg       0.79      0.72      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_svcLinear_train_predict = grid_svcLinear.predict(X_train_rus)\n",
    "y_svcLinear_predict = grid_svcLinear.predict(X_test)\n",
    "svcLinear_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_svcLinear_predict)\n",
    "svcLinear_classification_report = classification_report(y_true = y_test, y_pred = y_svcLinear_predict)\n",
    "print(svcLinear_cmatrix)\n",
    "print()\n",
    "print(svcLinear_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.7703435804701627\n",
      "Test F1-Score: 0.6131284916201117\n"
     ]
    }
   ],
   "source": [
    "svcLinear_classifier = svm.SVC(kernel='linear', C=100, random_state=0) \n",
    "svcLinear_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=svcLinear_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=svcLinear_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"SVM (kernel='linear')\", 'C = 100', f1_score(y_true=y_train_rus, y_pred=svcLinear_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=svcLinear_classifier.predict(X_test)),\n",
    "                 df_grid_svcLinear_results[df_grid_svcLinear_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classification Model                         Model Parameter  \\\n",
       "0                              KNN  n_neighbors = 20, weights = 'distance'   \n",
       "1              Logistic Regression                C = 0.01, penalty = 'l2'   \n",
       "2               SVM (kernel='rbf')                   C = 1, gamma = 'auto'   \n",
       "3  SVM (kernel='poly', degree = 3)                  C = 1, gamma = 'scale'   \n",
       "4            SVM (kernel='linear')                                 C = 100   \n",
       "\n",
       "   Train F1-Score  Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.999520       0.584980                 0.788615  \n",
       "1        0.782490       0.609739                 0.779720  \n",
       "2        0.819056       0.606868                 0.776477  \n",
       "3        0.835155       0.602759                 0.770910  \n",
       "4        0.770344       0.613128                 0.767012  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 234, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 974, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 830, in _get_liblinear_solver_type\n",
      "    raise ValueError('Unsupported set of arguments: %s, '\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.77743269        nan 0.77222297        nan 0.7763123\n",
      "        nan 0.77632301        nan 0.7690518         nan 0.66381387]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7835144927536233"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svc = LinearSVC(random_state=0)\n",
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "grid_lin_svc = GridSearchCV(lin_svc, param_grid, cv = 5, scoring='f1')\n",
    "grid_lin_svc.fit(X_train_rus, y_train_rus)\n",
    "grid_lin_svc.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6026629292221443"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lin_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lin_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7774326943702261"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lin_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.801762</td>\n",
       "      <td>0.783410</td>\n",
       "      <td>0.777433</td>\n",
       "      <td>0.017451</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009342</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l2'}</td>\n",
       "      <td>0.752887</td>\n",
       "      <td>0.771363</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.788419</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.012128</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.008138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.780045</td>\n",
       "      <td>0.798226</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.776312</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.171790</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.755760</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.796421</td>\n",
       "      <td>0.779097</td>\n",
       "      <td>0.776323</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.747100</td>\n",
       "      <td>0.772834</td>\n",
       "      <td>0.770975</td>\n",
       "      <td>0.776573</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.769052</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 100, 'penalty': 'l1'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.170607</td>\n",
       "      <td>0.011297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 100, 'penalty': 'l2'}</td>\n",
       "      <td>0.663342</td>\n",
       "      <td>0.693780</td>\n",
       "      <td>0.604534</td>\n",
       "      <td>0.590529</td>\n",
       "      <td>0.766885</td>\n",
       "      <td>0.663814</td>\n",
       "      <td>0.063885</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.000616      0.001231         0.000000        0.000000   0.001   \n",
       "1        0.006250      0.007654         0.000000        0.000000   0.001   \n",
       "2        0.000000      0.000000         0.000000        0.000000    0.01   \n",
       "3        0.009342      0.006106         0.000399        0.000489    0.01   \n",
       "4        0.000997      0.000002         0.000000        0.000000     0.1   \n",
       "5        0.021346      0.008138         0.000000        0.000000     0.1   \n",
       "6        0.000399      0.000489         0.000000        0.000000       1   \n",
       "7        0.171790      0.010642         0.003125        0.006249       1   \n",
       "8        0.000000      0.000000         0.000000        0.000000      10   \n",
       "9        0.170600      0.005173         0.000000        0.000000      10   \n",
       "10       0.000000      0.000000         0.000000        0.000000     100   \n",
       "11       0.170607      0.011297         0.000000        0.000000     100   \n",
       "\n",
       "   param_penalty                         params  split0_test_score  \\\n",
       "0             l1  {'C': 0.001, 'penalty': 'l1'}                NaN   \n",
       "1             l2  {'C': 0.001, 'penalty': 'l2'}           0.747706   \n",
       "2             l1   {'C': 0.01, 'penalty': 'l1'}                NaN   \n",
       "3             l2   {'C': 0.01, 'penalty': 'l2'}           0.752887   \n",
       "4             l1    {'C': 0.1, 'penalty': 'l1'}                NaN   \n",
       "5             l2    {'C': 0.1, 'penalty': 'l2'}           0.750000   \n",
       "6             l1      {'C': 1, 'penalty': 'l1'}                NaN   \n",
       "7             l2      {'C': 1, 'penalty': 'l2'}           0.755760   \n",
       "8             l1     {'C': 10, 'penalty': 'l1'}                NaN   \n",
       "9             l2     {'C': 10, 'penalty': 'l2'}           0.747100   \n",
       "10            l1    {'C': 100, 'penalty': 'l1'}                NaN   \n",
       "11            l2    {'C': 100, 'penalty': 'l2'}           0.663342   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0                 NaN                NaN                NaN   \n",
       "1            0.774775           0.779510           0.801762   \n",
       "2                 NaN                NaN                NaN   \n",
       "3            0.771363           0.767494           0.788419   \n",
       "4                 NaN                NaN                NaN   \n",
       "5            0.773148           0.780045           0.798226   \n",
       "6                 NaN                NaN                NaN   \n",
       "7            0.768519           0.781818           0.796421   \n",
       "8                 NaN                NaN                NaN   \n",
       "9            0.772834           0.770975           0.776573   \n",
       "10                NaN                NaN                NaN   \n",
       "11           0.693780           0.604534           0.590529   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0                 NaN              NaN             NaN                7  \n",
       "1            0.783410         0.777433        0.017451                1  \n",
       "2                 NaN              NaN             NaN                8  \n",
       "3            0.780952         0.772223        0.012128                4  \n",
       "4                 NaN              NaN             NaN                9  \n",
       "5            0.780142         0.776312        0.015564                3  \n",
       "6                 NaN              NaN             NaN               10  \n",
       "7            0.779097         0.776323        0.013605                2  \n",
       "8                 NaN              NaN             NaN               11  \n",
       "9            0.777778         0.769052        0.011248                5  \n",
       "10                NaN              NaN             NaN               12  \n",
       "11           0.766885         0.663814        0.063885                6  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_lin_svc_results = pd.DataFrame(grid_lin_svc.cv_results_)\n",
    "df_grid_lin_svc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[979 451]\n",
      " [116 430]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.68      0.78      1430\n",
      "           1       0.49      0.79      0.60       546\n",
      "\n",
      "    accuracy                           0.71      1976\n",
      "   macro avg       0.69      0.74      0.69      1976\n",
      "weighted avg       0.78      0.71      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_lin_svc_train_predict = grid_lin_svc.predict(X_train_rus)\n",
    "y_lin_svc_predict = grid_lin_svc.predict(X_test)\n",
    "lin_svc_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_lin_svc_predict)\n",
    "lin_svc_classification_report = classification_report(y_true = y_test, y_pred = y_lin_svc_predict)\n",
    "print(lin_svc_cmatrix)\n",
    "print()\n",
    "print(lin_svc_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.7835144927536233\n",
      "Test F1-Score: 0.6026629292221443\n"
     ]
    }
   ],
   "source": [
    "lin_svc_classifier = LinearSVC(C=0.001, penalty='l2', random_state=0)\n",
    "lin_svc_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=lin_svc_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=lin_svc_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Linear SVM\", \"C = 0.001, penalty = 'l2'\", f1_score(y_true=y_train_rus, y_pred=lin_svc_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=lin_svc_classifier.predict(X_test)),\n",
    "                 df_grid_lin_svc_results[df_grid_lin_svc_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classification Model                         Model Parameter  \\\n",
       "0                              KNN  n_neighbors = 20, weights = 'distance'   \n",
       "1              Logistic Regression                C = 0.01, penalty = 'l2'   \n",
       "2               SVM (kernel='rbf')                   C = 1, gamma = 'auto'   \n",
       "3  SVM (kernel='poly', degree = 3)                  C = 1, gamma = 'scale'   \n",
       "4            SVM (kernel='linear')                                 C = 100   \n",
       "5                       Linear SVM               C = 0.001, penalty = 'l2'   \n",
       "\n",
       "   Train F1-Score  Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.999520       0.584980                 0.788615  \n",
       "1        0.782490       0.609739                 0.779720  \n",
       "2        0.819056       0.606868                 0.776477  \n",
       "3        0.835155       0.602759                 0.770910  \n",
       "4        0.770344       0.613128                 0.767012  \n",
       "5        0.783514       0.602663                 0.777433  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814569536423841"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {'max_depth':[1, 2, 3, 4, 5, 6], 'splitter': ['best', 'random'], 'criterion': ['gini', 'entropy']}\n",
    "\n",
    "grid_tree = GridSearchCV(dtree, param_grid, cv = 5, scoring='f1')\n",
    "grid_tree.fit(X_train_rus, y_train_rus)\n",
    "grid_tree.score(X_train_rus,y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5967741935483871"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 6, 'splitter': 'best'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7702909581214521"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_splitter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'splitte...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'splitte...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'splitte...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'splitte...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'splitte...</td>\n",
       "      <td>0.740047</td>\n",
       "      <td>0.694789</td>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.763033</td>\n",
       "      <td>0.750260</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'splitte...</td>\n",
       "      <td>0.739535</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.755003</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'splitte...</td>\n",
       "      <td>0.715640</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.772443</td>\n",
       "      <td>0.758454</td>\n",
       "      <td>0.747612</td>\n",
       "      <td>0.020081</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'splitte...</td>\n",
       "      <td>0.750572</td>\n",
       "      <td>0.713287</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.771552</td>\n",
       "      <td>0.760494</td>\n",
       "      <td>0.753645</td>\n",
       "      <td>0.021701</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'splitte...</td>\n",
       "      <td>0.734967</td>\n",
       "      <td>0.750594</td>\n",
       "      <td>0.769932</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.777518</td>\n",
       "      <td>0.760101</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'splitte...</td>\n",
       "      <td>0.725995</td>\n",
       "      <td>0.741419</td>\n",
       "      <td>0.760369</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.754752</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'splitte...</td>\n",
       "      <td>0.760181</td>\n",
       "      <td>0.758294</td>\n",
       "      <td>0.747100</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.765630</td>\n",
       "      <td>0.016240</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'splitte...</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.752735</td>\n",
       "      <td>0.788288</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.759232</td>\n",
       "      <td>0.023560</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'spli...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.006232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'spli...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'spli...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'spli...</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>0.749482</td>\n",
       "      <td>0.775934</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.768240</td>\n",
       "      <td>0.765302</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'spli...</td>\n",
       "      <td>0.740047</td>\n",
       "      <td>0.694789</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.746858</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'spli...</td>\n",
       "      <td>0.739535</td>\n",
       "      <td>0.735160</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.755457</td>\n",
       "      <td>0.019954</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'spli...</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.734341</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.772443</td>\n",
       "      <td>0.762980</td>\n",
       "      <td>0.747653</td>\n",
       "      <td>0.025459</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'spli...</td>\n",
       "      <td>0.753363</td>\n",
       "      <td>0.720358</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.786749</td>\n",
       "      <td>0.760494</td>\n",
       "      <td>0.758657</td>\n",
       "      <td>0.022245</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'spli...</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.762791</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.762465</td>\n",
       "      <td>0.013667</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'spli...</td>\n",
       "      <td>0.753488</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.747045</td>\n",
       "      <td>0.784648</td>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.759050</td>\n",
       "      <td>0.015652</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>best</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'spli...</td>\n",
       "      <td>0.755656</td>\n",
       "      <td>0.763033</td>\n",
       "      <td>0.757848</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.805687</td>\n",
       "      <td>0.770291</td>\n",
       "      <td>0.018307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>random</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'spli...</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.761229</td>\n",
       "      <td>0.757238</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.783848</td>\n",
       "      <td>0.758559</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.000798      0.001595         0.000427        0.000854   \n",
       "1        0.003125      0.006251         0.000000        0.000000   \n",
       "2        0.003124      0.006249         0.000000        0.000000   \n",
       "3        0.000000      0.000000         0.006248        0.007653   \n",
       "4        0.003572      0.002537         0.000797        0.000399   \n",
       "5        0.002403      0.000480         0.000798        0.000399   \n",
       "6        0.000000      0.000000         0.003125        0.006250   \n",
       "7        0.003124      0.006247         0.000000        0.000000   \n",
       "8        0.006249      0.007654         0.003124        0.006248   \n",
       "9        0.000000      0.000000         0.003124        0.006247   \n",
       "10       0.003131      0.006262         0.003118        0.006235   \n",
       "11       0.006249      0.007654         0.000000        0.000000   \n",
       "12       0.003132      0.006263         0.000000        0.000000   \n",
       "13       0.003116      0.006232         0.000000        0.000000   \n",
       "14       0.003125      0.006250         0.000000        0.000000   \n",
       "15       0.000000      0.000000         0.003124        0.006249   \n",
       "16       0.000000      0.000000         0.003124        0.006248   \n",
       "17       0.006249      0.007653         0.000000        0.000000   \n",
       "18       0.003124      0.006248         0.000000        0.000000   \n",
       "19       0.006248      0.007652         0.000000        0.000000   \n",
       "20       0.003124      0.006249         0.003124        0.006248   \n",
       "21       0.003125      0.006250         0.000000        0.000000   \n",
       "22       0.003124      0.006248         0.003124        0.006248   \n",
       "23       0.006248      0.007653         0.000000        0.000000   \n",
       "\n",
       "   param_criterion param_max_depth param_splitter  \\\n",
       "0             gini               1           best   \n",
       "1             gini               1         random   \n",
       "2             gini               2           best   \n",
       "3             gini               2         random   \n",
       "4             gini               3           best   \n",
       "5             gini               3         random   \n",
       "6             gini               4           best   \n",
       "7             gini               4         random   \n",
       "8             gini               5           best   \n",
       "9             gini               5         random   \n",
       "10            gini               6           best   \n",
       "11            gini               6         random   \n",
       "12         entropy               1           best   \n",
       "13         entropy               1         random   \n",
       "14         entropy               2           best   \n",
       "15         entropy               2         random   \n",
       "16         entropy               3           best   \n",
       "17         entropy               3         random   \n",
       "18         entropy               4           best   \n",
       "19         entropy               4         random   \n",
       "20         entropy               5           best   \n",
       "21         entropy               5         random   \n",
       "22         entropy               6           best   \n",
       "23         entropy               6         random   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'criterion': 'gini', 'max_depth': 1, 'splitte...           0.754167   \n",
       "1   {'criterion': 'gini', 'max_depth': 1, 'splitte...           0.754167   \n",
       "2   {'criterion': 'gini', 'max_depth': 2, 'splitte...           0.754167   \n",
       "3   {'criterion': 'gini', 'max_depth': 2, 'splitte...           0.754167   \n",
       "4   {'criterion': 'gini', 'max_depth': 3, 'splitte...           0.740047   \n",
       "5   {'criterion': 'gini', 'max_depth': 3, 'splitte...           0.739535   \n",
       "6   {'criterion': 'gini', 'max_depth': 4, 'splitte...           0.715640   \n",
       "7   {'criterion': 'gini', 'max_depth': 4, 'splitte...           0.750572   \n",
       "8   {'criterion': 'gini', 'max_depth': 5, 'splitte...           0.734967   \n",
       "9   {'criterion': 'gini', 'max_depth': 5, 'splitte...           0.725995   \n",
       "10  {'criterion': 'gini', 'max_depth': 6, 'splitte...           0.760181   \n",
       "11  {'criterion': 'gini', 'max_depth': 6, 'splitte...           0.726027   \n",
       "12  {'criterion': 'entropy', 'max_depth': 1, 'spli...           0.754167   \n",
       "13  {'criterion': 'entropy', 'max_depth': 1, 'spli...           0.754167   \n",
       "14  {'criterion': 'entropy', 'max_depth': 2, 'spli...           0.754167   \n",
       "15  {'criterion': 'entropy', 'max_depth': 2, 'spli...           0.754167   \n",
       "16  {'criterion': 'entropy', 'max_depth': 3, 'spli...           0.740047   \n",
       "17  {'criterion': 'entropy', 'max_depth': 3, 'spli...           0.739535   \n",
       "18  {'criterion': 'entropy', 'max_depth': 4, 'spli...           0.703797   \n",
       "19  {'criterion': 'entropy', 'max_depth': 4, 'spli...           0.753363   \n",
       "20  {'criterion': 'entropy', 'max_depth': 5, 'spli...           0.752294   \n",
       "21  {'criterion': 'entropy', 'max_depth': 5, 'spli...           0.753488   \n",
       "22  {'criterion': 'entropy', 'max_depth': 6, 'spli...           0.755656   \n",
       "23  {'criterion': 'entropy', 'max_depth': 6, 'spli...           0.747748   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.749482           0.775934           0.778689   \n",
       "1            0.749482           0.775934           0.778689   \n",
       "2            0.749482           0.775934           0.778689   \n",
       "3            0.749482           0.775934           0.778689   \n",
       "4            0.694789           0.777311           0.776119   \n",
       "5            0.736842           0.761682           0.786957   \n",
       "6            0.734513           0.757009           0.772443   \n",
       "7            0.713287           0.772321           0.771552   \n",
       "8            0.750594           0.769932           0.767494   \n",
       "9            0.741419           0.760369           0.774892   \n",
       "10           0.758294           0.747100           0.767123   \n",
       "11           0.745455           0.752735           0.788288   \n",
       "12           0.749482           0.775934           0.778689   \n",
       "13           0.749482           0.775934           0.778689   \n",
       "14           0.749482           0.775934           0.778689   \n",
       "15           0.749482           0.775934           0.778689   \n",
       "16           0.694789           0.773333           0.776119   \n",
       "17           0.735160           0.761682           0.790909   \n",
       "18           0.734341           0.764706           0.772443   \n",
       "19           0.720358           0.772321           0.786749   \n",
       "20           0.745763           0.762791           0.765766   \n",
       "21           0.741573           0.747045           0.784648   \n",
       "22           0.763033           0.757848           0.769231   \n",
       "23           0.761229           0.757238           0.742729   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.768240         0.765302        0.011620                3  \n",
       "1            0.768240         0.765302        0.011620                3  \n",
       "2            0.768240         0.765302        0.011620                3  \n",
       "3            0.768240         0.765302        0.011620                3  \n",
       "4            0.763033         0.750260        0.030806               21  \n",
       "5            0.750000         0.755003        0.018219               18  \n",
       "6            0.758454         0.747612        0.020081               23  \n",
       "7            0.760494         0.753645        0.021701               20  \n",
       "8            0.777518         0.760101        0.015341               12  \n",
       "9            0.771084         0.754752        0.018486               19  \n",
       "10           0.795455         0.765630        0.016240                2  \n",
       "11           0.783654         0.759232        0.023560               13  \n",
       "12           0.768240         0.765302        0.011620                3  \n",
       "13           0.768240         0.765302        0.011620                3  \n",
       "14           0.768240         0.765302        0.011620                3  \n",
       "15           0.768240         0.765302        0.011620                3  \n",
       "16           0.750000         0.746858        0.029410               24  \n",
       "17           0.750000         0.755457        0.019954               17  \n",
       "18           0.762980         0.747653        0.025459               22  \n",
       "19           0.760494         0.758657        0.022245               15  \n",
       "20           0.785714         0.762465        0.013667               11  \n",
       "21           0.768496         0.759050        0.015652               14  \n",
       "22           0.805687         0.770291        0.018307                1  \n",
       "23           0.783848         0.758559        0.014254               16  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_tree_results = pd.DataFrame(grid_tree.cv_results_)\n",
    "df_grid_tree_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1019  411]\n",
      " [ 139  407]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79      1430\n",
      "           1       0.50      0.75      0.60       546\n",
      "\n",
      "    accuracy                           0.72      1976\n",
      "   macro avg       0.69      0.73      0.69      1976\n",
      "weighted avg       0.77      0.72      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_tree_train_predict = grid_tree.predict(X_train_rus)\n",
    "y_tree_predict = grid_tree.predict(X_test)\n",
    "tree_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_tree_predict)\n",
    "tree_classification_report = classification_report(y_true = y_test, y_pred = y_tree_predict)\n",
    "print(tree_cmatrix)\n",
    "print()\n",
    "print(tree_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.814569536423841\n",
      "Test F1-Score: 0.5967741935483871\n"
     ]
    }
   ],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=0, max_depth=6, criterion='entropy', splitter='best')\n",
    "tree_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=tree_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=tree_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Decision Tree\", \"max_depth=6, criterion = 'entropy', splitter = 'best'\", f1_score(y_true=y_train_rus, y_pred=tree_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=tree_classifier.predict(X_test)),\n",
    "                 df_grid_tree_results[df_grid_tree_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classification Model  \\\n",
       "0                              KNN   \n",
       "1              Logistic Regression   \n",
       "2               SVM (kernel='rbf')   \n",
       "3  SVM (kernel='poly', degree = 3)   \n",
       "4            SVM (kernel='linear')   \n",
       "5                       Linear SVM   \n",
       "6                    Decision Tree   \n",
       "\n",
       "                                     Model Parameter  Train F1-Score  \\\n",
       "0             n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                           C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                              C = 1, gamma = 'auto'        0.819056   \n",
       "3                             C = 1, gamma = 'scale'        0.835155   \n",
       "4                                            C = 100        0.770344   \n",
       "5                          C = 0.001, penalty = 'l2'        0.783514   \n",
       "6  max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "\n",
       "   Test F1-Score  Best Mean Test F1-Score  \n",
       "0       0.584980                 0.788615  \n",
       "1       0.609739                 0.779720  \n",
       "2       0.606868                 0.776477  \n",
       "3       0.602759                 0.770910  \n",
       "4       0.613128                 0.767012  \n",
       "5       0.602663                 0.777433  \n",
       "6       0.596774                 0.770291  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8170289855072463"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "param_grid = {'n_estimators':[10,50,100,200,500], 'max_depth': [1,2,3,4,5,6], 'criterion': ['entropy', 'gini']}\n",
    "\n",
    "grid_rfc = GridSearchCV(rfc, param_grid, cv = 5, scoring='f1')\n",
    "grid_rfc.fit(X_train_rus, y_train_rus)\n",
    "grid_rfc.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6208333333333333"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 200}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7812631564629469"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014194</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'n_es...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.720358</td>\n",
       "      <td>0.770925</td>\n",
       "      <td>0.770563</td>\n",
       "      <td>0.754258</td>\n",
       "      <td>0.751267</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056243</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'n_es...</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.770601</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.777528</td>\n",
       "      <td>0.764531</td>\n",
       "      <td>0.014585</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112467</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'n_es...</td>\n",
       "      <td>0.743875</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.763459</td>\n",
       "      <td>0.015952</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.214355</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'n_es...</td>\n",
       "      <td>0.736142</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.762095</td>\n",
       "      <td>0.017845</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.524876</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>0.043739</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>entropy</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'n_es...</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.775056</td>\n",
       "      <td>0.783726</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.764038</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015628</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>0.739229</td>\n",
       "      <td>0.743875</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.779343</td>\n",
       "      <td>0.763639</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.056229</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.768182</td>\n",
       "      <td>0.782418</td>\n",
       "      <td>0.780600</td>\n",
       "      <td>0.766347</td>\n",
       "      <td>0.014023</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.115597</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.744493</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.763709</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.230731</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.018745</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.741228</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.785249</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.763237</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.577989</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.043739</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>entropy</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 2, 'n_es...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.741228</td>\n",
       "      <td>0.766816</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.762125</td>\n",
       "      <td>0.759943</td>\n",
       "      <td>0.015193</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.742459</td>\n",
       "      <td>0.752336</td>\n",
       "      <td>0.760820</td>\n",
       "      <td>0.783964</td>\n",
       "      <td>0.761446</td>\n",
       "      <td>0.760205</td>\n",
       "      <td>0.013735</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.776524</td>\n",
       "      <td>0.781457</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.763143</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.124964</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.740909</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.764254</td>\n",
       "      <td>0.016953</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.253073</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.018737</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.744292</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.787746</td>\n",
       "      <td>0.775229</td>\n",
       "      <td>0.765394</td>\n",
       "      <td>0.017034</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.665616</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3, 'n_es...</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.745011</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.794760</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.764880</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.014005</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'n_es...</td>\n",
       "      <td>0.734411</td>\n",
       "      <td>0.750583</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.763601</td>\n",
       "      <td>0.018706</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.081420</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'n_es...</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.760369</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.769259</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.190226</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'n_es...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>0.780045</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.778032</td>\n",
       "      <td>0.768857</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.288816</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>0.022929</td>\n",
       "      <td>0.007043</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'n_es...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.760820</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.788546</td>\n",
       "      <td>0.773455</td>\n",
       "      <td>0.768718</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.848551</td>\n",
       "      <td>0.059071</td>\n",
       "      <td>0.055436</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>entropy</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 4, 'n_es...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.753363</td>\n",
       "      <td>0.780045</td>\n",
       "      <td>0.783186</td>\n",
       "      <td>0.759907</td>\n",
       "      <td>0.764499</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.018585</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.749415</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.761021</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.759037</td>\n",
       "      <td>0.016158</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.112099</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.753488</td>\n",
       "      <td>0.768868</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.781176</td>\n",
       "      <td>0.772585</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.159374</td>\n",
       "      <td>0.011308</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.755760</td>\n",
       "      <td>0.764977</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.783186</td>\n",
       "      <td>0.774942</td>\n",
       "      <td>0.770033</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.356088</td>\n",
       "      <td>0.040143</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.752887</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.775785</td>\n",
       "      <td>0.783186</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.771352</td>\n",
       "      <td>0.010581</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.962849</td>\n",
       "      <td>0.084539</td>\n",
       "      <td>0.066824</td>\n",
       "      <td>0.008848</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'n_es...</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.771689</td>\n",
       "      <td>0.778281</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.772814</td>\n",
       "      <td>0.011602</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'n_es...</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.762125</td>\n",
       "      <td>0.762980</td>\n",
       "      <td>0.782022</td>\n",
       "      <td>0.798122</td>\n",
       "      <td>0.773343</td>\n",
       "      <td>0.014585</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.084038</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'n_es...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.768879</td>\n",
       "      <td>0.773756</td>\n",
       "      <td>0.795556</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.777638</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.187895</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'n_es...</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.770975</td>\n",
       "      <td>0.781038</td>\n",
       "      <td>0.788419</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.778953</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.359831</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>0.022087</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'n_es...</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.792873</td>\n",
       "      <td>0.783217</td>\n",
       "      <td>0.781263</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.197836</td>\n",
       "      <td>0.332116</td>\n",
       "      <td>0.066455</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>entropy</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 6, 'n_es...</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.792873</td>\n",
       "      <td>0.784223</td>\n",
       "      <td>0.779742</td>\n",
       "      <td>0.009990</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'n_estim...</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.720358</td>\n",
       "      <td>0.778022</td>\n",
       "      <td>0.770563</td>\n",
       "      <td>0.754258</td>\n",
       "      <td>0.752686</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.055459</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'n_estim...</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.768889</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.763638</td>\n",
       "      <td>0.013984</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.109309</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'n_estim...</td>\n",
       "      <td>0.746067</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.770601</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.762262</td>\n",
       "      <td>0.014460</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.221807</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'n_estim...</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.785408</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.764594</td>\n",
       "      <td>0.015835</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.569085</td>\n",
       "      <td>0.035505</td>\n",
       "      <td>0.045272</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>gini</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'n_estim...</td>\n",
       "      <td>0.744921</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>0.766816</td>\n",
       "      <td>0.783726</td>\n",
       "      <td>0.770975</td>\n",
       "      <td>0.762853</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.014567</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>0.739229</td>\n",
       "      <td>0.747204</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.779343</td>\n",
       "      <td>0.763628</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.067214</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.754464</td>\n",
       "      <td>0.771689</td>\n",
       "      <td>0.785088</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.766763</td>\n",
       "      <td>0.013059</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.179320</td>\n",
       "      <td>0.093054</td>\n",
       "      <td>0.019149</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.744493</td>\n",
       "      <td>0.775785</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.763995</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.263694</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.019150</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.741228</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.783550</td>\n",
       "      <td>0.769585</td>\n",
       "      <td>0.762332</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.626723</td>\n",
       "      <td>0.047815</td>\n",
       "      <td>0.045079</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>gini</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 2, 'n_estim...</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.741228</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.764977</td>\n",
       "      <td>0.762438</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.015365</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.755760</td>\n",
       "      <td>0.762980</td>\n",
       "      <td>0.783186</td>\n",
       "      <td>0.761446</td>\n",
       "      <td>0.761512</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.063430</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>0.746067</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.786813</td>\n",
       "      <td>0.757647</td>\n",
       "      <td>0.761473</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.133842</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>0.744292</td>\n",
       "      <td>0.752784</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.766744</td>\n",
       "      <td>0.764713</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.258317</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.019340</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>0.744292</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.790393</td>\n",
       "      <td>0.769585</td>\n",
       "      <td>0.765591</td>\n",
       "      <td>0.016245</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.641277</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.048883</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>gini</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3, 'n_estim...</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.748330</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.794760</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.765339</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'n_estim...</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.739030</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.778302</td>\n",
       "      <td>0.762018</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.074188</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'n_estim...</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.786207</td>\n",
       "      <td>0.769070</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.138230</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'n_estim...</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.757991</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0.777042</td>\n",
       "      <td>0.777273</td>\n",
       "      <td>0.767254</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.333111</td>\n",
       "      <td>0.055606</td>\n",
       "      <td>0.028721</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'n_estim...</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.777528</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>0.777273</td>\n",
       "      <td>0.767358</td>\n",
       "      <td>0.013456</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.752188</td>\n",
       "      <td>0.072652</td>\n",
       "      <td>0.056134</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>gini</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 4, 'n_estim...</td>\n",
       "      <td>0.747706</td>\n",
       "      <td>0.750562</td>\n",
       "      <td>0.777273</td>\n",
       "      <td>0.785088</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.764904</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.725995</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>0.782998</td>\n",
       "      <td>0.757794</td>\n",
       "      <td>0.752472</td>\n",
       "      <td>0.019950</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.075569</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>0.755760</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.776439</td>\n",
       "      <td>0.012865</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.189860</td>\n",
       "      <td>0.033715</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.791855</td>\n",
       "      <td>0.783186</td>\n",
       "      <td>0.790805</td>\n",
       "      <td>0.777248</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.350201</td>\n",
       "      <td>0.046477</td>\n",
       "      <td>0.025077</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>0.754023</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.785553</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.776861</td>\n",
       "      <td>0.012141</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.783429</td>\n",
       "      <td>0.046318</td>\n",
       "      <td>0.056871</td>\n",
       "      <td>0.008622</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'n_estim...</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.768182</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.774122</td>\n",
       "      <td>0.013975</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.763218</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.792541</td>\n",
       "      <td>0.775328</td>\n",
       "      <td>0.016778</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.079385</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.756881</td>\n",
       "      <td>0.769932</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.775414</td>\n",
       "      <td>0.773789</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.175194</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.757991</td>\n",
       "      <td>0.765376</td>\n",
       "      <td>0.781038</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.775412</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.343228</td>\n",
       "      <td>0.025717</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.760820</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.789116</td>\n",
       "      <td>0.793792</td>\n",
       "      <td>0.779582</td>\n",
       "      <td>0.779764</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.807213</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.055696</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>gini</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 6, 'n_estim...</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.771689</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.792873</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.779972</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.014194      0.002856         0.000000        0.000000   \n",
       "1        0.056243      0.007658         0.006248        0.007652   \n",
       "2        0.112467      0.006253         0.006249        0.007653   \n",
       "3        0.214355      0.006130         0.015615        0.000011   \n",
       "4        0.524876      0.007654         0.043739        0.006248   \n",
       "5        0.015628      0.000013         0.000000        0.000000   \n",
       "6        0.056229      0.007648         0.006256        0.007662   \n",
       "7        0.115597      0.007653         0.012497        0.006249   \n",
       "8        0.230731      0.006081         0.018745        0.006247   \n",
       "9        0.577989      0.024200         0.043739        0.006249   \n",
       "10       0.012498      0.006249         0.003124        0.006247   \n",
       "11       0.062498      0.000015         0.009360        0.007642   \n",
       "12       0.124964      0.000026         0.009380        0.007658   \n",
       "13       0.253073      0.006246         0.018737        0.006252   \n",
       "14       0.665616      0.023747         0.043071        0.005840   \n",
       "15       0.014005      0.005736         0.003530        0.006112   \n",
       "16       0.081420      0.006415         0.006577        0.000786   \n",
       "17       0.190226      0.027587         0.013146        0.002926   \n",
       "18       0.288816      0.009923         0.022929        0.007043   \n",
       "19       0.848551      0.059071         0.055436        0.007268   \n",
       "20       0.018585      0.004938         0.002388        0.001624   \n",
       "21       0.112099      0.002572         0.008179        0.001716   \n",
       "22       0.159374      0.011308         0.011170        0.000746   \n",
       "23       0.356088      0.040143         0.028524        0.005224   \n",
       "24       0.962849      0.084539         0.066824        0.008848   \n",
       "25       0.018550      0.000503         0.002593        0.000799   \n",
       "26       0.084038      0.006450         0.004521        0.006174   \n",
       "27       0.187895      0.011587         0.013571        0.001851   \n",
       "28       0.359831      0.012818         0.022087        0.003314   \n",
       "29       1.197836      0.332116         0.066455        0.016225   \n",
       "30       0.012367      0.000502         0.001995        0.000002   \n",
       "31       0.055459      0.003078         0.005385        0.000489   \n",
       "32       0.109309      0.003180         0.009567        0.000498   \n",
       "33       0.221807      0.005627         0.018351        0.000488   \n",
       "34       0.569085      0.035505         0.045272        0.003410   \n",
       "35       0.014567      0.000801         0.002194        0.000399   \n",
       "36       0.067214      0.008849         0.006183        0.000400   \n",
       "37       0.179320      0.093054         0.019149        0.010099   \n",
       "38       0.263694      0.032450         0.019150        0.000415   \n",
       "39       0.626723      0.047815         0.045079        0.000992   \n",
       "40       0.015365      0.000804         0.002586        0.001201   \n",
       "41       0.063430      0.002315         0.005791        0.000403   \n",
       "42       0.133842      0.009343         0.009973        0.001092   \n",
       "43       0.258317      0.014390         0.019340        0.000810   \n",
       "44       0.641277      0.022706         0.048883        0.003563   \n",
       "45       0.015559      0.001197         0.002188        0.000401   \n",
       "46       0.074188      0.004576         0.006988        0.000886   \n",
       "47       0.138230      0.002148         0.010964        0.000642   \n",
       "48       0.333111      0.055606         0.028721        0.014110   \n",
       "49       0.752188      0.072652         0.056134        0.007371   \n",
       "50       0.015622      0.000003         0.000000        0.000000   \n",
       "51       0.075569      0.013157         0.009832        0.005773   \n",
       "52       0.189860      0.033715         0.016487        0.000859   \n",
       "53       0.350201      0.046477         0.025077        0.005952   \n",
       "54       0.783429      0.046318         0.056871        0.008622   \n",
       "55       0.018608      0.006477         0.002202        0.001334   \n",
       "56       0.079385      0.011137         0.004920        0.006383   \n",
       "57       0.175194      0.028275         0.011091        0.005701   \n",
       "58       0.343228      0.025717         0.027178        0.002865   \n",
       "59       0.807213      0.051355         0.055696        0.008393   \n",
       "\n",
       "   param_criterion param_max_depth param_n_estimators  \\\n",
       "0          entropy               1                 10   \n",
       "1          entropy               1                 50   \n",
       "2          entropy               1                100   \n",
       "3          entropy               1                200   \n",
       "4          entropy               1                500   \n",
       "5          entropy               2                 10   \n",
       "6          entropy               2                 50   \n",
       "7          entropy               2                100   \n",
       "8          entropy               2                200   \n",
       "9          entropy               2                500   \n",
       "10         entropy               3                 10   \n",
       "11         entropy               3                 50   \n",
       "12         entropy               3                100   \n",
       "13         entropy               3                200   \n",
       "14         entropy               3                500   \n",
       "15         entropy               4                 10   \n",
       "16         entropy               4                 50   \n",
       "17         entropy               4                100   \n",
       "18         entropy               4                200   \n",
       "19         entropy               4                500   \n",
       "20         entropy               5                 10   \n",
       "21         entropy               5                 50   \n",
       "22         entropy               5                100   \n",
       "23         entropy               5                200   \n",
       "24         entropy               5                500   \n",
       "25         entropy               6                 10   \n",
       "26         entropy               6                 50   \n",
       "27         entropy               6                100   \n",
       "28         entropy               6                200   \n",
       "29         entropy               6                500   \n",
       "30            gini               1                 10   \n",
       "31            gini               1                 50   \n",
       "32            gini               1                100   \n",
       "33            gini               1                200   \n",
       "34            gini               1                500   \n",
       "35            gini               2                 10   \n",
       "36            gini               2                 50   \n",
       "37            gini               2                100   \n",
       "38            gini               2                200   \n",
       "39            gini               2                500   \n",
       "40            gini               3                 10   \n",
       "41            gini               3                 50   \n",
       "42            gini               3                100   \n",
       "43            gini               3                200   \n",
       "44            gini               3                500   \n",
       "45            gini               4                 10   \n",
       "46            gini               4                 50   \n",
       "47            gini               4                100   \n",
       "48            gini               4                200   \n",
       "49            gini               4                500   \n",
       "50            gini               5                 10   \n",
       "51            gini               5                 50   \n",
       "52            gini               5                100   \n",
       "53            gini               5                200   \n",
       "54            gini               5                500   \n",
       "55            gini               6                 10   \n",
       "56            gini               6                 50   \n",
       "57            gini               6                100   \n",
       "58            gini               6                200   \n",
       "59            gini               6                500   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'criterion': 'entropy', 'max_depth': 1, 'n_es...           0.740230   \n",
       "1   {'criterion': 'entropy', 'max_depth': 1, 'n_es...           0.742729   \n",
       "2   {'criterion': 'entropy', 'max_depth': 1, 'n_es...           0.743875   \n",
       "3   {'criterion': 'entropy', 'max_depth': 1, 'n_es...           0.736142   \n",
       "4   {'criterion': 'entropy', 'max_depth': 1, 'n_es...           0.741573   \n",
       "5   {'criterion': 'entropy', 'max_depth': 2, 'n_es...           0.739229   \n",
       "6   {'criterion': 'entropy', 'max_depth': 2, 'n_es...           0.749425   \n",
       "7   {'criterion': 'entropy', 'max_depth': 2, 'n_es...           0.745995   \n",
       "8   {'criterion': 'entropy', 'max_depth': 2, 'n_es...           0.745995   \n",
       "9   {'criterion': 'entropy', 'max_depth': 2, 'n_es...           0.745995   \n",
       "10  {'criterion': 'entropy', 'max_depth': 3, 'n_es...           0.742459   \n",
       "11  {'criterion': 'entropy', 'max_depth': 3, 'n_es...           0.749425   \n",
       "12  {'criterion': 'entropy', 'max_depth': 3, 'n_es...           0.740909   \n",
       "13  {'criterion': 'entropy', 'max_depth': 3, 'n_es...           0.744292   \n",
       "14  {'criterion': 'entropy', 'max_depth': 3, 'n_es...           0.747706   \n",
       "15  {'criterion': 'entropy', 'max_depth': 4, 'n_es...           0.734411   \n",
       "16  {'criterion': 'entropy', 'max_depth': 4, 'n_es...           0.749425   \n",
       "17  {'criterion': 'entropy', 'max_depth': 4, 'n_es...           0.745995   \n",
       "18  {'criterion': 'entropy', 'max_depth': 4, 'n_es...           0.745995   \n",
       "19  {'criterion': 'entropy', 'max_depth': 4, 'n_es...           0.745995   \n",
       "20  {'criterion': 'entropy', 'max_depth': 5, 'n_es...           0.749415   \n",
       "21  {'criterion': 'entropy', 'max_depth': 5, 'n_es...           0.753488   \n",
       "22  {'criterion': 'entropy', 'max_depth': 5, 'n_es...           0.755760   \n",
       "23  {'criterion': 'entropy', 'max_depth': 5, 'n_es...           0.752887   \n",
       "24  {'criterion': 'entropy', 'max_depth': 5, 'n_es...           0.752294   \n",
       "25  {'criterion': 'entropy', 'max_depth': 6, 'n_es...           0.761468   \n",
       "26  {'criterion': 'entropy', 'max_depth': 6, 'n_es...           0.750000   \n",
       "27  {'criterion': 'entropy', 'max_depth': 6, 'n_es...           0.763636   \n",
       "28  {'criterion': 'entropy', 'max_depth': 6, 'n_es...           0.764706   \n",
       "29  {'criterion': 'entropy', 'max_depth': 6, 'n_es...           0.763636   \n",
       "30  {'criterion': 'gini', 'max_depth': 1, 'n_estim...           0.740230   \n",
       "31  {'criterion': 'gini', 'max_depth': 1, 'n_estim...           0.742729   \n",
       "32  {'criterion': 'gini', 'max_depth': 1, 'n_estim...           0.746067   \n",
       "33  {'criterion': 'gini', 'max_depth': 1, 'n_estim...           0.744395   \n",
       "34  {'criterion': 'gini', 'max_depth': 1, 'n_estim...           0.744921   \n",
       "35  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           0.739229   \n",
       "36  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           0.749425   \n",
       "37  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           0.745995   \n",
       "38  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           0.745995   \n",
       "39  {'criterion': 'gini', 'max_depth': 2, 'n_estim...           0.745995   \n",
       "40  {'criterion': 'gini', 'max_depth': 3, 'n_estim...           0.744186   \n",
       "41  {'criterion': 'gini', 'max_depth': 3, 'n_estim...           0.744828   \n",
       "42  {'criterion': 'gini', 'max_depth': 3, 'n_estim...           0.744292   \n",
       "43  {'criterion': 'gini', 'max_depth': 3, 'n_estim...           0.744292   \n",
       "44  {'criterion': 'gini', 'max_depth': 3, 'n_estim...           0.747706   \n",
       "45  {'criterion': 'gini', 'max_depth': 4, 'n_estim...           0.744186   \n",
       "46  {'criterion': 'gini', 'max_depth': 4, 'n_estim...           0.746544   \n",
       "47  {'criterion': 'gini', 'max_depth': 4, 'n_estim...           0.747706   \n",
       "48  {'criterion': 'gini', 'max_depth': 4, 'n_estim...           0.747706   \n",
       "49  {'criterion': 'gini', 'max_depth': 4, 'n_estim...           0.747706   \n",
       "50  {'criterion': 'gini', 'max_depth': 5, 'n_estim...           0.735849   \n",
       "51  {'criterion': 'gini', 'max_depth': 5, 'n_estim...           0.755760   \n",
       "52  {'criterion': 'gini', 'max_depth': 5, 'n_estim...           0.754630   \n",
       "53  {'criterion': 'gini', 'max_depth': 5, 'n_estim...           0.754023   \n",
       "54  {'criterion': 'gini', 'max_depth': 5, 'n_estim...           0.749425   \n",
       "55  {'criterion': 'gini', 'max_depth': 6, 'n_estim...           0.749425   \n",
       "56  {'criterion': 'gini', 'max_depth': 6, 'n_estim...           0.756881   \n",
       "57  {'criterion': 'gini', 'max_depth': 6, 'n_estim...           0.757991   \n",
       "58  {'criterion': 'gini', 'max_depth': 6, 'n_estim...           0.760820   \n",
       "59  {'criterion': 'gini', 'max_depth': 6, 'n_estim...           0.763636   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.720358           0.770925           0.770563   \n",
       "1            0.752137           0.770601           0.779661   \n",
       "2            0.745098           0.773333           0.782979   \n",
       "3            0.747826           0.769912           0.786325   \n",
       "4            0.747826           0.775056           0.783726   \n",
       "5            0.743875           0.764444           0.791304   \n",
       "6            0.751111           0.768182           0.782418   \n",
       "7            0.744493           0.773034           0.782609   \n",
       "8            0.741228           0.771300           0.785249   \n",
       "9            0.741228           0.766816           0.783550   \n",
       "10           0.752336           0.760820           0.783964   \n",
       "11           0.747748           0.776524           0.781457   \n",
       "12           0.751111           0.773034           0.789474   \n",
       "13           0.746667           0.773034           0.787746   \n",
       "14           0.745011           0.773034           0.794760   \n",
       "15           0.750583           0.770642           0.786667   \n",
       "16           0.760369           0.781818           0.780488   \n",
       "17           0.759725           0.780045           0.780488   \n",
       "18           0.760820           0.774775           0.788546   \n",
       "19           0.753363           0.780045           0.783186   \n",
       "20           0.733813           0.761021           0.779510   \n",
       "21           0.768868           0.772727           0.786667   \n",
       "22           0.764977           0.771300           0.783186   \n",
       "23           0.767123           0.775785           0.783186   \n",
       "24           0.771689           0.778281           0.787611   \n",
       "25           0.762125           0.762980           0.782022   \n",
       "26           0.768879           0.773756           0.795556   \n",
       "27           0.770975           0.781038           0.788419   \n",
       "28           0.772727           0.792793           0.792873   \n",
       "29           0.774194           0.783784           0.792873   \n",
       "30           0.720358           0.778022           0.770563   \n",
       "31           0.752137           0.768889           0.779661   \n",
       "32           0.745098           0.770601           0.782051   \n",
       "33           0.747826           0.773333           0.785408   \n",
       "34           0.747826           0.766816           0.783726   \n",
       "35           0.747204           0.761062           0.791304   \n",
       "36           0.754464           0.771689           0.785088   \n",
       "37           0.744493           0.775785           0.786957   \n",
       "38           0.741228           0.771300           0.783550   \n",
       "39           0.741228           0.773034           0.786957   \n",
       "40           0.755760           0.762980           0.783186   \n",
       "41           0.746067           0.772009           0.786813   \n",
       "42           0.752784           0.770270           0.789474   \n",
       "43           0.751678           0.772009           0.790393   \n",
       "44           0.748330           0.772009           0.794760   \n",
       "45           0.739030           0.761905           0.786667   \n",
       "46           0.752294           0.779817           0.780488   \n",
       "47           0.757991           0.776256           0.777042   \n",
       "48           0.754545           0.777528           0.779736   \n",
       "49           0.750562           0.777273           0.785088   \n",
       "50           0.725995           0.759725           0.782998   \n",
       "51           0.767442           0.782609           0.786667   \n",
       "52           0.765766           0.791855           0.783186   \n",
       "53           0.775510           0.785553           0.787611   \n",
       "54           0.768182           0.783784           0.787611   \n",
       "55           0.763218           0.779510           0.791946   \n",
       "56           0.769932           0.774775           0.791946   \n",
       "57           0.765376           0.781038           0.787611   \n",
       "58           0.775510           0.789116           0.793792   \n",
       "59           0.771689           0.783784           0.792873   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.754258         0.751267        0.019209               60  \n",
       "1            0.777528         0.764531        0.014585               34  \n",
       "2            0.772009         0.763459        0.015952               44  \n",
       "3            0.770270         0.762095        0.017845               51  \n",
       "4            0.772009         0.764038        0.016371               37  \n",
       "5            0.779343         0.763639        0.019995               40  \n",
       "6            0.780600         0.766347        0.014023               26  \n",
       "7            0.772414         0.763709        0.015511               39  \n",
       "8            0.772414         0.763237        0.016825               45  \n",
       "9            0.762125         0.759943        0.015193               56  \n",
       "10           0.761446         0.760205        0.013735               55  \n",
       "11           0.760563         0.763143        0.013757               46  \n",
       "12           0.766744         0.764254        0.016953               36  \n",
       "13           0.775229         0.765394        0.017034               28  \n",
       "14           0.763889         0.764880        0.018166               31  \n",
       "15           0.775701         0.763601        0.018706               43  \n",
       "16           0.774194         0.769259        0.012495               19  \n",
       "17           0.778032         0.768857        0.013789               21  \n",
       "18           0.773455         0.768718        0.014360               22  \n",
       "19           0.759907         0.764499        0.014686               35  \n",
       "20           0.771429         0.759037        0.016158               57  \n",
       "21           0.781176         0.772585        0.011407               16  \n",
       "22           0.774942         0.770033        0.009252               18  \n",
       "23           0.777778         0.771352        0.010581               17  \n",
       "24           0.774194         0.772814        0.011602               15  \n",
       "25           0.798122         0.773343        0.014585               14  \n",
       "26           0.800000         0.777638        0.018313                6  \n",
       "27           0.790698         0.778953        0.010297                5  \n",
       "28           0.783217         0.781263        0.011122                1  \n",
       "29           0.784223         0.779742        0.009990                4  \n",
       "30           0.754258         0.752686        0.020805               58  \n",
       "31           0.774775         0.763638        0.013984               41  \n",
       "32           0.767494         0.762262        0.014460               50  \n",
       "33           0.772009         0.764594        0.015835               33  \n",
       "34           0.770975         0.762853        0.014593               47  \n",
       "35           0.779343         0.763628        0.019414               42  \n",
       "36           0.773148         0.766763        0.013059               25  \n",
       "37           0.766744         0.763995        0.016602               38  \n",
       "38           0.769585         0.762332        0.016097               49  \n",
       "39           0.764977         0.762438        0.016971               48  \n",
       "40           0.761446         0.761512        0.012690               53  \n",
       "41           0.757647         0.761473        0.016014               54  \n",
       "42           0.766744         0.764713        0.015540               32  \n",
       "43           0.769585         0.765591        0.016245               27  \n",
       "44           0.763889         0.765339        0.017392               29  \n",
       "45           0.778302         0.762018        0.018543               52  \n",
       "46           0.786207         0.769070        0.016300               20  \n",
       "47           0.777273         0.767254        0.012208               24  \n",
       "48           0.777273         0.767358        0.013456               23  \n",
       "49           0.763889         0.764904        0.014580               30  \n",
       "50           0.757794         0.752472        0.019950               59  \n",
       "51           0.789720         0.776439        0.012865                9  \n",
       "52           0.790805         0.777248        0.014669                7  \n",
       "53           0.781609         0.776861        0.012141                8  \n",
       "54           0.781609         0.774122        0.013975               12  \n",
       "55           0.792541         0.775328        0.016778               11  \n",
       "56           0.775414         0.773789        0.011261               13  \n",
       "57           0.785047         0.775412        0.011641               10  \n",
       "58           0.779582         0.779764        0.011500                3  \n",
       "59           0.787879         0.779972        0.010760                2  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_rfc_results = pd.DataFrame(grid_rfc.cv_results_)\n",
    "df_grid_rfc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[983 447]\n",
      " [ 99 447]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.78      1430\n",
      "           1       0.50      0.82      0.62       546\n",
      "\n",
      "    accuracy                           0.72      1976\n",
      "   macro avg       0.70      0.75      0.70      1976\n",
      "weighted avg       0.80      0.72      0.74      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_rfc_train_predict = grid_rfc.predict(X_train_rus)\n",
    "y_rfc_predict = grid_rfc.predict(X_test)\n",
    "rfc_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_rfc_predict)\n",
    "rfc_classification_report = classification_report(y_true = y_test, y_pred = y_rfc_predict)\n",
    "print(rfc_cmatrix)\n",
    "print()\n",
    "print(rfc_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.8170289855072463\n",
      "Test F1-Score: 0.6208333333333333\n"
     ]
    }
   ],
   "source": [
    "rfc_classifier = RandomForestClassifier(random_state=0, criterion='entropy', max_depth=6, n_estimators=200)\n",
    "rfc_classifier.fit(X_train_rus,y_train_rus)\n",
    "print('Train F1-Score:', f1_score(y_true=y_train_rus, y_pred=rfc_classifier.predict(X_train_rus)))\n",
    "print('Test F1-Score:', f1_score(y_true=y_test, y_pred=rfc_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Random Forest\", \"max_depth = 6, n_estimators = 200, criterion = 'entropy'\", f1_score(y_true=y_train_rus, y_pred=rfc_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=rfc_classifier.predict(X_test)),\n",
    "                  df_grid_rfc_results[df_grid_rfc_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Classification Model  \\\n",
       "0                              KNN   \n",
       "1              Logistic Regression   \n",
       "2               SVM (kernel='rbf')   \n",
       "3  SVM (kernel='poly', degree = 3)   \n",
       "4            SVM (kernel='linear')   \n",
       "5                       Linear SVM   \n",
       "6                    Decision Tree   \n",
       "7                    Random Forest   \n",
       "\n",
       "                                     Model Parameter  Train F1-Score  \\\n",
       "0             n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                           C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                              C = 1, gamma = 'auto'        0.819056   \n",
       "3                             C = 1, gamma = 'scale'        0.835155   \n",
       "4                                            C = 100        0.770344   \n",
       "5                          C = 0.001, penalty = 'l2'        0.783514   \n",
       "6  max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7  max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "\n",
       "   Test F1-Score  Best Mean Test F1-Score  \n",
       "0       0.584980                 0.788615  \n",
       "1       0.609739                 0.779720  \n",
       "2       0.606868                 0.776477  \n",
       "3       0.602759                 0.770910  \n",
       "4       0.613128                 0.767012  \n",
       "5       0.602663                 0.777433  \n",
       "6       0.596774                 0.770291  \n",
       "7       0.620833                 0.781263  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers\n",
    "#### Hard Voting Classifier with the SVM Poly Kernel (Degree = 3) Classification Model and Decision Tree Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8034107058266224"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_poly = svm.SVC(kernel='poly', degree=3, random_state=0)\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "hard_voting_classifier = VotingClassifier(estimators=[('svc_poly', svc_poly), ('dtree', dtree)], voting='hard')\n",
    "grid_hard_voting_classifier_params = {'svc_poly__C': [0.01, 0.1, 1, 10, 100], 'dtree__max_depth': [1, 2, 3, 4, 5, 6]}\n",
    "grid_hard_voting_classifier = GridSearchCV(hard_voting_classifier, param_grid=grid_hard_voting_classifier_params, cv = 5,\n",
    "                                          scoring='f1')\n",
    "grid_hard_voting_classifier.fit(X_train_rus, y_train_rus)\n",
    "grid_hard_voting_classifier.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616519174041298"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_hard_voting_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtree__max_depth': 1, 'svc_poly__C': 1}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_hard_voting_classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7629528064951349"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_hard_voting_classifier.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_dtree__max_depth</th>\n",
       "      <th>param_svc_poly__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097164</td>\n",
       "      <td>0.010773</td>\n",
       "      <td>0.020595</td>\n",
       "      <td>0.006426</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.739910</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.770241</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.760353</td>\n",
       "      <td>0.016830</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078496</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.016489</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.781857</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.760325</td>\n",
       "      <td>0.017636</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.079342</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>0.016367</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 1}</td>\n",
       "      <td>0.748837</td>\n",
       "      <td>0.730679</td>\n",
       "      <td>0.757647</td>\n",
       "      <td>0.810934</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.762953</td>\n",
       "      <td>0.026777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108059</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>0.015816</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 10}</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.720988</td>\n",
       "      <td>0.750605</td>\n",
       "      <td>0.731830</td>\n",
       "      <td>0.727704</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187035</td>\n",
       "      <td>0.020091</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 100}</td>\n",
       "      <td>0.691729</td>\n",
       "      <td>0.688776</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.728643</td>\n",
       "      <td>0.704961</td>\n",
       "      <td>0.707266</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.134878</td>\n",
       "      <td>0.043522</td>\n",
       "      <td>0.026719</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.739910</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.770241</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.760353</td>\n",
       "      <td>0.016830</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.091652</td>\n",
       "      <td>0.013806</td>\n",
       "      <td>0.019020</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.781857</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.760325</td>\n",
       "      <td>0.017636</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089344</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.017815</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 1}</td>\n",
       "      <td>0.748837</td>\n",
       "      <td>0.730679</td>\n",
       "      <td>0.757647</td>\n",
       "      <td>0.810934</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.762953</td>\n",
       "      <td>0.026777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.113442</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>0.012949</td>\n",
       "      <td>0.005883</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 10}</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.720988</td>\n",
       "      <td>0.750605</td>\n",
       "      <td>0.731830</td>\n",
       "      <td>0.727704</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.250118</td>\n",
       "      <td>0.096453</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 100}</td>\n",
       "      <td>0.691729</td>\n",
       "      <td>0.688776</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.728643</td>\n",
       "      <td>0.704961</td>\n",
       "      <td>0.707266</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.106118</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.677003</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.739695</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.084176</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.020736</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.776524</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.744079</td>\n",
       "      <td>0.032984</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.083578</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 1}</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.671958</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.786047</td>\n",
       "      <td>0.749367</td>\n",
       "      <td>0.740071</td>\n",
       "      <td>0.039416</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.107512</td>\n",
       "      <td>0.007175</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 10}</td>\n",
       "      <td>0.710327</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.716049</td>\n",
       "      <td>0.737101</td>\n",
       "      <td>0.721485</td>\n",
       "      <td>0.706663</td>\n",
       "      <td>0.030489</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.019242</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 100}</td>\n",
       "      <td>0.673797</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.720812</td>\n",
       "      <td>0.685083</td>\n",
       "      <td>0.683938</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.094761</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.022725</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.712589</td>\n",
       "      <td>0.720183</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.764579</td>\n",
       "      <td>0.758454</td>\n",
       "      <td>0.742563</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.079187</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.700980</td>\n",
       "      <td>0.720183</td>\n",
       "      <td>0.757647</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.757282</td>\n",
       "      <td>0.741201</td>\n",
       "      <td>0.026126</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.076781</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.015964</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 1}</td>\n",
       "      <td>0.710327</td>\n",
       "      <td>0.706161</td>\n",
       "      <td>0.748166</td>\n",
       "      <td>0.784404</td>\n",
       "      <td>0.753117</td>\n",
       "      <td>0.740435</td>\n",
       "      <td>0.029103</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.102725</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.015366</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 10}</td>\n",
       "      <td>0.682415</td>\n",
       "      <td>0.700252</td>\n",
       "      <td>0.709512</td>\n",
       "      <td>0.736077</td>\n",
       "      <td>0.723861</td>\n",
       "      <td>0.710423</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.200843</td>\n",
       "      <td>0.028999</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 100}</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.716792</td>\n",
       "      <td>0.690808</td>\n",
       "      <td>0.679811</td>\n",
       "      <td>0.028089</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.719626</td>\n",
       "      <td>0.735802</td>\n",
       "      <td>0.763218</td>\n",
       "      <td>0.769932</td>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.751415</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.238762</td>\n",
       "      <td>0.110580</td>\n",
       "      <td>0.060239</td>\n",
       "      <td>0.030783</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.716707</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.768868</td>\n",
       "      <td>0.775982</td>\n",
       "      <td>0.766265</td>\n",
       "      <td>0.753564</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.040286</td>\n",
       "      <td>0.029399</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 1}</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.715365</td>\n",
       "      <td>0.755448</td>\n",
       "      <td>0.789598</td>\n",
       "      <td>0.766585</td>\n",
       "      <td>0.751741</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.154187</td>\n",
       "      <td>0.030582</td>\n",
       "      <td>0.019348</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 10}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.687664</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.742105</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>0.022788</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.441706</td>\n",
       "      <td>0.067657</td>\n",
       "      <td>0.029123</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 100}</td>\n",
       "      <td>0.661458</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.712401</td>\n",
       "      <td>0.720627</td>\n",
       "      <td>0.706849</td>\n",
       "      <td>0.692876</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.125952</td>\n",
       "      <td>0.031205</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.745843</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.745283</td>\n",
       "      <td>0.769585</td>\n",
       "      <td>0.789838</td>\n",
       "      <td>0.760354</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.121668</td>\n",
       "      <td>0.049236</td>\n",
       "      <td>0.029394</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.757426</td>\n",
       "      <td>0.752381</td>\n",
       "      <td>0.777518</td>\n",
       "      <td>0.769596</td>\n",
       "      <td>0.760404</td>\n",
       "      <td>0.011704</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.082958</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.014123</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 1}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.743842</td>\n",
       "      <td>0.787589</td>\n",
       "      <td>0.768856</td>\n",
       "      <td>0.757058</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.116466</td>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.016095</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 10}</td>\n",
       "      <td>0.710327</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.739796</td>\n",
       "      <td>0.748052</td>\n",
       "      <td>0.720757</td>\n",
       "      <td>0.019538</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.192180</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.017122</td>\n",
       "      <td>0.010330</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 100}</td>\n",
       "      <td>0.678947</td>\n",
       "      <td>0.675603</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.719577</td>\n",
       "      <td>0.713514</td>\n",
       "      <td>0.695094</td>\n",
       "      <td>0.018067</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.097164      0.010773         0.020595        0.006426   \n",
       "1        0.078496      0.000785         0.016489        0.001729   \n",
       "2        0.079342      0.005834         0.016367        0.000936   \n",
       "3        0.108059      0.013355         0.015816        0.000160   \n",
       "4        0.187035      0.020091         0.012504        0.006252   \n",
       "5        0.134878      0.043522         0.026719        0.006321   \n",
       "6        0.091652      0.013806         0.019020        0.003074   \n",
       "7        0.089344      0.002039         0.017815        0.000899   \n",
       "8        0.113442      0.011436         0.012949        0.005883   \n",
       "9        0.250118      0.096453         0.015571        0.001479   \n",
       "10       0.106118      0.006830         0.023728        0.000734   \n",
       "11       0.084176      0.004308         0.020736        0.001461   \n",
       "12       0.083578      0.003914         0.018349        0.001498   \n",
       "13       0.107512      0.007175         0.015559        0.000489   \n",
       "14       0.186900      0.019242         0.013157        0.001141   \n",
       "15       0.094761      0.001410         0.022725        0.000742   \n",
       "16       0.079187      0.002054         0.018558        0.000494   \n",
       "17       0.076781      0.001401         0.015964        0.000631   \n",
       "18       0.102725      0.005462         0.015366        0.000794   \n",
       "19       0.200843      0.028999         0.014381        0.001483   \n",
       "20       0.104315      0.004376         0.026130        0.001466   \n",
       "21       0.238762      0.110580         0.060239        0.030783   \n",
       "22       0.165781      0.085365         0.040286        0.029399   \n",
       "23       0.154187      0.030582         0.019348        0.002721   \n",
       "24       0.441706      0.067657         0.029123        0.004342   \n",
       "25       0.125952      0.031205         0.025835        0.004623   \n",
       "26       0.121668      0.049236         0.029394        0.007644   \n",
       "27       0.082958      0.003225         0.014123        0.006090   \n",
       "28       0.116466      0.006525         0.016095        0.000767   \n",
       "29       0.192180      0.021634         0.017122        0.010330   \n",
       "\n",
       "   param_dtree__max_depth param_svc_poly__C  \\\n",
       "0                       1              0.01   \n",
       "1                       1               0.1   \n",
       "2                       1                 1   \n",
       "3                       1                10   \n",
       "4                       1               100   \n",
       "5                       2              0.01   \n",
       "6                       2               0.1   \n",
       "7                       2                 1   \n",
       "8                       2                10   \n",
       "9                       2               100   \n",
       "10                      3              0.01   \n",
       "11                      3               0.1   \n",
       "12                      3                 1   \n",
       "13                      3                10   \n",
       "14                      3               100   \n",
       "15                      4              0.01   \n",
       "16                      4               0.1   \n",
       "17                      4                 1   \n",
       "18                      4                10   \n",
       "19                      4               100   \n",
       "20                      5              0.01   \n",
       "21                      5               0.1   \n",
       "22                      5                 1   \n",
       "23                      5                10   \n",
       "24                      5               100   \n",
       "25                      6              0.01   \n",
       "26                      6               0.1   \n",
       "27                      6                 1   \n",
       "28                      6                10   \n",
       "29                      6               100   \n",
       "\n",
       "                                          params  split0_test_score  \\\n",
       "0   {'dtree__max_depth': 1, 'svc_poly__C': 0.01}           0.739910   \n",
       "1    {'dtree__max_depth': 1, 'svc_poly__C': 0.1}           0.738739   \n",
       "2      {'dtree__max_depth': 1, 'svc_poly__C': 1}           0.748837   \n",
       "3     {'dtree__max_depth': 1, 'svc_poly__C': 10}           0.719048   \n",
       "4    {'dtree__max_depth': 1, 'svc_poly__C': 100}           0.691729   \n",
       "5   {'dtree__max_depth': 2, 'svc_poly__C': 0.01}           0.739910   \n",
       "6    {'dtree__max_depth': 2, 'svc_poly__C': 0.1}           0.738739   \n",
       "7      {'dtree__max_depth': 2, 'svc_poly__C': 1}           0.748837   \n",
       "8     {'dtree__max_depth': 2, 'svc_poly__C': 10}           0.719048   \n",
       "9    {'dtree__max_depth': 2, 'svc_poly__C': 100}           0.691729   \n",
       "10  {'dtree__max_depth': 3, 'svc_poly__C': 0.01}           0.727273   \n",
       "11   {'dtree__max_depth': 3, 'svc_poly__C': 0.1}           0.727273   \n",
       "12     {'dtree__max_depth': 3, 'svc_poly__C': 1}           0.725926   \n",
       "13    {'dtree__max_depth': 3, 'svc_poly__C': 10}           0.710327   \n",
       "14   {'dtree__max_depth': 3, 'svc_poly__C': 100}           0.673797   \n",
       "15  {'dtree__max_depth': 4, 'svc_poly__C': 0.01}           0.712589   \n",
       "16   {'dtree__max_depth': 4, 'svc_poly__C': 0.1}           0.700980   \n",
       "17     {'dtree__max_depth': 4, 'svc_poly__C': 1}           0.710327   \n",
       "18    {'dtree__max_depth': 4, 'svc_poly__C': 10}           0.682415   \n",
       "19   {'dtree__max_depth': 4, 'svc_poly__C': 100}           0.631579   \n",
       "20  {'dtree__max_depth': 5, 'svc_poly__C': 0.01}           0.719626   \n",
       "21   {'dtree__max_depth': 5, 'svc_poly__C': 0.1}           0.716707   \n",
       "22     {'dtree__max_depth': 5, 'svc_poly__C': 1}           0.731707   \n",
       "23    {'dtree__max_depth': 5, 'svc_poly__C': 10}           0.700000   \n",
       "24   {'dtree__max_depth': 5, 'svc_poly__C': 100}           0.661458   \n",
       "25  {'dtree__max_depth': 6, 'svc_poly__C': 0.01}           0.745843   \n",
       "26   {'dtree__max_depth': 6, 'svc_poly__C': 0.1}           0.745098   \n",
       "27     {'dtree__max_depth': 6, 'svc_poly__C': 1}           0.750000   \n",
       "28    {'dtree__max_depth': 6, 'svc_poly__C': 10}           0.710327   \n",
       "29   {'dtree__max_depth': 6, 'svc_poly__C': 100}           0.678947   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.740088           0.770241           0.778495   \n",
       "1            0.740088           0.768212           0.781857   \n",
       "2            0.730679           0.757647           0.810934   \n",
       "3            0.716049           0.720988           0.750605   \n",
       "4            0.688776           0.722222           0.728643   \n",
       "5            0.740088           0.770241           0.778495   \n",
       "6            0.740088           0.768212           0.781857   \n",
       "7            0.730679           0.757647           0.810934   \n",
       "8            0.716049           0.720988           0.750605   \n",
       "9            0.688776           0.722222           0.728643   \n",
       "10           0.677003           0.769231           0.768212   \n",
       "11           0.687831           0.776524           0.772009   \n",
       "12           0.671958           0.767059           0.786047   \n",
       "13           0.648352           0.716049           0.737101   \n",
       "14           0.625000           0.715000           0.720812   \n",
       "15           0.720183           0.757009           0.764579   \n",
       "16           0.720183           0.757647           0.769912   \n",
       "17           0.706161           0.748166           0.784404   \n",
       "18           0.700252           0.709512           0.736077   \n",
       "19           0.671875           0.688000           0.716792   \n",
       "20           0.735802           0.763218           0.769932   \n",
       "21           0.740000           0.768868           0.775982   \n",
       "22           0.715365           0.755448           0.789598   \n",
       "23           0.687664           0.732824           0.742424   \n",
       "24           0.663043           0.712401           0.720627   \n",
       "25           0.751220           0.745283           0.769585   \n",
       "26           0.757426           0.752381           0.777518   \n",
       "27           0.735000           0.743842           0.787589   \n",
       "28           0.697917           0.707692           0.739796   \n",
       "29           0.675603           0.687831           0.719577   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.773034         0.760353        0.016830                5  \n",
       "1            0.772727         0.760325        0.017636                7  \n",
       "2            0.766667         0.762953        0.026777                1  \n",
       "3            0.731830         0.727704        0.012628               19  \n",
       "4            0.704961         0.707266        0.015933               24  \n",
       "5            0.773034         0.760353        0.016830                5  \n",
       "6            0.772727         0.760325        0.017636                7  \n",
       "7            0.766667         0.762953        0.026777                1  \n",
       "8            0.731830         0.727704        0.012628               19  \n",
       "9            0.704961         0.707266        0.015933               24  \n",
       "10           0.756757         0.739695        0.034818               18  \n",
       "11           0.756757         0.744079        0.032984               13  \n",
       "12           0.749367         0.740071        0.039416               17  \n",
       "13           0.721485         0.706663        0.030489               26  \n",
       "14           0.685083         0.683938        0.034358               29  \n",
       "15           0.758454         0.742563        0.021657               14  \n",
       "16           0.757282         0.741201        0.026126               15  \n",
       "17           0.753117         0.740435        0.029103               16  \n",
       "18           0.723861         0.710423        0.018587               23  \n",
       "19           0.690808         0.679811        0.028089               30  \n",
       "20           0.768496         0.751415        0.020141               12  \n",
       "21           0.766265         0.753564        0.022093               10  \n",
       "22           0.766585         0.751741        0.026045               11  \n",
       "23           0.742105         0.721004        0.022788               21  \n",
       "24           0.706849         0.692876        0.025391               28  \n",
       "25           0.789838         0.760354        0.017180                4  \n",
       "26           0.769596         0.760404        0.011704                3  \n",
       "27           0.768856         0.757058        0.018881                9  \n",
       "28           0.748052         0.720757        0.019538               22  \n",
       "29           0.713514         0.695094        0.018067               27  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_hard_voting_classifier_results = pd.DataFrame(grid_hard_voting_classifier.cv_results_)\n",
    "df_grid_hard_voting_classifier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1038  392]\n",
      " [ 128  418]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80      1430\n",
      "           1       0.52      0.77      0.62       546\n",
      "\n",
      "    accuracy                           0.74      1976\n",
      "   macro avg       0.70      0.75      0.71      1976\n",
      "weighted avg       0.79      0.74      0.75      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hard_voting_train_predict = grid_hard_voting_classifier.predict(X_train_rus)\n",
    "y_hard_voting_predict = grid_hard_voting_classifier.predict(X_test)\n",
    "hard_voting_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_hard_voting_predict)\n",
    "hard_voting_classification_report = classification_report(y_true = y_test, y_pred = y_hard_voting_predict)\n",
    "print(hard_voting_cmatrix)\n",
    "print()\n",
    "print(hard_voting_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Hard Voting (SVM Poly Kernel, Decision Tree)\", \"C = 1, max_depth = 1\", f1_score(y_true=y_train_rus, y_pred=grid_hard_voting_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=grid_hard_voting_classifier.predict(X_test)),\n",
    "                  df_grid_hard_voting_classifier_results[df_grid_hard_voting_classifier_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Classification Model  \\\n",
       "0                                           KNN   \n",
       "1                           Logistic Regression   \n",
       "2                            SVM (kernel='rbf')   \n",
       "3               SVM (kernel='poly', degree = 3)   \n",
       "4                         SVM (kernel='linear')   \n",
       "5                                    Linear SVM   \n",
       "6                                 Decision Tree   \n",
       "7                                 Random Forest   \n",
       "8  Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "\n",
       "                                     Model Parameter  Train F1-Score  \\\n",
       "0             n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                           C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                              C = 1, gamma = 'auto'        0.819056   \n",
       "3                             C = 1, gamma = 'scale'        0.835155   \n",
       "4                                            C = 100        0.770344   \n",
       "5                          C = 0.001, penalty = 'l2'        0.783514   \n",
       "6  max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7  max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                               C = 1, max_depth = 1        0.803411   \n",
       "\n",
       "   Test F1-Score  Best Mean Test F1-Score  \n",
       "0       0.584980                 0.788615  \n",
       "1       0.609739                 0.779720  \n",
       "2       0.606868                 0.776477  \n",
       "3       0.602759                 0.770910  \n",
       "4       0.613128                 0.767012  \n",
       "5       0.602663                 0.777433  \n",
       "6       0.596774                 0.770291  \n",
       "7       0.620833                 0.781263  \n",
       "8       0.616519                 0.762953  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting Classifier with the SVM Poly Kernel (Degree = 3) Classification Model and Decision Tree Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8432869330938481"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_poly = svm.SVC(kernel='poly', degree=3, random_state=0, probability=True)\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "soft_voting_classifier = VotingClassifier(estimators=[('svc_poly', svc_poly), ('dtree', dtree)], voting='soft')\n",
    "grid_soft_voting_classifier_params = {'svc_poly__C': [0.01, 0.1, 1, 10, 100], 'dtree__max_depth': [1, 2, 3, 4, 5, 6]}\n",
    "grid_soft_voting_classifier = GridSearchCV(soft_voting_classifier, param_grid=grid_hard_voting_classifier_params, cv = 5,\n",
    "                                          scoring='f1')\n",
    "grid_soft_voting_classifier.fit(X_train_rus, y_train_rus)\n",
    "grid_soft_voting_classifier.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086369770580297"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_soft_voting_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtree__max_depth': 3, 'svc_poly__C': 10}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_soft_voting_classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7710485070922833"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_soft_voting_classifier.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_dtree__max_depth</th>\n",
       "      <th>param_svc_poly__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.589229</td>\n",
       "      <td>0.139073</td>\n",
       "      <td>0.021878</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.739910</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.770241</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.760353</td>\n",
       "      <td>0.016830</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.484314</td>\n",
       "      <td>0.061628</td>\n",
       "      <td>0.024376</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.742729</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.764835</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.773756</td>\n",
       "      <td>0.759981</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.449455</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.016742</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 1}</td>\n",
       "      <td>0.741722</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.799136</td>\n",
       "      <td>0.771689</td>\n",
       "      <td>0.763416</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.480234</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.007388</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 10}</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.745011</td>\n",
       "      <td>0.782418</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.764440</td>\n",
       "      <td>0.015661</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.848769</td>\n",
       "      <td>0.107446</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 1, 'svc_poly__C': 100}</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.733624</td>\n",
       "      <td>0.760259</td>\n",
       "      <td>0.776824</td>\n",
       "      <td>0.749446</td>\n",
       "      <td>0.752307</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.479623</td>\n",
       "      <td>0.018560</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.740088</td>\n",
       "      <td>0.767544</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.759942</td>\n",
       "      <td>0.015995</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.390539</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.752784</td>\n",
       "      <td>0.769575</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.764977</td>\n",
       "      <td>0.762618</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.365531</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 1}</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.770302</td>\n",
       "      <td>0.763688</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.469445</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 10}</td>\n",
       "      <td>0.714932</td>\n",
       "      <td>0.738041</td>\n",
       "      <td>0.753880</td>\n",
       "      <td>0.792123</td>\n",
       "      <td>0.777011</td>\n",
       "      <td>0.755197</td>\n",
       "      <td>0.027411</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.709206</td>\n",
       "      <td>0.023379</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 2, 'svc_poly__C': 100}</td>\n",
       "      <td>0.721604</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.779343</td>\n",
       "      <td>0.756532</td>\n",
       "      <td>0.022128</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.495860</td>\n",
       "      <td>0.013888</td>\n",
       "      <td>0.021871</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.737819</td>\n",
       "      <td>0.757370</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.781659</td>\n",
       "      <td>0.775120</td>\n",
       "      <td>0.764654</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>0.018754</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.743649</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.779043</td>\n",
       "      <td>0.784922</td>\n",
       "      <td>0.777251</td>\n",
       "      <td>0.769310</td>\n",
       "      <td>0.014955</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.368655</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 1}</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.751152</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.792035</td>\n",
       "      <td>0.784038</td>\n",
       "      <td>0.768639</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.457014</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>0.012489</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 10}</td>\n",
       "      <td>0.749436</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.760626</td>\n",
       "      <td>0.789357</td>\n",
       "      <td>0.797203</td>\n",
       "      <td>0.771049</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.713931</td>\n",
       "      <td>0.030162</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 3, 'svc_poly__C': 100}</td>\n",
       "      <td>0.744493</td>\n",
       "      <td>0.748268</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.774617</td>\n",
       "      <td>0.784870</td>\n",
       "      <td>0.766590</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.581127</td>\n",
       "      <td>0.042125</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.723112</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.767241</td>\n",
       "      <td>0.776256</td>\n",
       "      <td>0.755763</td>\n",
       "      <td>0.020097</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.437529</td>\n",
       "      <td>0.040992</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.732265</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.781457</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.758402</td>\n",
       "      <td>0.020359</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.410585</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.015628</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 1}</td>\n",
       "      <td>0.737819</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.775229</td>\n",
       "      <td>0.784753</td>\n",
       "      <td>0.776978</td>\n",
       "      <td>0.764956</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.532236</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>0.008848</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 10}</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.748837</td>\n",
       "      <td>0.762557</td>\n",
       "      <td>0.782998</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.760274</td>\n",
       "      <td>0.021226</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.811579</td>\n",
       "      <td>0.101723</td>\n",
       "      <td>0.017225</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 4, 'svc_poly__C': 100}</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>0.733945</td>\n",
       "      <td>0.766816</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.760148</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.512072</td>\n",
       "      <td>0.018472</td>\n",
       "      <td>0.025093</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.742597</td>\n",
       "      <td>0.762125</td>\n",
       "      <td>0.776524</td>\n",
       "      <td>0.767184</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.763891</td>\n",
       "      <td>0.011644</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.456331</td>\n",
       "      <td>0.068522</td>\n",
       "      <td>0.020077</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.736597</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.776744</td>\n",
       "      <td>0.775056</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.763641</td>\n",
       "      <td>0.014910</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.494326</td>\n",
       "      <td>0.053975</td>\n",
       "      <td>0.018552</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 1}</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.762125</td>\n",
       "      <td>0.784753</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.764455</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.597415</td>\n",
       "      <td>0.108419</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 10}</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.765661</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.763118</td>\n",
       "      <td>0.012947</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.752254</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.011021</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 5, 'svc_poly__C': 100}</td>\n",
       "      <td>0.745536</td>\n",
       "      <td>0.747045</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.788288</td>\n",
       "      <td>0.777518</td>\n",
       "      <td>0.764618</td>\n",
       "      <td>0.016731</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.506777</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 0.01}</td>\n",
       "      <td>0.750583</td>\n",
       "      <td>0.739229</td>\n",
       "      <td>0.756881</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.790805</td>\n",
       "      <td>0.762045</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.421088</td>\n",
       "      <td>0.010335</td>\n",
       "      <td>0.018489</td>\n",
       "      <td>0.007343</td>\n",
       "      <td>6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 0.1}</td>\n",
       "      <td>0.735363</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.751152</td>\n",
       "      <td>0.784580</td>\n",
       "      <td>0.772093</td>\n",
       "      <td>0.760490</td>\n",
       "      <td>0.016941</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.401631</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 1}</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.751708</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.763916</td>\n",
       "      <td>0.015453</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.488741</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>0.014026</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 10}</td>\n",
       "      <td>0.741419</td>\n",
       "      <td>0.768879</td>\n",
       "      <td>0.752834</td>\n",
       "      <td>0.778523</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.761523</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.728838</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>{'dtree__max_depth': 6, 'svc_poly__C': 100}</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.760369</td>\n",
       "      <td>0.766816</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.784870</td>\n",
       "      <td>0.766712</td>\n",
       "      <td>0.013240</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.589229      0.139073         0.021878        0.005863   \n",
       "1        0.484314      0.061628         0.024376        0.003781   \n",
       "2        0.449455      0.015802         0.016742        0.001478   \n",
       "3        0.480234      0.011883         0.012682        0.007388   \n",
       "4        0.848769      0.107446         0.008310        0.006854   \n",
       "5        0.479623      0.018560         0.022553        0.007200   \n",
       "6        0.390539      0.013965         0.015615        0.000013   \n",
       "7        0.365531      0.007671         0.015629        0.000011   \n",
       "8        0.469445      0.011176         0.008688        0.007199   \n",
       "9        0.709206      0.023379         0.009374        0.007654   \n",
       "10       0.495860      0.013888         0.021871        0.007639   \n",
       "11       0.387400      0.006247         0.018754        0.006245   \n",
       "12       0.368655      0.012492         0.015630        0.000025   \n",
       "13       0.457014      0.010372         0.012489        0.006244   \n",
       "14       0.713931      0.030162         0.009365        0.007647   \n",
       "15       0.581127      0.042125         0.030345        0.004648   \n",
       "16       0.437529      0.040992         0.021134        0.009038   \n",
       "17       0.410585      0.007998         0.015628        0.000408   \n",
       "18       0.532236      0.020293         0.008848        0.007290   \n",
       "19       0.811579      0.101723         0.017225        0.005446   \n",
       "20       0.512072      0.018472         0.025093        0.006536   \n",
       "21       0.456331      0.068522         0.020077        0.005812   \n",
       "22       0.494326      0.053975         0.018552        0.003711   \n",
       "23       0.597415      0.108419         0.014288        0.000918   \n",
       "24       0.752254      0.032408         0.011021        0.010364   \n",
       "25       0.506777      0.008178         0.021958        0.005594   \n",
       "26       0.421088      0.010335         0.018489        0.007343   \n",
       "27       0.401631      0.010672         0.016560        0.001698   \n",
       "28       0.488741      0.010328         0.014026        0.001444   \n",
       "29       0.728838      0.036740         0.013367        0.006888   \n",
       "\n",
       "   param_dtree__max_depth param_svc_poly__C  \\\n",
       "0                       1              0.01   \n",
       "1                       1               0.1   \n",
       "2                       1                 1   \n",
       "3                       1                10   \n",
       "4                       1               100   \n",
       "5                       2              0.01   \n",
       "6                       2               0.1   \n",
       "7                       2                 1   \n",
       "8                       2                10   \n",
       "9                       2               100   \n",
       "10                      3              0.01   \n",
       "11                      3               0.1   \n",
       "12                      3                 1   \n",
       "13                      3                10   \n",
       "14                      3               100   \n",
       "15                      4              0.01   \n",
       "16                      4               0.1   \n",
       "17                      4                 1   \n",
       "18                      4                10   \n",
       "19                      4               100   \n",
       "20                      5              0.01   \n",
       "21                      5               0.1   \n",
       "22                      5                 1   \n",
       "23                      5                10   \n",
       "24                      5               100   \n",
       "25                      6              0.01   \n",
       "26                      6               0.1   \n",
       "27                      6                 1   \n",
       "28                      6                10   \n",
       "29                      6               100   \n",
       "\n",
       "                                          params  split0_test_score  \\\n",
       "0   {'dtree__max_depth': 1, 'svc_poly__C': 0.01}           0.739910   \n",
       "1    {'dtree__max_depth': 1, 'svc_poly__C': 0.1}           0.742729   \n",
       "2      {'dtree__max_depth': 1, 'svc_poly__C': 1}           0.741722   \n",
       "3     {'dtree__max_depth': 1, 'svc_poly__C': 10}           0.746725   \n",
       "4    {'dtree__max_depth': 1, 'svc_poly__C': 100}           0.741379   \n",
       "5   {'dtree__max_depth': 2, 'svc_poly__C': 0.01}           0.741573   \n",
       "6    {'dtree__max_depth': 2, 'svc_poly__C': 0.1}           0.739726   \n",
       "7      {'dtree__max_depth': 2, 'svc_poly__C': 1}           0.736111   \n",
       "8     {'dtree__max_depth': 2, 'svc_poly__C': 10}           0.714932   \n",
       "9    {'dtree__max_depth': 2, 'svc_poly__C': 100}           0.721604   \n",
       "10  {'dtree__max_depth': 3, 'svc_poly__C': 0.01}           0.737819   \n",
       "11   {'dtree__max_depth': 3, 'svc_poly__C': 0.1}           0.743649   \n",
       "12     {'dtree__max_depth': 3, 'svc_poly__C': 1}           0.743243   \n",
       "13    {'dtree__max_depth': 3, 'svc_poly__C': 10}           0.749436   \n",
       "14   {'dtree__max_depth': 3, 'svc_poly__C': 100}           0.744493   \n",
       "15  {'dtree__max_depth': 4, 'svc_poly__C': 0.01}           0.741935   \n",
       "16   {'dtree__max_depth': 4, 'svc_poly__C': 0.1}           0.736111   \n",
       "17     {'dtree__max_depth': 4, 'svc_poly__C': 1}           0.737819   \n",
       "18    {'dtree__max_depth': 4, 'svc_poly__C': 10}           0.726027   \n",
       "19   {'dtree__max_depth': 4, 'svc_poly__C': 100}           0.744395   \n",
       "20  {'dtree__max_depth': 5, 'svc_poly__C': 0.01}           0.742597   \n",
       "21   {'dtree__max_depth': 5, 'svc_poly__C': 0.1}           0.736597   \n",
       "22     {'dtree__max_depth': 5, 'svc_poly__C': 1}           0.744828   \n",
       "23    {'dtree__max_depth': 5, 'svc_poly__C': 10}           0.739726   \n",
       "24   {'dtree__max_depth': 5, 'svc_poly__C': 100}           0.745536   \n",
       "25  {'dtree__max_depth': 6, 'svc_poly__C': 0.01}           0.750583   \n",
       "26   {'dtree__max_depth': 6, 'svc_poly__C': 0.1}           0.735363   \n",
       "27     {'dtree__max_depth': 6, 'svc_poly__C': 1}           0.744186   \n",
       "28    {'dtree__max_depth': 6, 'svc_poly__C': 10}           0.741419   \n",
       "29   {'dtree__max_depth': 6, 'svc_poly__C': 100}           0.745995   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.740088           0.770241           0.778495   \n",
       "1            0.740088           0.764835           0.778495   \n",
       "2            0.740088           0.764444           0.799136   \n",
       "3            0.745011           0.782418           0.777778   \n",
       "4            0.733624           0.760259           0.776824   \n",
       "5            0.740088           0.767544           0.778495   \n",
       "6            0.752784           0.769575           0.786026   \n",
       "7            0.741935           0.756757           0.813333   \n",
       "8            0.738041           0.753880           0.792123   \n",
       "9            0.745455           0.755556           0.780702   \n",
       "10           0.757370           0.771300           0.781659   \n",
       "11           0.761682           0.779043           0.784922   \n",
       "12           0.751152           0.772727           0.792035   \n",
       "13           0.758621           0.760626           0.789357   \n",
       "14           0.748268           0.780702           0.774617   \n",
       "15           0.723112           0.770270           0.767241   \n",
       "16           0.732265           0.775510           0.781457   \n",
       "17           0.750000           0.775229           0.784753   \n",
       "18           0.748837           0.762557           0.782998   \n",
       "19           0.733945           0.766816           0.771930   \n",
       "20           0.762125           0.776524           0.767184   \n",
       "21           0.758782           0.776744           0.775056   \n",
       "22           0.754098           0.762125           0.784753   \n",
       "23           0.765661           0.763636           0.779510   \n",
       "24           0.747045           0.764706           0.788288   \n",
       "25           0.739229           0.756881           0.772727   \n",
       "26           0.759259           0.751152           0.784580   \n",
       "27           0.761468           0.751708           0.786517   \n",
       "28           0.768879           0.752834           0.778523   \n",
       "29           0.760369           0.766816           0.775510   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.773034         0.760353        0.016830               21  \n",
       "1            0.773756         0.759981        0.015808               24  \n",
       "2            0.771689         0.763416        0.021727               15  \n",
       "3            0.770270         0.764440        0.015661               10  \n",
       "4            0.749446         0.752307        0.015106               30  \n",
       "5            0.772009         0.759942        0.015995               25  \n",
       "6            0.764977         0.762618        0.015638               17  \n",
       "7            0.770302         0.763688        0.027532               13  \n",
       "8            0.777011         0.755197        0.027411               29  \n",
       "9            0.779343         0.756532        0.022128               27  \n",
       "10           0.775120         0.764654        0.015595                7  \n",
       "11           0.777251         0.769310        0.014955                2  \n",
       "12           0.784038         0.768639        0.018719                3  \n",
       "13           0.797203         0.771049        0.018705                1  \n",
       "14           0.784870         0.766590        0.016862                5  \n",
       "15           0.776256         0.755763        0.020097               28  \n",
       "16           0.766667         0.758402        0.020359               26  \n",
       "17           0.776978         0.764956        0.017900                6  \n",
       "18           0.780952         0.760274        0.021226               22  \n",
       "19           0.783654         0.760148        0.018279               23  \n",
       "20           0.771028         0.763891        0.011644               12  \n",
       "21           0.771028         0.763641        0.014910               14  \n",
       "22           0.776471         0.764455        0.014521                9  \n",
       "23           0.767059         0.763118        0.012947               16  \n",
       "24           0.777518         0.764618        0.016731                8  \n",
       "25           0.790805         0.762045        0.017999               18  \n",
       "26           0.772093         0.760490        0.016941               20  \n",
       "27           0.775701         0.763916        0.015453               11  \n",
       "28           0.765957         0.761523        0.012980               19  \n",
       "29           0.784870         0.766712        0.013240                4  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_soft_voting_classifier_results = pd.DataFrame(grid_soft_voting_classifier.cv_results_)\n",
    "df_grid_soft_voting_classifier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[945 485]\n",
      " [ 95 451]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.66      0.77      1430\n",
      "           1       0.48      0.83      0.61       546\n",
      "\n",
      "    accuracy                           0.71      1976\n",
      "   macro avg       0.70      0.74      0.69      1976\n",
      "weighted avg       0.79      0.71      0.72      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_soft_voting_train_predict = grid_soft_voting_classifier.predict(X_train_rus)\n",
    "y_soft_voting_predict = grid_soft_voting_classifier.predict(X_test)\n",
    "soft_voting_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_soft_voting_predict)\n",
    "soft_voting_classification_report = classification_report(y_true = y_test, y_pred = y_soft_voting_predict)\n",
    "print(soft_voting_cmatrix)\n",
    "print()\n",
    "print(soft_voting_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Soft Voting (SVM Poly Kernel, Decision Tree)\", \"C = 10, max_depth = 3\", f1_score(y_true=y_train_rus, y_pred=grid_soft_voting_classifier.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=grid_soft_voting_classifier.predict(X_test)),\n",
    "                  df_grid_soft_voting_classifier_results[df_grid_soft_voting_classifier_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Soft Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 10, max_depth = 3</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Classification Model  \\\n",
       "0                                           KNN   \n",
       "1                           Logistic Regression   \n",
       "2                            SVM (kernel='rbf')   \n",
       "3               SVM (kernel='poly', degree = 3)   \n",
       "4                         SVM (kernel='linear')   \n",
       "5                                    Linear SVM   \n",
       "6                                 Decision Tree   \n",
       "7                                 Random Forest   \n",
       "8  Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "9  Soft Voting (SVM Poly Kernel, Decision Tree)   \n",
       "\n",
       "                                     Model Parameter  Train F1-Score  \\\n",
       "0             n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                           C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                              C = 1, gamma = 'auto'        0.819056   \n",
       "3                             C = 1, gamma = 'scale'        0.835155   \n",
       "4                                            C = 100        0.770344   \n",
       "5                          C = 0.001, penalty = 'l2'        0.783514   \n",
       "6  max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7  max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                               C = 1, max_depth = 1        0.803411   \n",
       "9                              C = 10, max_depth = 3        0.843287   \n",
       "\n",
       "   Test F1-Score  Best Mean Test F1-Score  \n",
       "0       0.584980                 0.788615  \n",
       "1       0.609739                 0.779720  \n",
       "2       0.606868                 0.776477  \n",
       "3       0.602759                 0.770910  \n",
       "4       0.613128                 0.767012  \n",
       "5       0.602663                 0.777433  \n",
       "6       0.596774                 0.770291  \n",
       "7       0.620833                 0.781263  \n",
       "8       0.616519                 0.762953  \n",
       "9       0.608637                 0.771049  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "#### Bagging Model with a Decision Tree Classification Model that has a max_depth of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7584097859327217"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_1 = DecisionTreeClassifier(max_depth = 1, random_state = 0)\n",
    "bagging_dtree_1 = BaggingClassifier(base_estimator=dtree_1, random_state=0, bootstrap=True)\n",
    "grid_bagging_params_grid = {\"max_features\": [2,5,10]}\n",
    "grid_bagging_dtree_1 = GridSearchCV(bagging_dtree_1, param_grid=grid_bagging_params_grid, cv=5, scoring='f1')\n",
    "grid_bagging_dtree_1.fit(X_train_rus, y_train_rus)\n",
    "grid_bagging_dtree_1.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6002621231979031"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_bagging_dtree_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 10}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_bagging_dtree_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7614530250970516"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_bagging_dtree_1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017609</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_features': 2}</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.647520</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.719212</td>\n",
       "      <td>0.695076</td>\n",
       "      <td>0.027515</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_features': 5}</td>\n",
       "      <td>0.722467</td>\n",
       "      <td>0.736142</td>\n",
       "      <td>0.693780</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.734141</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015297</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_features': 10}</td>\n",
       "      <td>0.728507</td>\n",
       "      <td>0.743982</td>\n",
       "      <td>0.774336</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.761453</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.017609      0.004530         0.001005        0.000892   \n",
       "1       0.014375      0.003792         0.000000        0.000000   \n",
       "2       0.015297      0.007016         0.003915        0.005904   \n",
       "\n",
       "  param_max_features                params  split0_test_score  \\\n",
       "0                  2   {'max_features': 2}           0.719048   \n",
       "1                  5   {'max_features': 5}           0.722467   \n",
       "2                 10  {'max_features': 10}           0.728507   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.681265           0.647520           0.708333           0.719212   \n",
       "1           0.736142           0.693780           0.756410           0.761905   \n",
       "2           0.743982           0.774336           0.791209           0.769231   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.695076        0.027515                3  \n",
       "1         0.734141        0.024642                2  \n",
       "2         0.761453        0.022372                1  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_bagging_dtree_1_results = pd.DataFrame(grid_bagging_dtree_1.cv_results_)\n",
    "df_grid_bagging_dtree_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[908 522]\n",
      " [ 88 458]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.63      0.75      1430\n",
      "           1       0.47      0.84      0.60       546\n",
      "\n",
      "    accuracy                           0.69      1976\n",
      "   macro avg       0.69      0.74      0.67      1976\n",
      "weighted avg       0.79      0.69      0.71      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_bagging_dtree_train_predict = grid_bagging_dtree_1.predict(X_train_rus)\n",
    "y_bagging_dtree_predict = grid_bagging_dtree_1.predict(X_test)\n",
    "bagging_dtree_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_bagging_dtree_predict)\n",
    "bagging_dtree_classification_report = classification_report(y_true = y_test, y_pred = y_bagging_dtree_predict)\n",
    "print(bagging_dtree_cmatrix)\n",
    "print()\n",
    "print(bagging_dtree_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Bagging (Decision Tree (max_depth = 1))\", \"max_features = 10\", f1_score(y_true=y_train_rus, y_pred=grid_bagging_dtree_1.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=grid_bagging_dtree_1.predict(X_test)),\n",
    "                  df_grid_bagging_dtree_1_results[df_grid_bagging_dtree_1_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Soft Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 10, max_depth = 3</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bagging (Decision Tree (max_depth = 1))</td>\n",
       "      <td>max_features = 10</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.600262</td>\n",
       "      <td>0.761453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Classification Model  \\\n",
       "0                                            KNN   \n",
       "1                            Logistic Regression   \n",
       "2                             SVM (kernel='rbf')   \n",
       "3                SVM (kernel='poly', degree = 3)   \n",
       "4                          SVM (kernel='linear')   \n",
       "5                                     Linear SVM   \n",
       "6                                  Decision Tree   \n",
       "7                                  Random Forest   \n",
       "8   Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "9   Soft Voting (SVM Poly Kernel, Decision Tree)   \n",
       "10       Bagging (Decision Tree (max_depth = 1))   \n",
       "\n",
       "                                      Model Parameter  Train F1-Score  \\\n",
       "0              n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                            C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                               C = 1, gamma = 'auto'        0.819056   \n",
       "3                              C = 1, gamma = 'scale'        0.835155   \n",
       "4                                             C = 100        0.770344   \n",
       "5                           C = 0.001, penalty = 'l2'        0.783514   \n",
       "6   max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7   max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                                C = 1, max_depth = 1        0.803411   \n",
       "9                               C = 10, max_depth = 3        0.843287   \n",
       "10                                  max_features = 10        0.758410   \n",
       "\n",
       "    Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.584980                 0.788615  \n",
       "1        0.609739                 0.779720  \n",
       "2        0.606868                 0.776477  \n",
       "3        0.602759                 0.770910  \n",
       "4        0.613128                 0.767012  \n",
       "5        0.602663                 0.777433  \n",
       "6        0.596774                 0.770291  \n",
       "7        0.620833                 0.781263  \n",
       "8        0.616519                 0.762953  \n",
       "9        0.608637                 0.771049  \n",
       "10       0.600262                 0.761453  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasting\n",
    "#### Pasting Model with a Decision Tree Classification Model that has a max_depth of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736185383244206"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_3 = DecisionTreeClassifier(max_depth = 3, random_state = 0)\n",
    "pasting_dtree_3 = BaggingClassifier(base_estimator=dtree_3, random_state=0, bootstrap=False)\n",
    "grid_pasting_params_grid = {\"max_features\": [5,10,20], 'max_samples': [0.1,0.5,1]}\n",
    "grid_pasting_dtree_3 = GridSearchCV(pasting_dtree_3, param_grid=grid_pasting_params_grid, cv=5, scoring='f1', \n",
    "                                     return_train_score=True)\n",
    "grid_pasting_dtree_3.fit(X_train_rus, y_train_rus)\n",
    "grid_pasting_dtree_3.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616326530612245"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_pasting_dtree_3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 20, 'max_samples': 0.5}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_pasting_dtree_3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7677159497581357"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_pasting_dtree_3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_max_samples</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015348</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_features': 5, 'max_samples': 0.1}</td>\n",
       "      <td>0.752252</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>0.731377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749603</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.760405</td>\n",
       "      <td>0.756274</td>\n",
       "      <td>0.753525</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.745940</td>\n",
       "      <td>0.754106</td>\n",
       "      <td>0.004722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013277</td>\n",
       "      <td>0.012066</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'max_features': 5, 'max_samples': 0.5}</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.740047</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742960</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>6</td>\n",
       "      <td>0.758148</td>\n",
       "      <td>0.753368</td>\n",
       "      <td>0.750292</td>\n",
       "      <td>0.747240</td>\n",
       "      <td>0.744131</td>\n",
       "      <td>0.750636</td>\n",
       "      <td>0.004855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_features': 5, 'max_samples': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.007887</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_features': 10, 'max_samples': 0.1}</td>\n",
       "      <td>0.741784</td>\n",
       "      <td>0.764977</td>\n",
       "      <td>0.744828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754466</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>4</td>\n",
       "      <td>0.772648</td>\n",
       "      <td>0.781465</td>\n",
       "      <td>0.762623</td>\n",
       "      <td>0.772367</td>\n",
       "      <td>0.757683</td>\n",
       "      <td>0.769357</td>\n",
       "      <td>0.008344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'max_features': 10, 'max_samples': 0.5}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.740566</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760653</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>3</td>\n",
       "      <td>0.776136</td>\n",
       "      <td>0.774419</td>\n",
       "      <td>0.762018</td>\n",
       "      <td>0.775842</td>\n",
       "      <td>0.768074</td>\n",
       "      <td>0.771298</td>\n",
       "      <td>0.005483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_features': 10, 'max_samples': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.006049</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'max_features': 20, 'max_samples': 0.1}</td>\n",
       "      <td>0.756522</td>\n",
       "      <td>0.765101</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764247</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>2</td>\n",
       "      <td>0.785249</td>\n",
       "      <td>0.783099</td>\n",
       "      <td>0.756537</td>\n",
       "      <td>0.769752</td>\n",
       "      <td>0.755607</td>\n",
       "      <td>0.770049</td>\n",
       "      <td>0.012590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017518</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'max_features': 20, 'max_samples': 0.5}</td>\n",
       "      <td>0.777273</td>\n",
       "      <td>0.743764</td>\n",
       "      <td>0.769575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767716</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785633</td>\n",
       "      <td>0.775185</td>\n",
       "      <td>0.778091</td>\n",
       "      <td>0.776445</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.777758</td>\n",
       "      <td>0.004222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.015845</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_features': 20, 'max_samples': 1}</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.015348      0.001023         0.001752        0.002623   \n",
       "1       0.013277      0.012066         0.003531        0.006095   \n",
       "2       0.015622      0.000024         0.000000        0.000000   \n",
       "3       0.008160      0.009481         0.007065        0.007887   \n",
       "4       0.013568      0.004782         0.000399        0.000798   \n",
       "5       0.016109      0.010667         0.003724        0.006063   \n",
       "6       0.015578      0.002207         0.003710        0.006049   \n",
       "7       0.017518      0.003792         0.003722        0.006061   \n",
       "8       0.015845      0.002750         0.001417        0.001365   \n",
       "\n",
       "  param_max_features param_max_samples  \\\n",
       "0                  5               0.1   \n",
       "1                  5               0.5   \n",
       "2                  5                 1   \n",
       "3                 10               0.1   \n",
       "4                 10               0.5   \n",
       "5                 10                 1   \n",
       "6                 20               0.1   \n",
       "7                 20               0.5   \n",
       "8                 20                 1   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "0   {'max_features': 5, 'max_samples': 0.1}           0.752252   \n",
       "1   {'max_features': 5, 'max_samples': 0.5}           0.755149   \n",
       "2     {'max_features': 5, 'max_samples': 1}           0.000000   \n",
       "3  {'max_features': 10, 'max_samples': 0.1}           0.741784   \n",
       "4  {'max_features': 10, 'max_samples': 0.5}           0.750000   \n",
       "5    {'max_features': 10, 'max_samples': 1}           0.000000   \n",
       "6  {'max_features': 20, 'max_samples': 0.1}           0.756522   \n",
       "7  {'max_features': 20, 'max_samples': 0.5}           0.777273   \n",
       "8    {'max_features': 20, 'max_samples': 1}           0.000000   \n",
       "\n",
       "   split1_test_score  split2_test_score  ...  mean_test_score  std_test_score  \\\n",
       "0           0.744395           0.731377  ...         0.749603        0.011107   \n",
       "1           0.740047           0.717949  ...         0.742960        0.013494   \n",
       "2           0.000000           0.000000  ...         0.000000        0.000000   \n",
       "3           0.764977           0.744828  ...         0.754466        0.014540   \n",
       "4           0.740566           0.757009  ...         0.760653        0.015653   \n",
       "5           0.000000           0.000000  ...         0.000000        0.000000   \n",
       "6           0.765101           0.759725  ...         0.764247        0.006072   \n",
       "7           0.743764           0.769575  ...         0.767716        0.016119   \n",
       "8           0.000000           0.000000  ...         0.000000        0.000000   \n",
       "\n",
       "   rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                5            0.760405            0.756274   \n",
       "1                6            0.758148            0.753368   \n",
       "2                7            0.000000            0.000000   \n",
       "3                4            0.772648            0.781465   \n",
       "4                3            0.776136            0.774419   \n",
       "5                7            0.000000            0.000000   \n",
       "6                2            0.785249            0.783099   \n",
       "7                1            0.785633            0.775185   \n",
       "8                7            0.000000            0.000000   \n",
       "\n",
       "   split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0            0.753525            0.754386            0.745940   \n",
       "1            0.750292            0.747240            0.744131   \n",
       "2            0.000000            0.000000            0.000000   \n",
       "3            0.762623            0.772367            0.757683   \n",
       "4            0.762018            0.775842            0.768074   \n",
       "5            0.000000            0.000000            0.000000   \n",
       "6            0.756537            0.769752            0.755607   \n",
       "7            0.778091            0.776445            0.773438   \n",
       "8            0.000000            0.000000            0.000000   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "0          0.754106         0.004722  \n",
       "1          0.750636         0.004855  \n",
       "2          0.000000         0.000000  \n",
       "3          0.769357         0.008344  \n",
       "4          0.771298         0.005483  \n",
       "5          0.000000         0.000000  \n",
       "6          0.770049         0.012590  \n",
       "7          0.777758         0.004222  \n",
       "8          0.000000         0.000000  \n",
       "\n",
       "[9 rows x 22 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_pasting_dtree_3_results = pd.DataFrame(grid_pasting_dtree_3.cv_results_)\n",
    "df_grid_pasting_dtree_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[959 471]\n",
      " [ 93 453]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.67      0.77      1430\n",
      "           1       0.49      0.83      0.62       546\n",
      "\n",
      "    accuracy                           0.71      1976\n",
      "   macro avg       0.70      0.75      0.69      1976\n",
      "weighted avg       0.80      0.71      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pasting_dtree_train_predict = grid_pasting_dtree_3.predict(X_train_rus)\n",
    "y_pasting_dtree_predict = grid_pasting_dtree_3.predict(X_test)\n",
    "pasting_dtree_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_pasting_dtree_predict)\n",
    "pasting_dtree_classification_report = classification_report(y_true = y_test, y_pred = y_pasting_dtree_predict)\n",
    "print(pasting_dtree_cmatrix)\n",
    "print()\n",
    "print(pasting_dtree_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Pasting (Decision Tree (max_depth = 3))\", \"max_features = 20, max_samples = 0.5\", f1_score(y_true=y_train_rus, y_pred=grid_pasting_dtree_3.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=grid_pasting_dtree_3.predict(X_test)),\n",
    "                  df_grid_pasting_dtree_3_results[df_grid_pasting_dtree_3_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Soft Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 10, max_depth = 3</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bagging (Decision Tree (max_depth = 1))</td>\n",
       "      <td>max_features = 10</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.600262</td>\n",
       "      <td>0.761453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pasting (Decision Tree (max_depth = 3))</td>\n",
       "      <td>max_features = 20, max_samples = 0.5</td>\n",
       "      <td>0.773619</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>0.767716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Classification Model  \\\n",
       "0                                            KNN   \n",
       "1                            Logistic Regression   \n",
       "2                             SVM (kernel='rbf')   \n",
       "3                SVM (kernel='poly', degree = 3)   \n",
       "4                          SVM (kernel='linear')   \n",
       "5                                     Linear SVM   \n",
       "6                                  Decision Tree   \n",
       "7                                  Random Forest   \n",
       "8   Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "9   Soft Voting (SVM Poly Kernel, Decision Tree)   \n",
       "10       Bagging (Decision Tree (max_depth = 1))   \n",
       "11       Pasting (Decision Tree (max_depth = 3))   \n",
       "\n",
       "                                      Model Parameter  Train F1-Score  \\\n",
       "0              n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                            C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                               C = 1, gamma = 'auto'        0.819056   \n",
       "3                              C = 1, gamma = 'scale'        0.835155   \n",
       "4                                             C = 100        0.770344   \n",
       "5                           C = 0.001, penalty = 'l2'        0.783514   \n",
       "6   max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7   max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                                C = 1, max_depth = 1        0.803411   \n",
       "9                               C = 10, max_depth = 3        0.843287   \n",
       "10                                  max_features = 10        0.758410   \n",
       "11               max_features = 20, max_samples = 0.5        0.773619   \n",
       "\n",
       "    Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.584980                 0.788615  \n",
       "1        0.609739                 0.779720  \n",
       "2        0.606868                 0.776477  \n",
       "3        0.602759                 0.770910  \n",
       "4        0.613128                 0.767012  \n",
       "5        0.602663                 0.777433  \n",
       "6        0.596774                 0.770291  \n",
       "7        0.620833                 0.781263  \n",
       "8        0.616519                 0.762953  \n",
       "9        0.608637                 0.771049  \n",
       "10       0.600262                 0.761453  \n",
       "11       0.616327                 0.767716  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "#### AdaBoost Model with a Decision Tree Classification Model that has a max_depth of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8003688335638544"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_2 = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "ada_dtree_2 = AdaBoostClassifier(dtree_2, algorithm=\"SAMME.R\", random_state=0)\n",
    "grid_ada_params_grid = {'n_estimators': [50, 100, 200, 250], 'learning_rate': [0.2, 0.5, 1]}\n",
    "grid_ada_dtree_2 = GridSearchCV(ada_dtree_2, param_grid=grid_ada_params_grid, cv=5, scoring='f1')\n",
    "grid_ada_dtree_2.fit(X_train_rus, y_train_rus)\n",
    "grid_ada_dtree_2.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6234701223902088"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ada_dtree_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'n_estimators': 50}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ada_dtree_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.769226954505063"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_ada_dtree_2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.143083</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.012491</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.2</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 50}</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>0.752336</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.779510</td>\n",
       "      <td>0.787440</td>\n",
       "      <td>0.769227</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322903</td>\n",
       "      <td>0.029249</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.007647</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 100}</td>\n",
       "      <td>0.758140</td>\n",
       "      <td>0.736597</td>\n",
       "      <td>0.752834</td>\n",
       "      <td>0.770975</td>\n",
       "      <td>0.792363</td>\n",
       "      <td>0.762182</td>\n",
       "      <td>0.018690</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.616902</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.045038</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 200}</td>\n",
       "      <td>0.759434</td>\n",
       "      <td>0.734118</td>\n",
       "      <td>0.762557</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756570</td>\n",
       "      <td>0.012218</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.742882</td>\n",
       "      <td>0.013199</td>\n",
       "      <td>0.043029</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.2</td>\n",
       "      <td>250</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 250}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.738609</td>\n",
       "      <td>0.745370</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.743142</td>\n",
       "      <td>0.749907</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147323</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.006899</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 50}</td>\n",
       "      <td>0.759907</td>\n",
       "      <td>0.737819</td>\n",
       "      <td>0.778281</td>\n",
       "      <td>0.775785</td>\n",
       "      <td>0.768868</td>\n",
       "      <td>0.764132</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.294783</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 100}</td>\n",
       "      <td>0.768496</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.740909</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.743842</td>\n",
       "      <td>0.750643</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.566873</td>\n",
       "      <td>0.012791</td>\n",
       "      <td>0.037476</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 200}</td>\n",
       "      <td>0.758454</td>\n",
       "      <td>0.734118</td>\n",
       "      <td>0.745283</td>\n",
       "      <td>0.735084</td>\n",
       "      <td>0.715365</td>\n",
       "      <td>0.737661</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.811560</td>\n",
       "      <td>0.062446</td>\n",
       "      <td>0.047120</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>0.5</td>\n",
       "      <td>250</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 250}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.713217</td>\n",
       "      <td>0.731603</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.185216</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 50}</td>\n",
       "      <td>0.733645</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.749425</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.745679</td>\n",
       "      <td>0.737804</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.322728</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 100}</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.706161</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.725441</td>\n",
       "      <td>0.722089</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.607244</td>\n",
       "      <td>0.018114</td>\n",
       "      <td>0.033152</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 200}</td>\n",
       "      <td>0.751807</td>\n",
       "      <td>0.674938</td>\n",
       "      <td>0.729858</td>\n",
       "      <td>0.726829</td>\n",
       "      <td>0.692112</td>\n",
       "      <td>0.715109</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.825235</td>\n",
       "      <td>0.117225</td>\n",
       "      <td>0.044692</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>{'learning_rate': 1, 'n_estimators': 250}</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.671679</td>\n",
       "      <td>0.712264</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.685139</td>\n",
       "      <td>0.700113</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.143083      0.003370         0.012491        0.006245   \n",
       "1        0.322903      0.029249         0.021875        0.007647   \n",
       "2        0.616902      0.041512         0.045038        0.007108   \n",
       "3        0.742882      0.013199         0.043029        0.011083   \n",
       "4        0.147323      0.008844         0.006899        0.006077   \n",
       "5        0.294783      0.011348         0.018584        0.006245   \n",
       "6        0.566873      0.012791         0.037476        0.007665   \n",
       "7        0.811560      0.062446         0.047120        0.008702   \n",
       "8        0.185216      0.009360         0.011972        0.001093   \n",
       "9        0.322728      0.018507         0.018282        0.001935   \n",
       "10       0.607244      0.018114         0.033152        0.002356   \n",
       "11       0.825235      0.117225         0.044692        0.001582   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "0                  0.2                 50   \n",
       "1                  0.2                100   \n",
       "2                  0.2                200   \n",
       "3                  0.2                250   \n",
       "4                  0.5                 50   \n",
       "5                  0.5                100   \n",
       "6                  0.5                200   \n",
       "7                  0.5                250   \n",
       "8                    1                 50   \n",
       "9                    1                100   \n",
       "10                   1                200   \n",
       "11                   1                250   \n",
       "\n",
       "                                         params  split0_test_score  \\\n",
       "0    {'learning_rate': 0.2, 'n_estimators': 50}           0.759725   \n",
       "1   {'learning_rate': 0.2, 'n_estimators': 100}           0.758140   \n",
       "2   {'learning_rate': 0.2, 'n_estimators': 200}           0.759434   \n",
       "3   {'learning_rate': 0.2, 'n_estimators': 250}           0.750000   \n",
       "4    {'learning_rate': 0.5, 'n_estimators': 50}           0.759907   \n",
       "5   {'learning_rate': 0.5, 'n_estimators': 100}           0.768496   \n",
       "6   {'learning_rate': 0.5, 'n_estimators': 200}           0.758454   \n",
       "7   {'learning_rate': 0.5, 'n_estimators': 250}           0.750000   \n",
       "8      {'learning_rate': 1, 'n_estimators': 50}           0.733645   \n",
       "9     {'learning_rate': 1, 'n_estimators': 100}           0.723404   \n",
       "10    {'learning_rate': 1, 'n_estimators': 200}           0.751807   \n",
       "11    {'learning_rate': 1, 'n_estimators': 250}           0.709677   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.752336           0.767123           0.779510   \n",
       "1            0.736597           0.752834           0.770975   \n",
       "2            0.734118           0.762557           0.770642   \n",
       "3            0.738609           0.745370           0.772414   \n",
       "4            0.737819           0.778281           0.775785   \n",
       "5            0.746544           0.740909           0.753425   \n",
       "6            0.734118           0.745283           0.735084   \n",
       "7            0.722892           0.738095           0.733813   \n",
       "8            0.724638           0.749425           0.735632   \n",
       "9            0.706161           0.726027           0.729412   \n",
       "10           0.674938           0.729858           0.726829   \n",
       "11           0.671679           0.712264           0.721805   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.787440         0.769227        0.012786                1  \n",
       "1            0.792363         0.762182        0.018690                3  \n",
       "2            0.756098         0.756570        0.012218                4  \n",
       "3            0.743142         0.749907        0.011837                6  \n",
       "4            0.768868         0.764132        0.014619                2  \n",
       "5            0.743842         0.750643        0.009842                5  \n",
       "6            0.715365         0.737661        0.014190                8  \n",
       "7            0.713217         0.731603        0.012645                9  \n",
       "8            0.745679         0.737804        0.008859                7  \n",
       "9            0.725441         0.722089        0.008195               10  \n",
       "10           0.692112         0.715109        0.027728               11  \n",
       "11           0.685139         0.700113        0.018667               12  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_ada_dtree_2_results = pd.DataFrame(grid_ada_dtree_2.cv_results_)\n",
    "df_grid_ada_dtree_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1020  410]\n",
      " [ 113  433]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.71      0.80      1430\n",
      "           1       0.51      0.79      0.62       546\n",
      "\n",
      "    accuracy                           0.74      1976\n",
      "   macro avg       0.71      0.75      0.71      1976\n",
      "weighted avg       0.79      0.74      0.75      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_ada_dtree_train_predict = grid_ada_dtree_2.predict(X_train_rus)\n",
    "y_ada_dtree_predict = grid_ada_dtree_2.predict(X_test)\n",
    "ada_dtree_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_ada_dtree_predict)\n",
    "ada_dtree_classification_report = classification_report(y_true = y_test, y_pred = y_ada_dtree_predict)\n",
    "print(ada_dtree_cmatrix)\n",
    "print()\n",
    "print(ada_dtree_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"AdaBoost (Decision Tree (max_depth = 2))\", \"n_estimators = 50, learning_rate = 0.2\", f1_score(y_true=y_train_rus, y_pred=grid_ada_dtree_2.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=grid_ada_dtree_2.predict(X_test)),\n",
    "                  df_grid_ada_dtree_2_results[df_grid_ada_dtree_2_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Soft Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 10, max_depth = 3</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bagging (Decision Tree (max_depth = 1))</td>\n",
       "      <td>max_features = 10</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.600262</td>\n",
       "      <td>0.761453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pasting (Decision Tree (max_depth = 3))</td>\n",
       "      <td>max_features = 20, max_samples = 0.5</td>\n",
       "      <td>0.773619</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>0.767716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AdaBoost (Decision Tree (max_depth = 2))</td>\n",
       "      <td>n_estimators = 50, learning_rate = 0.2</td>\n",
       "      <td>0.800369</td>\n",
       "      <td>0.623470</td>\n",
       "      <td>0.769227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Classification Model  \\\n",
       "0                                            KNN   \n",
       "1                            Logistic Regression   \n",
       "2                             SVM (kernel='rbf')   \n",
       "3                SVM (kernel='poly', degree = 3)   \n",
       "4                          SVM (kernel='linear')   \n",
       "5                                     Linear SVM   \n",
       "6                                  Decision Tree   \n",
       "7                                  Random Forest   \n",
       "8   Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "9   Soft Voting (SVM Poly Kernel, Decision Tree)   \n",
       "10       Bagging (Decision Tree (max_depth = 1))   \n",
       "11       Pasting (Decision Tree (max_depth = 3))   \n",
       "12      AdaBoost (Decision Tree (max_depth = 2))   \n",
       "\n",
       "                                      Model Parameter  Train F1-Score  \\\n",
       "0              n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                            C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                               C = 1, gamma = 'auto'        0.819056   \n",
       "3                              C = 1, gamma = 'scale'        0.835155   \n",
       "4                                             C = 100        0.770344   \n",
       "5                           C = 0.001, penalty = 'l2'        0.783514   \n",
       "6   max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7   max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                                C = 1, max_depth = 1        0.803411   \n",
       "9                               C = 10, max_depth = 3        0.843287   \n",
       "10                                  max_features = 10        0.758410   \n",
       "11               max_features = 20, max_samples = 0.5        0.773619   \n",
       "12             n_estimators = 50, learning_rate = 0.2        0.800369   \n",
       "\n",
       "    Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.584980                 0.788615  \n",
       "1        0.609739                 0.779720  \n",
       "2        0.606868                 0.776477  \n",
       "3        0.602759                 0.770910  \n",
       "4        0.613128                 0.767012  \n",
       "5        0.602663                 0.777433  \n",
       "6        0.596774                 0.770291  \n",
       "7        0.620833                 0.781263  \n",
       "8        0.616519                 0.762953  \n",
       "9        0.608637                 0.771049  \n",
       "10       0.600262                 0.761453  \n",
       "11       0.616327                 0.767716  \n",
       "12       0.623470                 0.769227  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\peech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7745770461819845"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=40, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "number_of_features = X_train_rus.shape[1]\n",
    "seed = 10\n",
    "np.random.seed(10)\n",
    "keras_model = KerasClassifier(build_fn = create_model, verbose = 0)\n",
    "grid_keras_model_param_grid = {'batch_size': [10,20,30,40] , 'epochs': [10, 50, 100]}\n",
    "grid_keras_model = GridSearchCV(estimator= keras_model, param_grid = grid_keras_model_param_grid, cv = 5, scoring='f1')\n",
    "grid_keras_model.fit(X_train_rus, y_train_rus)\n",
    "grid_keras_model.score(X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6025280898876405"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_keras_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 40, 'epochs': 10}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_keras_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4841355546691336"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_keras_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_epochs</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.234637</td>\n",
       "      <td>0.049407</td>\n",
       "      <td>0.072609</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 10, 'epochs': 10}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764423</td>\n",
       "      <td>0.765579</td>\n",
       "      <td>0.821530</td>\n",
       "      <td>0.470306</td>\n",
       "      <td>0.384558</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.645117</td>\n",
       "      <td>0.592779</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'batch_size': 10, 'epochs': 50}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796163</td>\n",
       "      <td>0.794203</td>\n",
       "      <td>0.802878</td>\n",
       "      <td>0.478649</td>\n",
       "      <td>0.390826</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.333903</td>\n",
       "      <td>1.092691</td>\n",
       "      <td>0.106512</td>\n",
       "      <td>0.071701</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'batch_size': 10, 'epochs': 100}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740196</td>\n",
       "      <td>0.771049</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.465885</td>\n",
       "      <td>0.381204</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872008</td>\n",
       "      <td>0.047242</td>\n",
       "      <td>0.059652</td>\n",
       "      <td>0.004897</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 20, 'epochs': 10}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764846</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.478218</td>\n",
       "      <td>0.390862</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.569433</td>\n",
       "      <td>0.052913</td>\n",
       "      <td>0.064091</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>{'batch_size': 20, 'epochs': 50}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.755448</td>\n",
       "      <td>0.780059</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.468021</td>\n",
       "      <td>0.382453</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.128170</td>\n",
       "      <td>0.615267</td>\n",
       "      <td>0.068614</td>\n",
       "      <td>0.017456</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'batch_size': 20, 'epochs': 100}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.783626</td>\n",
       "      <td>0.799423</td>\n",
       "      <td>0.464134</td>\n",
       "      <td>0.379508</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.703572</td>\n",
       "      <td>0.054625</td>\n",
       "      <td>0.060965</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 30, 'epochs': 10}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.813124</td>\n",
       "      <td>0.823197</td>\n",
       "      <td>0.480741</td>\n",
       "      <td>0.392974</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.005792</td>\n",
       "      <td>0.189801</td>\n",
       "      <td>0.084794</td>\n",
       "      <td>0.051717</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>{'batch_size': 30, 'epochs': 50}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.787172</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.467729</td>\n",
       "      <td>0.381922</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.366284</td>\n",
       "      <td>0.141423</td>\n",
       "      <td>0.062028</td>\n",
       "      <td>0.009922</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>{'batch_size': 30, 'epochs': 100}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.764151</td>\n",
       "      <td>0.806313</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.472232</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.671682</td>\n",
       "      <td>0.072998</td>\n",
       "      <td>0.058078</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>{'batch_size': 40, 'epochs': 10}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.839609</td>\n",
       "      <td>0.484136</td>\n",
       "      <td>0.395801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.545839</td>\n",
       "      <td>0.092331</td>\n",
       "      <td>0.057119</td>\n",
       "      <td>0.011798</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>{'batch_size': 40, 'epochs': 50}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.781775</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.480864</td>\n",
       "      <td>0.392866</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.615398</td>\n",
       "      <td>0.057310</td>\n",
       "      <td>0.095104</td>\n",
       "      <td>0.061823</td>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>{'batch_size': 40, 'epochs': 100}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797136</td>\n",
       "      <td>0.781845</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.476716</td>\n",
       "      <td>0.389306</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        1.234637      0.049407         0.072609        0.014335   \n",
       "1        4.645117      0.592779         0.073565        0.009568   \n",
       "2        9.333903      1.092691         0.106512        0.071701   \n",
       "3        0.872008      0.047242         0.059652        0.004897   \n",
       "4        2.569433      0.052913         0.064091        0.008742   \n",
       "5        5.128170      0.615267         0.068614        0.017456   \n",
       "6        0.703572      0.054625         0.060965        0.007666   \n",
       "7        2.005792      0.189801         0.084794        0.051717   \n",
       "8        3.366284      0.141423         0.062028        0.009922   \n",
       "9        0.671682      0.072998         0.058078        0.005397   \n",
       "10       1.545839      0.092331         0.057119        0.011798   \n",
       "11       2.615398      0.057310         0.095104        0.061823   \n",
       "\n",
       "   param_batch_size param_epochs                             params  \\\n",
       "0                10           10   {'batch_size': 10, 'epochs': 10}   \n",
       "1                10           50   {'batch_size': 10, 'epochs': 50}   \n",
       "2                10          100  {'batch_size': 10, 'epochs': 100}   \n",
       "3                20           10   {'batch_size': 20, 'epochs': 10}   \n",
       "4                20           50   {'batch_size': 20, 'epochs': 50}   \n",
       "5                20          100  {'batch_size': 20, 'epochs': 100}   \n",
       "6                30           10   {'batch_size': 30, 'epochs': 10}   \n",
       "7                30           50   {'batch_size': 30, 'epochs': 50}   \n",
       "8                30          100  {'batch_size': 30, 'epochs': 100}   \n",
       "9                40           10   {'batch_size': 40, 'epochs': 10}   \n",
       "10               40           50   {'batch_size': 40, 'epochs': 50}   \n",
       "11               40          100  {'batch_size': 40, 'epochs': 100}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0                 0.0                0.0           0.764423   \n",
       "1                 0.0                0.0           0.796163   \n",
       "2                 0.0                0.0           0.740196   \n",
       "3                 0.0                0.0           0.764846   \n",
       "4                 0.0                0.0           0.755448   \n",
       "5                 0.0                0.0           0.737624   \n",
       "6                 0.0                0.0           0.767386   \n",
       "7                 0.0                0.0           0.775000   \n",
       "8                 0.0                0.0           0.764151   \n",
       "9                 0.0                0.0           0.776471   \n",
       "10                0.0                0.0           0.781775   \n",
       "11                0.0                0.0           0.797136   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.765579           0.821530         0.470306        0.384558   \n",
       "1            0.794203           0.802878         0.478649        0.390826   \n",
       "2            0.771049           0.818182         0.465885        0.381204   \n",
       "3            0.811429           0.814815         0.478218        0.390862   \n",
       "4            0.780059           0.804598         0.468021        0.382453   \n",
       "5            0.783626           0.799423         0.464134        0.379508   \n",
       "6            0.813124           0.823197         0.480741        0.392974   \n",
       "7            0.787172           0.776471         0.467729        0.381922   \n",
       "8            0.806313           0.790698         0.472232        0.385812   \n",
       "9            0.804598           0.839609         0.484136        0.395801   \n",
       "10           0.824859           0.797688         0.480864        0.392866   \n",
       "11           0.781845           0.804598         0.476716        0.389306   \n",
       "\n",
       "    rank_test_score  \n",
       "0                 8  \n",
       "1                 4  \n",
       "2                11  \n",
       "3                 5  \n",
       "4                 9  \n",
       "5                12  \n",
       "6                 3  \n",
       "7                10  \n",
       "8                 7  \n",
       "9                 1  \n",
       "10                2  \n",
       "11                6  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grid_keras_model_results = pd.DataFrame(grid_keras_model.cv_results_)\n",
    "df_grid_keras_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[981 449]\n",
      " [117 429]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.69      0.78      1430\n",
      "           1       0.49      0.79      0.60       546\n",
      "\n",
      "    accuracy                           0.71      1976\n",
      "   macro avg       0.69      0.74      0.69      1976\n",
      "weighted avg       0.78      0.71      0.73      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_keras_train_predict = grid_keras_model.predict(X_train_rus)\n",
    "y_keras_predict = grid_keras_model.predict(X_test)\n",
    "keras_cmatrix = confusion_matrix(y_true=y_test, y_pred=y_keras_predict)\n",
    "keras_classification_report = classification_report(y_true = y_test, y_pred = y_keras_predict)\n",
    "print(keras_cmatrix)\n",
    "print()\n",
    "print(keras_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_table += [[\"Keras\", \"batch_size = 20, epochs = 10\", f1_score(y_true=y_train_rus, y_pred=grid_keras_model.predict(X_train_rus)), f1_score(y_true=y_test, y_pred=grid_keras_model.predict(X_test)),\n",
    "                  df_grid_keras_model_results[df_grid_keras_model_results['rank_test_score'] == 1][['mean_test_score']].iloc[0].values[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Soft Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 10, max_depth = 3</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bagging (Decision Tree (max_depth = 1))</td>\n",
       "      <td>max_features = 10</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.600262</td>\n",
       "      <td>0.761453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pasting (Decision Tree (max_depth = 3))</td>\n",
       "      <td>max_features = 20, max_samples = 0.5</td>\n",
       "      <td>0.773619</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>0.767716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AdaBoost (Decision Tree (max_depth = 2))</td>\n",
       "      <td>n_estimators = 50, learning_rate = 0.2</td>\n",
       "      <td>0.800369</td>\n",
       "      <td>0.623470</td>\n",
       "      <td>0.769227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Keras</td>\n",
       "      <td>batch_size = 20, epochs = 10</td>\n",
       "      <td>0.774577</td>\n",
       "      <td>0.602528</td>\n",
       "      <td>0.484136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Classification Model  \\\n",
       "0                                            KNN   \n",
       "1                            Logistic Regression   \n",
       "2                             SVM (kernel='rbf')   \n",
       "3                SVM (kernel='poly', degree = 3)   \n",
       "4                          SVM (kernel='linear')   \n",
       "5                                     Linear SVM   \n",
       "6                                  Decision Tree   \n",
       "7                                  Random Forest   \n",
       "8   Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "9   Soft Voting (SVM Poly Kernel, Decision Tree)   \n",
       "10       Bagging (Decision Tree (max_depth = 1))   \n",
       "11       Pasting (Decision Tree (max_depth = 3))   \n",
       "12      AdaBoost (Decision Tree (max_depth = 2))   \n",
       "13                                         Keras   \n",
       "\n",
       "                                      Model Parameter  Train F1-Score  \\\n",
       "0              n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                            C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                               C = 1, gamma = 'auto'        0.819056   \n",
       "3                              C = 1, gamma = 'scale'        0.835155   \n",
       "4                                             C = 100        0.770344   \n",
       "5                           C = 0.001, penalty = 'l2'        0.783514   \n",
       "6   max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7   max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                                C = 1, max_depth = 1        0.803411   \n",
       "9                               C = 10, max_depth = 3        0.843287   \n",
       "10                                  max_features = 10        0.758410   \n",
       "11               max_features = 20, max_samples = 0.5        0.773619   \n",
       "12             n_estimators = 50, learning_rate = 0.2        0.800369   \n",
       "13                       batch_size = 20, epochs = 10        0.774577   \n",
       "\n",
       "    Test F1-Score  Best Mean Test F1-Score  \n",
       "0        0.584980                 0.788615  \n",
       "1        0.609739                 0.779720  \n",
       "2        0.606868                 0.776477  \n",
       "3        0.602759                 0.770910  \n",
       "4        0.613128                 0.767012  \n",
       "5        0.602663                 0.777433  \n",
       "6        0.596774                 0.770291  \n",
       "7        0.620833                 0.781263  \n",
       "8        0.616519                 0.762953  \n",
       "9        0.608637                 0.771049  \n",
       "10       0.600262                 0.761453  \n",
       "11       0.616327                 0.767716  \n",
       "12       0.623470                 0.769227  \n",
       "13       0.602528                 0.484136  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Classification Model', 'Model Parameter', 'Train F1-Score', 'Test F1-Score', 'Best Mean Test F1-Score'])\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The table above portrays the model parameters that produced the best mean test f1-score values on the training set during the 5-fold cross validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Model</th>\n",
       "      <th>Model Parameter</th>\n",
       "      <th>Train F1-Score</th>\n",
       "      <th>Test F1-Score</th>\n",
       "      <th>Best Mean Test F1-Score</th>\n",
       "      <th>Avg Test F1-Score and Best Mean Test F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>n_neighbors = 20, weights = 'distance'</td>\n",
       "      <td>0.999520</td>\n",
       "      <td>0.584980</td>\n",
       "      <td>0.788615</td>\n",
       "      <td>0.686798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>C = 0.01, penalty = 'l2'</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.609739</td>\n",
       "      <td>0.779720</td>\n",
       "      <td>0.694730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (kernel='rbf')</td>\n",
       "      <td>C = 1, gamma = 'auto'</td>\n",
       "      <td>0.819056</td>\n",
       "      <td>0.606868</td>\n",
       "      <td>0.776477</td>\n",
       "      <td>0.691672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (kernel='poly', degree = 3)</td>\n",
       "      <td>C = 1, gamma = 'scale'</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.602759</td>\n",
       "      <td>0.770910</td>\n",
       "      <td>0.686834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM (kernel='linear')</td>\n",
       "      <td>C = 100</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.613128</td>\n",
       "      <td>0.767012</td>\n",
       "      <td>0.690070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>C = 0.001, penalty = 'l2'</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.602663</td>\n",
       "      <td>0.777433</td>\n",
       "      <td>0.690048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>max_depth=6, criterion = 'entropy', splitter =...</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.770291</td>\n",
       "      <td>0.683533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>max_depth = 6, n_estimators = 200, criterion =...</td>\n",
       "      <td>0.817029</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>0.781263</td>\n",
       "      <td>0.701048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hard Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 1, max_depth = 1</td>\n",
       "      <td>0.803411</td>\n",
       "      <td>0.616519</td>\n",
       "      <td>0.762953</td>\n",
       "      <td>0.689736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Soft Voting (SVM Poly Kernel, Decision Tree)</td>\n",
       "      <td>C = 10, max_depth = 3</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.608637</td>\n",
       "      <td>0.771049</td>\n",
       "      <td>0.689843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bagging (Decision Tree (max_depth = 1))</td>\n",
       "      <td>max_features = 10</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.600262</td>\n",
       "      <td>0.761453</td>\n",
       "      <td>0.680858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pasting (Decision Tree (max_depth = 3))</td>\n",
       "      <td>max_features = 20, max_samples = 0.5</td>\n",
       "      <td>0.773619</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>0.767716</td>\n",
       "      <td>0.692021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AdaBoost (Decision Tree (max_depth = 2))</td>\n",
       "      <td>n_estimators = 50, learning_rate = 0.2</td>\n",
       "      <td>0.800369</td>\n",
       "      <td>0.623470</td>\n",
       "      <td>0.769227</td>\n",
       "      <td>0.696349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Keras</td>\n",
       "      <td>batch_size = 20, epochs = 10</td>\n",
       "      <td>0.774577</td>\n",
       "      <td>0.602528</td>\n",
       "      <td>0.484136</td>\n",
       "      <td>0.543332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Classification Model  \\\n",
       "0                                            KNN   \n",
       "1                            Logistic Regression   \n",
       "2                             SVM (kernel='rbf')   \n",
       "3                SVM (kernel='poly', degree = 3)   \n",
       "4                          SVM (kernel='linear')   \n",
       "5                                     Linear SVM   \n",
       "6                                  Decision Tree   \n",
       "7                                  Random Forest   \n",
       "8   Hard Voting (SVM Poly Kernel, Decision Tree)   \n",
       "9   Soft Voting (SVM Poly Kernel, Decision Tree)   \n",
       "10       Bagging (Decision Tree (max_depth = 1))   \n",
       "11       Pasting (Decision Tree (max_depth = 3))   \n",
       "12      AdaBoost (Decision Tree (max_depth = 2))   \n",
       "13                                         Keras   \n",
       "\n",
       "                                      Model Parameter  Train F1-Score  \\\n",
       "0              n_neighbors = 20, weights = 'distance'        0.999520   \n",
       "1                            C = 0.01, penalty = 'l2'        0.782490   \n",
       "2                               C = 1, gamma = 'auto'        0.819056   \n",
       "3                              C = 1, gamma = 'scale'        0.835155   \n",
       "4                                             C = 100        0.770344   \n",
       "5                           C = 0.001, penalty = 'l2'        0.783514   \n",
       "6   max_depth=6, criterion = 'entropy', splitter =...        0.814570   \n",
       "7   max_depth = 6, n_estimators = 200, criterion =...        0.817029   \n",
       "8                                C = 1, max_depth = 1        0.803411   \n",
       "9                               C = 10, max_depth = 3        0.843287   \n",
       "10                                  max_features = 10        0.758410   \n",
       "11               max_features = 20, max_samples = 0.5        0.773619   \n",
       "12             n_estimators = 50, learning_rate = 0.2        0.800369   \n",
       "13                       batch_size = 20, epochs = 10        0.774577   \n",
       "\n",
       "    Test F1-Score  Best Mean Test F1-Score  \\\n",
       "0        0.584980                 0.788615   \n",
       "1        0.609739                 0.779720   \n",
       "2        0.606868                 0.776477   \n",
       "3        0.602759                 0.770910   \n",
       "4        0.613128                 0.767012   \n",
       "5        0.602663                 0.777433   \n",
       "6        0.596774                 0.770291   \n",
       "7        0.620833                 0.781263   \n",
       "8        0.616519                 0.762953   \n",
       "9        0.608637                 0.771049   \n",
       "10       0.600262                 0.761453   \n",
       "11       0.616327                 0.767716   \n",
       "12       0.623470                 0.769227   \n",
       "13       0.602528                 0.484136   \n",
       "\n",
       "    Avg Test F1-Score and Best Mean Test F1-Score  \n",
       "0                                        0.686798  \n",
       "1                                        0.694730  \n",
       "2                                        0.691672  \n",
       "3                                        0.686834  \n",
       "4                                        0.690070  \n",
       "5                                        0.690048  \n",
       "6                                        0.683533  \n",
       "7                                        0.701048  \n",
       "8                                        0.689736  \n",
       "9                                        0.689843  \n",
       "10                                       0.680858  \n",
       "11                                       0.692021  \n",
       "12                                       0.696349  \n",
       "13                                       0.543332  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report['Avg Test F1-Score and Best Mean Test F1-Score'] = (report['Test F1-Score'] + report['Best Mean Test F1-Score'])/2\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The table above has a new column which is the average of the Test F1-Score and Best Mean Test F1-Score columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Metrics from the report dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test F1-Score for each Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification_model_list = list(report['Classification Model'])\n",
    "test_f1_list = list(report['Test F1-Score'])\n",
    "list_test_f1, list_classification_model = (list(t) for t in zip(*sorted(zip(test_f1_list, classification_model_list))))\n",
    "\n",
    "plt.bar(list_classification_model, list_test_f1)\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(20,5)\n",
    "plt.title('Test F1-Score Value for Each Classification Model')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('Classification Model')\n",
    "plt.ylabel('Test F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Mean Test F1-Score for each Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification_model_list = list(report['Classification Model'])\n",
    "test_f1_list = list(report['Best Mean Test F1-Score'])\n",
    "list_test_f1, list_classification_model = (list(t) for t in zip(*sorted(zip(test_f1_list, classification_model_list))))\n",
    "\n",
    "plt.bar(list_classification_model, list_test_f1)\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(20,5)\n",
    "plt.title('Best Mean Test F1-Score Value for Each Classification Model')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('Classification Model')\n",
    "plt.ylabel('Best Mean Test F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Test F1-Score and Best Mean Test F1-Score for each Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification_model_list = list(report['Classification Model'])\n",
    "test_f1_list = list(report['Avg Test F1-Score and Best Mean Test F1-Score'])\n",
    "list_test_f1, list_classification_model = (list(t) for t in zip(*sorted(zip(test_f1_list, classification_model_list))))\n",
    "\n",
    "plt.bar(list_classification_model, list_test_f1)\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(20,5)\n",
    "plt.title('Avg Test F1-Score and Best Mean Test F1-Score for Each Classification Model')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('Classification Model')\n",
    "plt.ylabel('Avg Test F1-Score and Best Mean Test F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the plots above, the Random Forest Classification Model had the 2nd highest Test F1-Score value, the 2nd highest Best Mean Test F1-Score value, and the highest Avg Test F1-Score and Best Mean Test F1-Score value\n",
    "\n",
    "\n",
    "### Thus, from the plots above, our \"best\" classification model is the Random Forest Classification Model with the model parameters being n_estimators = 200, max_depth = 6, and criterion = 'entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
